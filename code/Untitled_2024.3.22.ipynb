{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "#读取数据\n",
    "df = pd.read_csv(r'C:\\Users\\86130\\Desktop\\论文\\日本会议发表\\data\\app_use_info_label.csv', sep=',', header=0)\n",
    "df2 = pd.read_csv(r'C:\\Users\\86130\\Desktop\\论文\\日本会议发表\\data\\user_portrait.csv', sep=',', header=0)\n",
    "df3 = pd.read_csv(r'C:\\Users\\86130\\Desktop\\论文\\日本会议发表\\data\\user_trajectory2.csv', sep=',', header=0).drop(columns='Unnamed: 0')\n",
    "#合并数据\n",
    "df_all = pd.merge(df, df2, left_on='msisdn', right_on='userid', how='left').drop(columns = ['userid'])\n",
    "df_all = df_all.merge(df3, on = 'msisdn', how = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#日期特征处理\n",
    "def get_time_feature(df, col):\n",
    "    \n",
    "    df_copy = df.copy()\n",
    "    prefix = col + \"_\"\n",
    "    df_copy['new_'+col] = df_copy[col].astype(str)\n",
    "    \n",
    "    col = 'new_'+col\n",
    "    df_copy[col] = pd.to_datetime(df_copy[col])\n",
    "    df_copy[prefix + 'year'] = df_copy[col].dt.year\n",
    "    df_copy[prefix + 'month'] = df_copy[col].dt.month\n",
    "    df_copy[prefix + 'day'] = df_copy[col].dt.day\n",
    "    df_copy[prefix + 'dayofweek'] = df_copy[col].dt.dayofweek\n",
    "    df_copy[prefix + 'is_wknd'] = df_copy[col].dt.dayofweek // 6\n",
    "    df_copy[prefix + 'quarter'] = df_copy[col].dt.quarter\n",
    "    df_copy[prefix + 'is_month_start'] = df_copy[col].dt.is_month_start.astype(int)\n",
    "    df_copy[prefix + 'is_month_end'] = df_copy[col].dt.is_month_end.astype(int)\n",
    "    del df_copy[col]\n",
    "    \n",
    "    return df_copy   \n",
    "    \n",
    "df_all = get_time_feature(df_all, 'stime')\n",
    "df_all = get_time_feature(df_all, 'end_time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#label编码\n",
    "from sklearn import preprocessing\n",
    " \n",
    "enc=preprocessing.LabelEncoder() \n",
    "enc=enc.fit(df_all['app_class_1']) \n",
    "df_all['app_class_1']=enc.transform(df_all['app_class_1'])\n",
    "\n",
    "enc2=preprocessing.LabelEncoder() \n",
    "enc2=enc2.fit(df_all['app_class_2']) \n",
    "df_all['app_class_2']=enc2.transform(df_all['app_class_2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#特征衍生\n",
    "df_all['up_flow']= df_all['up_flow']/1024\n",
    "df_all['down_flow']= df_all['down_flow']/1024\n",
    "\n",
    "df_all['app1_upflow_mean'] = df_all['app_class_1'].map(df_all.groupby('app_class_1')['up_flow'].mean())\n",
    "df_all['app2_upflow_mean'] = df_all['app_class_2'].map(df_all.groupby('app_class_2')['up_flow'].mean())\n",
    "df_all['app1_upflow_max'] = df_all['app_class_1'].map(df_all.groupby('app_class_1')['up_flow'].max())\n",
    "df_all['app2_upflow_max'] = df_all['app_class_2'].map(df_all.groupby('app_class_2')['up_flow'].max())\n",
    "df_all['app1_upflow_min'] = df_all['app_class_1'].map(df_all.groupby('app_class_1')['up_flow'].min())\n",
    "df_all['app2_upflow_min'] = df_all['app_class_2'].map(df_all.groupby('app_class_2')['up_flow'].min())\n",
    "\n",
    "df_all['app1_downflow_mean'] = df_all['app_class_1'].map(df_all.groupby('app_class_1')['down_flow'].mean())\n",
    "df_all['app2_downflow_mean'] = df_all['app_class_2'].map(df_all.groupby('app_class_2')['down_flow'].mean())\n",
    "df_all['app1_downflow_max'] = df_all['app_class_1'].map(df_all.groupby('app_class_1')['down_flow'].max())\n",
    "df_all['app2_downflow_max'] = df_all['app_class_2'].map(df_all.groupby('app_class_2')['down_flow'].max())\n",
    "df_all['app1_downflow_min'] = df_all['app_class_1'].map(df_all.groupby('app_class_1')['down_flow'].min())\n",
    "df_all['app2_downflow_min'] = df_all['app_class_2'].map(df_all.groupby('app_class_2')['down_flow'].min())\n",
    "\n",
    "df_all['app1_consume_mean'] = df_all['app_class_1'].map(df_all.groupby('app_class_1')['consume'].mean())\n",
    "df_all['app2_consume_mean'] = df_all['app_class_2'].map(df_all.groupby('app_class_2')['consume'].mean())\n",
    "df_all['app1_consume_max'] = df_all['app_class_1'].map(df_all.groupby('app_class_1')['consume'].max())\n",
    "df_all['app2_consume_max'] = df_all['app_class_2'].map(df_all.groupby('app_class_2')['consume'].max())\n",
    "df_all['app1_consume_min'] = df_all['app_class_1'].map(df_all.groupby('app_class_1')['consume'].min())\n",
    "df_all['app2_consume_min'] = df_all['app_class_2'].map(df_all.groupby('app_class_2')['consume'].min())\n",
    "\n",
    "df_all['app1_age_mean'] = df_all['app_class_1'].map(df_all.groupby('app_class_1')['age'].mean())\n",
    "df_all['app2_age_mean'] = df_all['app_class_2'].map(df_all.groupby('app_class_2')['age'].mean())\n",
    "df_all['app1_age_max'] = df_all['app_class_1'].map(df_all.groupby('app_class_1')['age'].max())\n",
    "df_all['app2_age_max'] = df_all['app_class_2'].map(df_all.groupby('app_class_2')['age'].max())\n",
    "df_all['app1_age_min'] = df_all['app_class_1'].map(df_all.groupby('app_class_1')['age'].min())\n",
    "df_all['app2_age_min'] = df_all['app_class_2'].map(df_all.groupby('app_class_2')['age'].min())\n",
    "\n",
    "df_all['up-down'] = df_all['up_flow']-df_all['down_flow']\n",
    "df_all['up/down'] = df_all['up_flow']/df_all['down_flow']\n",
    "df_all['up+down'] = df_all['up_flow']+df_all['down_flow']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "63"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#分离训练集，测试集\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df_all=df_all.replace([np.inf, -np.inf], 0)\n",
    "df_all.fillna(0,inplace=True)\n",
    "train_df,test_df = train_test_split(df_all,train_size=0.8,shuffle=True,random_state=42)\n",
    "train_df = train_df.reset_index(drop=True)\n",
    "test_df = test_df.reset_index(drop=True)\n",
    "feature_cols = [cols for cols in df_all if cols not in ['msisdn','label','end_time','stime','times_month']]\n",
    "len(feature_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed: 888\n",
      "|  XGB  Fold  1  Training Start           |\n",
      "[0]\tvalidation_0-mlogloss:0.89551\tvalidation_1-mlogloss:0.89819\n",
      "[99]\tvalidation_0-mlogloss:0.31382\tvalidation_1-mlogloss:0.44397\n",
      "************ Val_Result ************\n",
      "AVG_acc : 0.8014725934006\n",
      "XGB_result :               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.89      0.89      5910\n",
      "           1       0.70      0.73      0.72      3685\n",
      "           2       0.71      0.62      0.66      1406\n",
      "\n",
      "    accuracy                           0.80     11001\n",
      "   macro avg       0.77      0.75      0.75     11001\n",
      "weighted avg       0.80      0.80      0.80     11001\n",
      "\n",
      "************ Test_Result ************\n",
      "AVG_acc : 0.8061231910406516\n",
      "XGB_result :               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.89      0.89      7458\n",
      "           1       0.70      0.74      0.72      4501\n",
      "           2       0.73      0.63      0.68      1792\n",
      "\n",
      "    accuracy                           0.81     13751\n",
      "   macro avg       0.77      0.75      0.76     13751\n",
      "weighted avg       0.81      0.81      0.81     13751\n",
      "\n",
      "|  XGB  Fold  2  Training Start           |\n",
      "[0]\tvalidation_0-mlogloss:0.89519\tvalidation_1-mlogloss:0.89720\n",
      "[99]\tvalidation_0-mlogloss:0.31243\tvalidation_1-mlogloss:0.44124\n",
      "************ Val_Result ************\n",
      "AVG_acc : 0.8018361967093901\n",
      "XGB_result :               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.89      0.89      5909\n",
      "           1       0.70      0.73      0.72      3685\n",
      "           2       0.70      0.62      0.66      1407\n",
      "\n",
      "    accuracy                           0.80     11001\n",
      "   macro avg       0.76      0.75      0.75     11001\n",
      "weighted avg       0.80      0.80      0.80     11001\n",
      "\n",
      "************ Test_Result ************\n",
      "AVG_acc : 0.80546869318595\n",
      "XGB_result :               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.89      0.89      7458\n",
      "           1       0.70      0.74      0.72      4501\n",
      "           2       0.72      0.62      0.67      1792\n",
      "\n",
      "    accuracy                           0.80     13751\n",
      "   macro avg       0.77      0.75      0.76     13751\n",
      "weighted avg       0.81      0.80      0.80     13751\n",
      "\n",
      "|  XGB  Fold  3  Training Start           |\n",
      "[0]\tvalidation_0-mlogloss:0.89519\tvalidation_1-mlogloss:0.89878\n",
      "[99]\tvalidation_0-mlogloss:0.31246\tvalidation_1-mlogloss:0.45190\n",
      "************ Val_Result ************\n",
      "AVG_acc : 0.8015937945035301\n",
      "XGB_result :               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.89      0.89      5909\n",
      "           1       0.70      0.74      0.72      3685\n",
      "           2       0.72      0.61      0.66      1407\n",
      "\n",
      "    accuracy                           0.80     11001\n",
      "   macro avg       0.77      0.74      0.75     11001\n",
      "weighted avg       0.80      0.80      0.80     11001\n",
      "\n",
      "************ Test_Result ************\n",
      "AVG_acc : 0.806074709718081\n",
      "XGB_result :               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.89      0.89      7458\n",
      "           1       0.70      0.75      0.72      4501\n",
      "           2       0.73      0.62      0.67      1792\n",
      "\n",
      "    accuracy                           0.81     13751\n",
      "   macro avg       0.77      0.75      0.76     13751\n",
      "weighted avg       0.81      0.81      0.81     13751\n",
      "\n",
      "|  XGB  Fold  4  Training Start           |\n",
      "[0]\tvalidation_0-mlogloss:0.89526\tvalidation_1-mlogloss:0.89901\n",
      "[99]\tvalidation_0-mlogloss:0.31702\tvalidation_1-mlogloss:0.44542\n",
      "************ Val_Result ************\n",
      "AVG_acc : 0.8026543041541678\n",
      "XGB_result :               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.89      0.89      5910\n",
      "           1       0.70      0.75      0.73      3684\n",
      "           2       0.71      0.62      0.66      1407\n",
      "\n",
      "    accuracy                           0.81     11001\n",
      "   macro avg       0.77      0.75      0.76     11001\n",
      "weighted avg       0.81      0.81      0.81     11001\n",
      "\n",
      "************ Test_Result ************\n",
      "AVG_acc : 0.8063958984801105\n",
      "XGB_result :               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.89      0.89      7458\n",
      "           1       0.70      0.75      0.72      4501\n",
      "           2       0.73      0.62      0.67      1792\n",
      "\n",
      "    accuracy                           0.81     13751\n",
      "   macro avg       0.78      0.75      0.76     13751\n",
      "weighted avg       0.81      0.81      0.81     13751\n",
      "\n",
      "|  XGB  Fold  5  Training Start           |\n",
      "[0]\tvalidation_0-mlogloss:0.89399\tvalidation_1-mlogloss:0.89763\n",
      "[99]\tvalidation_0-mlogloss:0.31531\tvalidation_1-mlogloss:0.44502\n",
      "************ Val_Result ************\n",
      "AVG_acc : 0.8031598069596979\n",
      "XGB_result :               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.89      0.89      5910\n",
      "           1       0.70      0.75      0.73      3684\n",
      "           2       0.71      0.60      0.65      1406\n",
      "\n",
      "    accuracy                           0.81     11000\n",
      "   macro avg       0.77      0.75      0.76     11000\n",
      "weighted avg       0.81      0.81      0.80     11000\n",
      "\n",
      "************ Test_Result ************\n",
      "AVG_acc : 0.8066613337211839\n",
      "XGB_result :               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.89      0.89      7458\n",
      "           1       0.70      0.75      0.72      4501\n",
      "           2       0.74      0.62      0.68      1792\n",
      "\n",
      "    accuracy                           0.81     13751\n",
      "   macro avg       0.78      0.75      0.76     13751\n",
      "weighted avg       0.81      0.81      0.81     13751\n",
      "\n",
      "Seed: 888\n",
      "|  LGB  Fold  1  Training Start           |\n",
      "************ Val_Result ************\n",
      "AVG_acc : 0.7989273702390691\n",
      "Cat_result :               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.89      0.88      5910\n",
      "           1       0.70      0.73      0.71      3685\n",
      "           2       0.72      0.61      0.66      1406\n",
      "\n",
      "    accuracy                           0.80     11001\n",
      "   macro avg       0.76      0.74      0.75     11001\n",
      "weighted avg       0.80      0.80      0.80     11001\n",
      "\n",
      "************ Test_Result ************\n",
      "AVG_acc : 0.8087411824594575\n",
      "Cat_result :               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.89      0.89      7458\n",
      "           1       0.70      0.75      0.72      4501\n",
      "           2       0.74      0.62      0.68      1792\n",
      "\n",
      "    accuracy                           0.81     13751\n",
      "   macro avg       0.78      0.75      0.76     13751\n",
      "weighted avg       0.81      0.81      0.81     13751\n",
      "\n",
      "|  LGB  Fold  2  Training Start           |\n",
      "************ Val_Result ************\n",
      "AVG_acc : 0.8036542132533406\n",
      "Cat_result :               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.89      0.89      5909\n",
      "           1       0.71      0.75      0.73      3685\n",
      "           2       0.71      0.63      0.67      1407\n",
      "\n",
      "    accuracy                           0.81     11001\n",
      "   macro avg       0.77      0.75      0.76     11001\n",
      "weighted avg       0.81      0.81      0.81     11001\n",
      "\n",
      "************ Test_Result ************\n",
      "AVG_acc : 0.8081594065886117\n",
      "Cat_result :               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.89      0.89      7458\n",
      "           1       0.70      0.74      0.72      4501\n",
      "           2       0.73      0.63      0.68      1792\n",
      "\n",
      "    accuracy                           0.81     13751\n",
      "   macro avg       0.77      0.75      0.76     13751\n",
      "weighted avg       0.81      0.81      0.81     13751\n",
      "\n",
      "|  LGB  Fold  3  Training Start           |\n",
      "************ Val_Result ************\n",
      "AVG_acc : 0.8045329212495833\n",
      "Cat_result :               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.89      0.89      5909\n",
      "           1       0.71      0.74      0.73      3685\n",
      "           2       0.73      0.61      0.67      1407\n",
      "\n",
      "    accuracy                           0.81     11001\n",
      "   macro avg       0.78      0.75      0.76     11001\n",
      "weighted avg       0.81      0.81      0.81     11001\n",
      "\n",
      "************ Test_Result ************\n",
      "AVG_acc : 0.8078200373306185\n",
      "Cat_result :               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.89      0.89      7458\n",
      "           1       0.70      0.74      0.72      4501\n",
      "           2       0.73      0.62      0.67      1792\n",
      "\n",
      "    accuracy                           0.81     13751\n",
      "   macro avg       0.77      0.75      0.76     13751\n",
      "weighted avg       0.81      0.81      0.81     13751\n",
      "\n",
      "|  LGB  Fold  4  Training Start           |\n",
      "************ Val_Result ************\n",
      "AVG_acc : 0.8051540769020997\n",
      "Cat_result :               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.89      0.89      5910\n",
      "           1       0.71      0.75      0.73      3684\n",
      "           2       0.73      0.61      0.66      1407\n",
      "\n",
      "    accuracy                           0.81     11001\n",
      "   macro avg       0.77      0.75      0.76     11001\n",
      "weighted avg       0.81      0.81      0.81     11001\n",
      "\n",
      "************ Test_Result ************\n",
      "AVG_acc : 0.8079957821249364\n",
      "Cat_result :               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.89      0.89      7458\n",
      "           1       0.70      0.75      0.72      4501\n",
      "           2       0.74      0.63      0.68      1792\n",
      "\n",
      "    accuracy                           0.81     13751\n",
      "   macro avg       0.78      0.76      0.76     13751\n",
      "weighted avg       0.81      0.81      0.81     13751\n",
      "\n",
      "|  LGB  Fold  5  Training Start           |\n",
      "************ Val_Result ************\n",
      "AVG_acc : 0.8054687160671342\n",
      "Cat_result :               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.89      0.89      5910\n",
      "           1       0.71      0.75      0.73      3684\n",
      "           2       0.72      0.61      0.66      1406\n",
      "\n",
      "    accuracy                           0.81     11000\n",
      "   macro avg       0.77      0.75      0.76     11000\n",
      "weighted avg       0.81      0.81      0.81     11000\n",
      "\n",
      "************ Test_Result ************\n",
      "AVG_acc : 0.8079412406370446\n",
      "Cat_result :               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.89      0.89      7458\n",
      "           1       0.70      0.75      0.72      4501\n",
      "           2       0.74      0.62      0.68      1792\n",
      "\n",
      "    accuracy                           0.81     13751\n",
      "   macro avg       0.78      0.75      0.76     13751\n",
      "weighted avg       0.81      0.81      0.81     13751\n",
      "\n",
      "Seed: 888\n",
      "|  cat  Fold  1  Training Start           |\n",
      "************ Val_Result ************\n",
      "AVG_acc : 0.7923825106808472\n",
      "Cat_result :               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.89      0.88      5910\n",
      "           1       0.69      0.71      0.70      3685\n",
      "           2       0.69      0.60      0.64      1406\n",
      "\n",
      "    accuracy                           0.79     11001\n",
      "   macro avg       0.75      0.73      0.74     11001\n",
      "weighted avg       0.79      0.79      0.79     11001\n",
      "\n",
      "************ Test_Result ************\n",
      "AVG_acc : 0.8038688095411243\n",
      "Cat_result :               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.89      0.89      7458\n",
      "           1       0.69      0.74      0.72      4501\n",
      "           2       0.73      0.60      0.66      1792\n",
      "\n",
      "    accuracy                           0.80     13751\n",
      "   macro avg       0.77      0.74      0.76     13751\n",
      "weighted avg       0.80      0.80      0.80     13751\n",
      "\n",
      "|  cat  Fold  2  Training Start           |\n",
      "************ Val_Result ************\n",
      "AVG_acc : 0.7951549859103717\n",
      "Cat_result :               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.89      0.89      5909\n",
      "           1       0.70      0.73      0.71      3685\n",
      "           2       0.69      0.59      0.64      1407\n",
      "\n",
      "    accuracy                           0.80     11001\n",
      "   macro avg       0.76      0.74      0.75     11001\n",
      "weighted avg       0.80      0.80      0.80     11001\n",
      "\n",
      "************ Test_Result ************\n",
      "AVG_acc : 0.8031052287106393\n",
      "Cat_result :               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.89      0.89      7458\n",
      "           1       0.70      0.73      0.71      4501\n",
      "           2       0.72      0.62      0.66      1792\n",
      "\n",
      "    accuracy                           0.80     13751\n",
      "   macro avg       0.77      0.75      0.76     13751\n",
      "weighted avg       0.80      0.80      0.80     13751\n",
      "\n",
      "|  cat  Fold  3  Training Start           |\n",
      "************ Val_Result ************\n",
      "AVG_acc : 0.7960791443202133\n",
      "Cat_result :               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.89      0.89      5909\n",
      "           1       0.70      0.73      0.71      3685\n",
      "           2       0.71      0.59      0.65      1407\n",
      "\n",
      "    accuracy                           0.80     11001\n",
      "   macro avg       0.76      0.74      0.75     11001\n",
      "weighted avg       0.80      0.80      0.80     11001\n",
      "\n",
      "************ Test_Result ************\n",
      "AVG_acc : 0.8022204445737279\n",
      "Cat_result :               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.89      0.89      7458\n",
      "           1       0.69      0.73      0.71      4501\n",
      "           2       0.72      0.60      0.66      1792\n",
      "\n",
      "    accuracy                           0.80     13751\n",
      "   macro avg       0.77      0.74      0.75     13751\n",
      "weighted avg       0.80      0.80      0.80     13751\n",
      "\n",
      "|  cat  Fold  4  Training Start           |\n",
      "************ Val_Result ************\n",
      "AVG_acc : 0.7974047813835106\n",
      "Cat_result :               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.89      0.89      5910\n",
      "           1       0.70      0.74      0.72      3684\n",
      "           2       0.71      0.59      0.64      1407\n",
      "\n",
      "    accuracy                           0.80     11001\n",
      "   macro avg       0.76      0.74      0.75     11001\n",
      "weighted avg       0.80      0.80      0.80     11001\n",
      "\n",
      "************ Test_Result ************\n",
      "AVG_acc : 0.8022325649043706\n",
      "Cat_result :               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.89      0.89      7458\n",
      "           1       0.69      0.74      0.71      4501\n",
      "           2       0.72      0.60      0.66      1792\n",
      "\n",
      "    accuracy                           0.80     13751\n",
      "   macro avg       0.77      0.74      0.75     13751\n",
      "weighted avg       0.80      0.80      0.80     13751\n",
      "\n",
      "|  cat  Fold  5  Training Start           |\n",
      "************ Val_Result ************\n",
      "AVG_acc : 0.7979238251068084\n",
      "Cat_result :               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.89      0.89      5910\n",
      "           1       0.70      0.74      0.72      3684\n",
      "           2       0.70      0.59      0.64      1406\n",
      "\n",
      "    accuracy                           0.80     11000\n",
      "   macro avg       0.76      0.74      0.75     11000\n",
      "weighted avg       0.80      0.80      0.80     11000\n",
      "\n",
      "************ Test_Result ************\n",
      "AVG_acc : 0.8020362155479601\n",
      "Cat_result :               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.89      0.89      7458\n",
      "           1       0.69      0.73      0.71      4501\n",
      "           2       0.71      0.61      0.66      1792\n",
      "\n",
      "    accuracy                           0.80     13751\n",
      "   macro avg       0.77      0.74      0.75     13751\n",
      "weighted avg       0.80      0.80      0.80     13751\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#构建模型\n",
    "import optuna\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from catboost import CatBoostClassifier \n",
    "from sklearn.model_selection import StratifiedKFold,KFold\n",
    "from sklearn.metrics import roc_auc_score,accuracy_score,classification_report\n",
    "from sklearn.preprocessing import StandardScaler,MinMaxScaler\n",
    "from pytorch_tabnet import tab_model\n",
    "from sklearn.svm import SVC\n",
    "from pytorch_tabnet.multitask import TabNetMultiTaskClassifier\n",
    "import torch\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# # 标准化\n",
    "# def scale(train_features,test_features):\n",
    "#     scaler=StandardScaler()\n",
    "#     scaler.fit(train_features)\n",
    "#     train_features=pd.DataFrame(scaler.transform(train_features),columns=test_features.keys())\n",
    "#     test_features=pd.DataFrame(scaler.transform(test_features),columns=test_features.keys())\n",
    "#     return train_features,test_features\n",
    "\n",
    "# 模型定义\n",
    "def ml_model(clf,train_x,train_y,test_x=[],test_y=[]):\n",
    "    seeds=[888]\n",
    "    train_oof = np.zeros([train_x.shape[0],3])\n",
    "    feat_imp_df = pd.DataFrame()\n",
    "    feat_imp_df['feature'] = train_x.columns\n",
    "    feat_imp_df['imp'] = 0\n",
    "    #标准化\n",
    "    scaler=StandardScaler()\n",
    "    scaler.fit(train_x)\n",
    "    train_x = scaler.transform(train_x)\n",
    "    if (len(test_x)!=0) and (len(test_x)!=0):\n",
    "        test_oof = np.zeros([test_x.shape[0],3])\n",
    "        test_x = scaler.transform(test_x)\n",
    "    for seed in seeds:\n",
    "        print('Seed:',seed)\n",
    "        folds = 5\n",
    "        kf = StratifiedKFold(n_splits=folds, shuffle=True, random_state=seed)\n",
    "        acc_scores_val = []\n",
    "        acc_scores_test = []\n",
    "        for i, (train_index, valid_index) in enumerate(kf.split(train_x, train_y)):\n",
    "            trn_x, trn_y, val_x, val_y = train_x[train_index], train_y[train_index], train_x[valid_index], \\\n",
    "                                        train_y[valid_index] \n",
    "            if clf == 'xgb':\n",
    "                print(\"|  XGB  Fold  {}  Training Start           |\".format(str(i + 1)))\n",
    "                xgb_params = {'booster': 'gbtree','objective': 'multi:softprob','eval_metric':'mlogloss','num_class':3,\n",
    "                    'n_estimators':500,'max_depth': 8,'lambda': 10,'subsample': 0.7,'colsample_bytree': 0.8,'eta': 0.1,\n",
    "                    'colsample_bylevel': 0.7,'tree_method': 'hist','seed': seed,'nthread': 16}\n",
    "                #训练模型\n",
    "                model = xgb.XGBClassifier(*xgb_params)\n",
    "                model.fit(trn_x,trn_y,eval_set=[(trn_x, trn_y),(val_x,val_y)],early_stopping_rounds=50,verbose=100)\n",
    "                #验证集\n",
    "                print('************ Val_Result ************')\n",
    "                val_pred  = model.predict_proba(val_x)\n",
    "                acc_score_val = accuracy_score(val_y, np.argmax(val_pred, axis=1))\n",
    "                acc_scores_val.append(acc_score_val)\n",
    "                print('AVG_acc :',sum(acc_scores_val)/len(acc_scores_val))\n",
    "                print('XGB_result :',classification_report(val_y, np.argmax(val_pred, axis=1)))\n",
    "                #测试集\n",
    "                if (len(test_x)!=0) and (len(test_x)!=0):\n",
    "                    print('************ Test_Result ************')\n",
    "                    test_pred  = model.predict_proba(test_x)\n",
    "                    acc_score_tset = accuracy_score(test_y, np.argmax(test_pred, axis=1))\n",
    "                    acc_scores_test.append(acc_score_tset)\n",
    "                    print('AVG_acc :',sum(acc_scores_test)/len(acc_scores_test))\n",
    "                    print('XGB_result :',classification_report(test_y, np.argmax(test_pred, axis=1)))\n",
    "                    #保存测试集结果\n",
    "                    test_oof += test_pred / kf.n_splits / len(seeds)\n",
    "                \n",
    "                #保存训练集结果\n",
    "                train_oof[valid_index] = val_pred / kf.n_splits / len(seeds)\n",
    "                \n",
    "                #模型特征重要性\n",
    "                feat_imp_df['imp'] += model.feature_importances_ / folds/ len(seeds)\n",
    "                feat_imp_df = feat_imp_df.sort_values(by='imp', ascending=False).reset_index(drop=True)\n",
    "                feat_imp_df['rank'] = range(feat_imp_df.shape[0])\n",
    "            if clf == 'lgb':\n",
    "                lgb_params = {'boosting_type': 'gbdt','n_estimators':500,'min_child_weight': 4,'num_leaves': 64,\n",
    "                    'feature_fraction': 0.8,'bagging_fraction': 0.8,'bagging_freq': 4,'learning_rate': 0.02,\n",
    "                    'seed': seed,'nthread': 32,'n_jobs':8,'verbose': -1}\n",
    "                print(\"|  LGB  Fold  {}  Training Start           |\".format(str(i + 1)))\n",
    "                #训练模型\n",
    "                model = lgb.LGBMClassifier(**lgb_params)\n",
    "                model.fit(trn_x,trn_y)\n",
    "                \n",
    "                #验证集\n",
    "                print('************ Val_Result ************')\n",
    "                val_pred  = model.predict_proba(val_x)\n",
    "                acc_score_val = accuracy_score(val_y, np.argmax(val_pred, axis=1))\n",
    "                acc_scores_val.append(acc_score_val)\n",
    "                print('AVG_acc :',sum(acc_scores_val)/len(acc_scores_val))\n",
    "                print('Cat_result :',classification_report(val_y, np.argmax(val_pred, axis=1)))\n",
    "                #测试集\n",
    "                if (len(test_x)!=0) and (len(test_x)!=0):\n",
    "                    print('************ Test_Result ************')\n",
    "                    test_pred  = model.predict_proba(test_x)\n",
    "                    acc_score_tset = accuracy_score(test_y, np.argmax(test_pred, axis=1))\n",
    "                    acc_scores_test.append(acc_score_tset)\n",
    "                    print('AVG_acc :',sum(acc_scores_test)/len(acc_scores_test))\n",
    "                    print('Cat_result :',classification_report(test_y, np.argmax(test_pred, axis=1))) \n",
    "                    #保存测试集结果\n",
    "                    test_oof += test_pred / kf.n_splits / len(seeds)\n",
    "                #保存训练集结果\n",
    "                train_oof[valid_index] = val_pred / kf.n_splits / len(seeds)\n",
    "            if clf == 'cat':\n",
    "                print(\"|  cat  Fold  {}  Training Start           |\".format(str(i + 1)))\n",
    "                #训练模型\n",
    "                model = CatBoostClassifier(verbose=False)\n",
    "                model.fit(trn_x,trn_y)\n",
    "                    \n",
    "                #验证集\n",
    "                print('************ Val_Result ************')\n",
    "                val_pred  = model.predict_proba(val_x)\n",
    "                acc_score_val = accuracy_score(val_y, np.argmax(val_pred, axis=1))\n",
    "                acc_scores_val.append(acc_score_val)\n",
    "                print('AVG_acc :',sum(acc_scores_val)/len(acc_scores_val))\n",
    "                print('Cat_result :',classification_report(val_y, np.argmax(val_pred, axis=1)))\n",
    "                #测试集\n",
    "                if (len(test_x)!=0) and (len(test_x)!=0):\n",
    "                    print('************ Test_Result ************')\n",
    "                    test_pred  = model.predict_proba(test_x)\n",
    "                    acc_score_tset = accuracy_score(test_y, np.argmax(test_pred, axis=1))\n",
    "                    acc_scores_test.append(acc_score_tset)\n",
    "                    print('AVG_acc :',sum(acc_scores_test)/len(acc_scores_test))\n",
    "                    print('Cat_result :',classification_report(test_y, np.argmax(test_pred, axis=1))) \n",
    "                    #保存测试集结果\n",
    "                    test_oof += test_pred / kf.n_splits / len(seeds)\n",
    "                    \n",
    "                #保存结果\n",
    "                train_oof[valid_index] = val_pred / kf.n_splits / len(seeds)\n",
    "            if clf == 'svm':\n",
    "                print(\"|  SVM  Fold  {}  Training Start           |\".format(str(i + 1)))\n",
    "                #训练模型\n",
    "                model = SVC(kernel='rbf', C=1, gamma='auto', probability=True,max_iter=1000)\n",
    "                model.fit(trn_x,trn_y)\n",
    "                \n",
    "                #验证集\n",
    "                print('************ Val_Result ************')\n",
    "                val_pred  = model.predict_proba(val_x)\n",
    "                acc_score_val = accuracy_score(val_y, np.argmax(val_pred, axis=1))\n",
    "                acc_scores_val.append(acc_score_val)\n",
    "                print('AVG_acc :',sum(acc_scores_val)/len(acc_scores_val))\n",
    "                print('Svm_result :',classification_report(val_y, np.argmax(val_pred, axis=1)))\n",
    "                #测试集\n",
    "                print('************ Test_Result ************')\n",
    "                test_pred  = model.predict_proba(test_x)\n",
    "                acc_score_tset = accuracy_score(test_y, np.argmax(test_pred, axis=1))\n",
    "                acc_scores_test.append(acc_score_tset)\n",
    "                print('AVG_acc :',sum(acc_scores_test)/len(acc_scores_test))\n",
    "                print('Svm_result :',classification_report(test_y, np.argmax(test_pred, axis=1))) \n",
    "                #保存结果\n",
    "                train_oof[valid_index] = val_pred / kf.n_splits / len(seeds)\n",
    "                test_oof += test_pred / kf.n_splits / len(seeds)\n",
    "                \n",
    "            if clf == 'tabnet':\n",
    "                print(f\"     Tab_model  Fold {i+1}  Training Starting       \")\n",
    "                if torch.cuda.is_available():\n",
    "                    print(\"Using GPU\")\n",
    "                    device = \"cuda\"\n",
    "                else:\n",
    "                    print(\"Using CPU\")\n",
    "                    device = \"cpu\"\n",
    "                    \n",
    "                torch.manual_seed(seed)\n",
    "                np.random.seed(seed)\n",
    "                model = tab_model.TabNetClassifier()\n",
    "\n",
    "                model.fit(trn_x, trn_y,eval_set=[(val_x, val_y)],eval_metric=['accuracy'])\n",
    "                #验证集\n",
    "                print('************ Val_Result ************')\n",
    "                val_pred  = model.predict_proba(val_x)\n",
    "                acc_score_val = accuracy_score(val_y, np.argmax(val_pred, axis=1))\n",
    "                acc_scores_val.append(acc_score_val)\n",
    "                print('AVG_acc :',sum(acc_scores_val)/len(acc_scores_val))\n",
    "                print('Tabnet_result :',classification_report(val_y, np.argmax(val_pred, axis=1)))\n",
    "                #测试集\n",
    "                print('************ Test_Result ************')\n",
    "                test_pred  = model.predict_proba(test_x)\n",
    "                acc_score_tset = accuracy_score(test_y, np.argmax(test_pred, axis=1))\n",
    "                acc_scores_test.append(acc_score_tset)\n",
    "                print('AVG_acc :',sum(acc_scores_test)/len(acc_scores_test))\n",
    "                print('Tabnet_result :',classification_report(test_y, np.argmax(test_pred, axis=1))) \n",
    "                #保存结果\n",
    "                train_oof[valid_index] = val_pred / kf.n_splits / len(seeds)\n",
    "                test_oof += test_pred / kf.n_splits / len(seeds)\n",
    "        if (len(test_x)!=0) and (len(test_x)!=0):\n",
    "            return train_oof,test_oof,model,scaler\n",
    "        else:\n",
    "            return train_oof,model,scaler\n",
    "\n",
    "# 训练 XGB模型\n",
    "xgb_train_oof_1,xgb_test_oof_1, xgb_model_1,scaler_1 = ml_model('xgb',train_df[feature_cols], train_df['label'],test_df[feature_cols], test_df['label'])\n",
    "\n",
    "# 训练 LGB模型\n",
    "lgb_train_oof_1,lgb_test_oof_1,lgb_model_1,scaler_1 = ml_model('lgb',train_df[feature_cols], train_df['label'],test_df[feature_cols], test_df['label'])\n",
    "\n",
    "# 训练 CAT模型\n",
    "cat_train_oof_1,cat_test_oof_1,cat_model_1,scaler_1 = ml_model('cat',train_df[feature_cols], train_df['label'],test_df[feature_cols], test_df['label'])\n",
    "\n",
    "# # 训练 SVM模型\n",
    "# svm_train_oof,svm_test_oof,svm_model = ml_model('svm',train_df[feature_cols], train_df['label'],test_df[feature_cols], test_df['label'])\n",
    "\n",
    "# # 训练 Tabnet模型\n",
    "# tab_train_oof,tab_test_oof,tab_model = ml_model('tabnet',train_df[feature_cols], train_df['label'],test_df[feature_cols], test_df['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed: 888\n",
      "|  XGB  Fold  1  Training Start           |\n",
      "[0]\tvalidation_0-mlogloss:0.96886\tvalidation_1-mlogloss:0.98020\n",
      "[99]\tvalidation_0-mlogloss:0.20563\tvalidation_1-mlogloss:0.62165\n",
      "************ Val_Result ************\n",
      "AVG_acc : 0.7434613244296049\n",
      "XGB_result :               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.67      0.68       546\n",
      "           1       0.84      0.85      0.85       771\n",
      "           2       0.65      0.65      0.65       480\n",
      "\n",
      "    accuracy                           0.74      1797\n",
      "   macro avg       0.72      0.72      0.72      1797\n",
      "weighted avg       0.74      0.74      0.74      1797\n",
      "\n",
      "|  XGB  Fold  2  Training Start           |\n",
      "[0]\tvalidation_0-mlogloss:0.97101\tvalidation_1-mlogloss:0.99008\n",
      "[99]\tvalidation_0-mlogloss:0.18730\tvalidation_1-mlogloss:0.62019\n",
      "************ Val_Result ************\n",
      "AVG_acc : 0.736505286588759\n",
      "XGB_result :               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.66      0.66       546\n",
      "           1       0.81      0.84      0.83       771\n",
      "           2       0.67      0.63      0.65       480\n",
      "\n",
      "    accuracy                           0.73      1797\n",
      "   macro avg       0.71      0.71      0.71      1797\n",
      "weighted avg       0.73      0.73      0.73      1797\n",
      "\n",
      "|  XGB  Fold  3  Training Start           |\n",
      "[0]\tvalidation_0-mlogloss:0.97155\tvalidation_1-mlogloss:0.98535\n",
      "[99]\tvalidation_0-mlogloss:0.20543\tvalidation_1-mlogloss:0.60999\n",
      "************ Val_Result ************\n",
      "AVG_acc : 0.7408644036356891\n",
      "XGB_result :               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.69      0.70       547\n",
      "           1       0.82      0.85      0.83       771\n",
      "           2       0.68      0.66      0.67       479\n",
      "\n",
      "    accuracy                           0.75      1797\n",
      "   macro avg       0.74      0.73      0.73      1797\n",
      "weighted avg       0.75      0.75      0.75      1797\n",
      "\n",
      "|  XGB  Fold  4  Training Start           |\n",
      "[0]\tvalidation_0-mlogloss:0.96680\tvalidation_1-mlogloss:0.97836\n",
      "[99]\tvalidation_0-mlogloss:0.19462\tvalidation_1-mlogloss:0.61792\n",
      "************ Val_Result ************\n",
      "AVG_acc : 0.7422092376182526\n",
      "XGB_result :               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.70      0.69       547\n",
      "           1       0.83      0.83      0.83       771\n",
      "           2       0.68      0.67      0.67       479\n",
      "\n",
      "    accuracy                           0.75      1797\n",
      "   macro avg       0.73      0.73      0.73      1797\n",
      "weighted avg       0.75      0.75      0.75      1797\n",
      "\n",
      "|  XGB  Fold  5  Training Start           |\n",
      "[0]\tvalidation_0-mlogloss:0.96519\tvalidation_1-mlogloss:0.98382\n",
      "[99]\tvalidation_0-mlogloss:0.19105\tvalidation_1-mlogloss:0.64402\n",
      "************ Val_Result ************\n",
      "AVG_acc : 0.7387562542371411\n",
      "XGB_result :               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.66      0.66       546\n",
      "           1       0.81      0.83      0.82       771\n",
      "           2       0.65      0.62      0.64       479\n",
      "\n",
      "    accuracy                           0.72      1796\n",
      "   macro avg       0.71      0.71      0.71      1796\n",
      "weighted avg       0.72      0.72      0.72      1796\n",
      "\n",
      "Seed: 888\n",
      "|  LGB  Fold  1  Training Start           |\n",
      "************ Val_Result ************\n",
      "AVG_acc : 0.7534780189204229\n",
      "Cat_result :               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.67      0.69       546\n",
      "           1       0.84      0.87      0.85       771\n",
      "           2       0.66      0.66      0.66       480\n",
      "\n",
      "    accuracy                           0.75      1797\n",
      "   macro avg       0.74      0.73      0.73      1797\n",
      "weighted avg       0.75      0.75      0.75      1797\n",
      "\n",
      "|  LGB  Fold  2  Training Start           |\n",
      "************ Val_Result ************\n",
      "AVG_acc : 0.7465219810795771\n",
      "Cat_result :               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.67      0.67       546\n",
      "           1       0.83      0.85      0.84       771\n",
      "           2       0.67      0.64      0.65       480\n",
      "\n",
      "    accuracy                           0.74      1797\n",
      "   macro avg       0.72      0.72      0.72      1797\n",
      "weighted avg       0.74      0.74      0.74      1797\n",
      "\n",
      "|  LGB  Fold  3  Training Start           |\n",
      "************ Val_Result ************\n",
      "AVG_acc : 0.7490261547022815\n",
      "Cat_result :               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.69      0.71       547\n",
      "           1       0.82      0.85      0.84       771\n",
      "           2       0.68      0.67      0.67       479\n",
      "\n",
      "    accuracy                           0.75      1797\n",
      "   macro avg       0.74      0.74      0.74      1797\n",
      "weighted avg       0.75      0.75      0.75      1797\n",
      "\n",
      "|  LGB  Fold  4  Training Start           |\n",
      "************ Val_Result ************\n",
      "AVG_acc : 0.7494435169727323\n",
      "Cat_result :               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.70      0.70       547\n",
      "           1       0.83      0.85      0.84       771\n",
      "           2       0.67      0.66      0.66       479\n",
      "\n",
      "    accuracy                           0.75      1797\n",
      "   macro avg       0.74      0.73      0.73      1797\n",
      "weighted avg       0.75      0.75      0.75      1797\n",
      "\n",
      "|  LGB  Fold  5  Training Start           |\n",
      "************ Val_Result ************\n",
      "AVG_acc : 0.7484412278320833\n",
      "Cat_result :               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.66      0.68       546\n",
      "           1       0.81      0.87      0.84       771\n",
      "           2       0.67      0.64      0.65       479\n",
      "\n",
      "    accuracy                           0.74      1796\n",
      "   macro avg       0.73      0.72      0.73      1796\n",
      "weighted avg       0.74      0.74      0.74      1796\n",
      "\n",
      "Seed: 888\n",
      "|  cat  Fold  1  Training Start           |\n",
      "************ Val_Result ************\n",
      "AVG_acc : 0.7445742904841403\n",
      "Cat_result :               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.66      0.68       546\n",
      "           1       0.83      0.86      0.85       771\n",
      "           2       0.64      0.64      0.64       480\n",
      "\n",
      "    accuracy                           0.74      1797\n",
      "   macro avg       0.73      0.72      0.72      1797\n",
      "weighted avg       0.74      0.74      0.74      1797\n",
      "\n",
      "|  cat  Fold  2  Training Start           |\n",
      "************ Val_Result ************\n",
      "AVG_acc : 0.7404006677796328\n",
      "Cat_result :               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.67      0.67       546\n",
      "           1       0.82      0.84      0.83       771\n",
      "           2       0.67      0.64      0.66       480\n",
      "\n",
      "    accuracy                           0.74      1797\n",
      "   macro avg       0.72      0.72      0.72      1797\n",
      "weighted avg       0.73      0.74      0.74      1797\n",
      "\n",
      "|  cat  Fold  3  Training Start           |\n",
      "************ Val_Result ************\n",
      "AVG_acc : 0.7462437395659433\n",
      "Cat_result :               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.69      0.71       547\n",
      "           1       0.82      0.85      0.84       771\n",
      "           2       0.69      0.68      0.68       479\n",
      "\n",
      "    accuracy                           0.76      1797\n",
      "   macro avg       0.74      0.74      0.74      1797\n",
      "weighted avg       0.76      0.76      0.76      1797\n",
      "\n",
      "|  cat  Fold  4  Training Start           |\n",
      "************ Val_Result ************\n",
      "AVG_acc : 0.7454090150250418\n",
      "Cat_result :               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.69      0.68       547\n",
      "           1       0.83      0.85      0.84       771\n",
      "           2       0.68      0.64      0.66       479\n",
      "\n",
      "    accuracy                           0.74      1797\n",
      "   macro avg       0.73      0.72      0.73      1797\n",
      "weighted avg       0.74      0.74      0.74      1797\n",
      "\n",
      "|  cat  Fold  5  Training Start           |\n",
      "************ Val_Result ************\n",
      "AVG_acc : 0.743877323378608\n",
      "Cat_result :               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.66      0.67       546\n",
      "           1       0.81      0.86      0.83       771\n",
      "           2       0.67      0.63      0.65       479\n",
      "\n",
      "    accuracy                           0.74      1796\n",
      "   macro avg       0.72      0.72      0.72      1796\n",
      "weighted avg       0.73      0.74      0.74      1796\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#训练集困难样本处理\n",
    "df_pre = pd.DataFrame()\n",
    "df_pre['xgb_pre'] = np.argmax(xgb_train_oof_1,axis=1)\n",
    "df_pre['lgb_pre'] = np.argmax(lgb_train_oof_1,axis=1)\n",
    "df_pre['cat_pre'] = np.argmax(cat_train_oof_1,axis=1)\n",
    "df_pre['label'] = train_df['label']\n",
    "\n",
    "grade_list = []\n",
    "for row in df_pre.itertuples():\n",
    "    grade = 0\n",
    "    if getattr(row,'xgb_pre') == getattr(row,'label'):\n",
    "        grade += 1\n",
    "    if getattr(row,'lgb_pre') == getattr(row,'label'):\n",
    "        grade += 1\n",
    "    if getattr(row,'cat_pre') == getattr(row,'label'):\n",
    "        grade += 1\n",
    "    grade_list.append(grade)\n",
    "\n",
    "#困难样本识别\n",
    "df_pre['grade'] = grade_list\n",
    "train_hard_index = df_pre.loc[(df_pre['grade']==0)].index\n",
    "\n",
    "train_hard_df = train_df.loc[train_hard_index].reset_index(drop=True)\n",
    "xgb_train_hard_oof, xgb_train_hard_model,scaler_2 = ml_model('xgb',train_hard_df[feature_cols],train_hard_df['label'])\n",
    "lgb_train_hard_oof, lgb_train_hard_model,scaler_2 = ml_model('lgb',train_hard_df[feature_cols],train_hard_df['label'])\n",
    "cat_train_hard_oof, cat_train_hard_model,scaler_2 = ml_model('cat',train_hard_df[feature_cols],train_hard_df['label'])\n",
    "\n",
    "#训练集替换困难样本结果\n",
    "xgb_train_oof_2 = np.argmax(xgb_train_oof_1,axis=1)\n",
    "xgb_train_oof_2[train_hard_index]=np.argmax(xgb_train_hard_oof,axis=1)\n",
    "\n",
    "lgb_train_oof_2 = np.argmax(lgb_train_oof_1,axis=1)\n",
    "lgb_train_oof_2[train_hard_index]=np.argmax(lgb_train_hard_oof,axis=1)\n",
    "\n",
    "cat_train_oof_2 = np.argmax(cat_train_oof_1,axis=1)\n",
    "cat_train_oof_2[train_hard_index]=np.argmax(cat_train_hard_oof,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test_hard result :\n",
      "xgb_test_hard result: 0.7623337623337624\n",
      "lgb_test_hard result: 0.7807807807807807\n",
      "cat_test_hard result: 0.7734877734877735\n"
     ]
    }
   ],
   "source": [
    "#测试集困难样本处理\n",
    "df_pre = pd.DataFrame()\n",
    "df_pre['xgb_pre'] = np.argmax(xgb_test_oof_1,axis=1)\n",
    "df_pre['lgb_pre'] = np.argmax(lgb_test_oof_1,axis=1)\n",
    "df_pre['cat_pre'] = np.argmax(cat_test_oof_1,axis=1).flatten()\n",
    "df_pre['label'] = test_df['label']\n",
    "\n",
    "grade_list = []\n",
    "for row in df_pre.itertuples():\n",
    "    grade = 0\n",
    "    if getattr(row,'xgb_pre') == getattr(row,'label'):\n",
    "        grade += 1\n",
    "    if getattr(row,'lgb_pre') == getattr(row,'label'):\n",
    "        grade += 1\n",
    "    if getattr(row,'cat_pre') == getattr(row,'label'):\n",
    "        grade += 1\n",
    "    grade_list.append(grade)\n",
    "\n",
    "#测试集替换困难样本结果\n",
    "df_pre['grade'] = grade_list\n",
    "test_hard_index = df_pre.loc[(df_pre['grade']==0)].index\n",
    "test_hard_df = test_df.loc[test_hard_index].reset_index(drop=True)\n",
    "\n",
    "print('Test_hard result :')\n",
    "xgb_test_hard_oof = xgb_train_hard_model.predict(scaler_2.transform(test_hard_df[feature_cols]))\n",
    "print('xgb_test_hard result:',accuracy_score(xgb_test_hard_oof,test_hard_df['label']))\n",
    "lgb_test_hard_oof = lgb_train_hard_model.predict(scaler_2.transform(test_hard_df[feature_cols]))\n",
    "print('lgb_test_hard result:',accuracy_score(lgb_test_hard_oof,test_hard_df['label']))\n",
    "cat_test_hard_oof = cat_train_hard_model.predict(scaler_2.transform(test_hard_df[feature_cols]))\n",
    "print('cat_test_hard result:',accuracy_score(cat_test_hard_oof,test_hard_df['label']))\n",
    "\n",
    "\n",
    "xgb_test_oof_2 = np.argmax(xgb_test_oof_1,axis=1)\n",
    "xgb_test_oof_2[test_hard_index]=xgb_test_hard_oof\n",
    "\n",
    "lgb_test_oof_2 = np.argmax(lgb_test_oof_1,axis=1)\n",
    "lgb_test_oof_2[test_hard_index]=lgb_test_hard_oof\n",
    "\n",
    "cat_test_oof_2 = np.argmax(cat_test_oof_1,axis=1)\n",
    "cat_test_oof_2[test_hard_index]=cat_test_hard_oof.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def hard_sample(xgb_oof,lgb_oof,cat_oof,df):\n",
    "#     df_pre = pd.DataFrame()\n",
    "#     df_pre['xgb_pre'] = np.argmax(xgb_oof,axis=1)\n",
    "#     df_pre['lgb_pre'] = np.argmax(lgb_oof,axis=1)\n",
    "#     df_pre['cat_pre'] = np.argmax(cat_oof,axis=1)\n",
    "#     df_pre['label'] = df['label']\n",
    "\n",
    "#     grade_list = []\n",
    "#     for row in df_pre.itertuples():\n",
    "#         grade = 0\n",
    "#         if getattr(row,'xgb_pre') == getattr(row,'label'):\n",
    "#             grade += 1\n",
    "#         if getattr(row,'lgb_pre') == getattr(row,'label'):\n",
    "#             grade += 1\n",
    "#         if getattr(row,'cat_pre') == getattr(row,'label'):\n",
    "#             grade += 1\n",
    "#         grade_list.append(grade)\n",
    "\n",
    "#     #困难样本\n",
    "#     df_pre['grade'] = grade_list\n",
    "#     hard_index = df_pre.loc[(df_pre['grade']==0)].index\n",
    "\n",
    "#     # 困难样本独立训练\n",
    "#     # 训练 XGB模型\n",
    "#     hard_df = df.loc[hard_index].reset_index(drop=True)\n",
    "#     xgb_oof_2, xgb_model_2 = ml_model('xgb',hard_df[feature_cols],hard_df['label'])\n",
    "#     # 训练 LGB模型\n",
    "#     lgb_oof_2,lgb_model_2 = ml_model('lgb',hard_df[feature_cols], hard_df['label'])\n",
    "\n",
    "#     # 训练 cat模型\n",
    "#     cat_oof_2,cat_model_2 = ml_model('cat',hard_df[feature_cols], hard_df['label'])\n",
    "\n",
    "#     #替换困难样本结果\n",
    "#     xgb_oof = np.argmax(xgb_oof,axis=1)\n",
    "#     xgb_oof[hard_index]=np.argmax(xgb_oof_2,axis=1)\n",
    "\n",
    "#     lgb_oof = np.argmax(lgb_oof,axis=1)\n",
    "#     lgb_oof[hard_index]=np.argmax(lgb_oof_2,axis=1)\n",
    "\n",
    "#     cat_oof = np.argmax(cat_oof,axis=1)\n",
    "#     cat_oof[hard_index]=np.argmax(cat_oof_2,axis=1)\n",
    "    \n",
    "#     return xgb_oof,lgb_oof,cat_oof\n",
    "\n",
    "# xgb_train_oof,lgb_train_oof,cat_train_oof = hard_sample(xgb_train_oof,lgb_train_oof,cat_train_oof,train_df)\n",
    "# xgb_test_oof,lgb_test_oof,cat_test_oof = hard_sample(xgb_test_oof,lgb_test_oof,cat_test_oof,test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed: 888\n",
      "     Tab_model  Fold 1  Training Starting       \n",
      "Using CPU\n",
      "epoch 0  | loss: 0.53504 | val_0_accuracy: 0.92083 |  0:00:02s\n",
      "epoch 1  | loss: 0.27773 | val_0_accuracy: 0.92155 |  0:00:04s\n",
      "epoch 2  | loss: 0.25551 | val_0_accuracy: 0.92473 |  0:00:06s\n",
      "epoch 3  | loss: 0.25235 | val_0_accuracy: 0.92592 |  0:00:08s\n",
      "epoch 4  | loss: 0.25103 | val_0_accuracy: 0.92337 |  0:00:10s\n",
      "epoch 5  | loss: 0.25265 | val_0_accuracy: 0.92446 |  0:00:12s\n",
      "epoch 6  | loss: 0.24759 | val_0_accuracy: 0.92292 |  0:00:14s\n",
      "epoch 7  | loss: 0.24627 | val_0_accuracy: 0.92782 |  0:00:16s\n",
      "epoch 8  | loss: 0.24313 | val_0_accuracy: 0.92519 |  0:00:18s\n",
      "epoch 9  | loss: 0.24407 | val_0_accuracy: 0.92273 |  0:00:20s\n",
      "epoch 10 | loss: 0.2427  | val_0_accuracy: 0.92573 |  0:00:22s\n",
      "epoch 11 | loss: 0.24061 | val_0_accuracy: 0.92437 |  0:00:24s\n",
      "epoch 12 | loss: 0.24533 | val_0_accuracy: 0.92473 |  0:00:26s\n",
      "epoch 13 | loss: 0.24438 | val_0_accuracy: 0.92573 |  0:00:28s\n",
      "epoch 14 | loss: 0.24475 | val_0_accuracy: 0.92346 |  0:00:30s\n",
      "epoch 15 | loss: 0.2428  | val_0_accuracy: 0.9251  |  0:00:32s\n",
      "epoch 16 | loss: 0.24264 | val_0_accuracy: 0.9251  |  0:00:34s\n",
      "epoch 17 | loss: 0.24204 | val_0_accuracy: 0.92682 |  0:00:36s\n",
      "\n",
      "Early stopping occurred at epoch 17 with best_epoch = 7 and best_val_0_accuracy = 0.92782\n",
      "************ Val_Result ************\n",
      "AVG_acc : 0.9278247432051632\n",
      "Tabnet_result :               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.95      0.95      5910\n",
      "           1       0.91      0.93      0.92      3685\n",
      "           2       0.85      0.82      0.84      1406\n",
      "\n",
      "    accuracy                           0.93     11001\n",
      "   macro avg       0.90      0.90      0.90     11001\n",
      "weighted avg       0.93      0.93      0.93     11001\n",
      "\n",
      "************ Test_Result ************\n",
      "AVG_acc : 0.9407315831575885\n",
      "Tabnet_result :               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.96      0.96      7458\n",
      "           1       0.92      0.95      0.94      4501\n",
      "           2       0.87      0.84      0.86      1792\n",
      "\n",
      "    accuracy                           0.94     13751\n",
      "   macro avg       0.92      0.92      0.92     13751\n",
      "weighted avg       0.94      0.94      0.94     13751\n",
      "\n",
      "     Tab_model  Fold 2  Training Starting       \n",
      "Using CPU\n",
      "epoch 0  | loss: 0.54884 | val_0_accuracy: 0.91346 |  0:00:02s\n",
      "epoch 1  | loss: 0.31108 | val_0_accuracy: 0.92519 |  0:00:04s\n",
      "epoch 2  | loss: 0.2859  | val_0_accuracy: 0.92492 |  0:00:06s\n",
      "epoch 3  | loss: 0.26315 | val_0_accuracy: 0.92746 |  0:00:08s\n",
      "epoch 4  | loss: 0.25612 | val_0_accuracy: 0.92628 |  0:00:10s\n",
      "epoch 5  | loss: 0.25449 | val_0_accuracy: 0.92692 |  0:00:12s\n",
      "epoch 6  | loss: 0.25512 | val_0_accuracy: 0.92483 |  0:00:13s\n",
      "epoch 7  | loss: 0.2509  | val_0_accuracy: 0.92392 |  0:00:16s\n",
      "epoch 8  | loss: 0.24982 | val_0_accuracy: 0.92564 |  0:00:18s\n",
      "epoch 9  | loss: 0.24919 | val_0_accuracy: 0.92473 |  0:00:20s\n",
      "epoch 10 | loss: 0.24554 | val_0_accuracy: 0.92592 |  0:00:22s\n",
      "epoch 11 | loss: 0.24369 | val_0_accuracy: 0.92564 |  0:00:24s\n",
      "epoch 12 | loss: 0.2422  | val_0_accuracy: 0.92601 |  0:00:26s\n",
      "epoch 13 | loss: 0.24072 | val_0_accuracy: 0.92601 |  0:00:28s\n",
      "\n",
      "Early stopping occurred at epoch 13 with best_epoch = 3 and best_val_0_accuracy = 0.92746\n",
      "************ Val_Result ************\n",
      "AVG_acc : 0.9276429415507681\n",
      "Tabnet_result :               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.96      0.95      5909\n",
      "           1       0.92      0.92      0.92      3685\n",
      "           2       0.84      0.84      0.84      1407\n",
      "\n",
      "    accuracy                           0.93     11001\n",
      "   macro avg       0.90      0.90      0.90     11001\n",
      "weighted avg       0.93      0.93      0.93     11001\n",
      "\n",
      "************ Test_Result ************\n",
      "AVG_acc : 0.9406225001818049\n",
      "Tabnet_result :               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.96      0.96      7458\n",
      "           1       0.93      0.94      0.94      4501\n",
      "           2       0.87      0.85      0.86      1792\n",
      "\n",
      "    accuracy                           0.94     13751\n",
      "   macro avg       0.92      0.92      0.92     13751\n",
      "weighted avg       0.94      0.94      0.94     13751\n",
      "\n",
      "     Tab_model  Fold 3  Training Starting       \n",
      "Using CPU\n",
      "epoch 0  | loss: 0.60737 | val_0_accuracy: 0.91128 |  0:00:02s\n",
      "epoch 1  | loss: 0.28784 | val_0_accuracy: 0.92355 |  0:00:04s\n",
      "epoch 2  | loss: 0.26792 | val_0_accuracy: 0.92155 |  0:00:05s\n",
      "epoch 3  | loss: 0.26715 | val_0_accuracy: 0.92201 |  0:00:08s\n",
      "epoch 4  | loss: 0.26375 | val_0_accuracy: 0.92537 |  0:00:10s\n",
      "epoch 5  | loss: 0.26028 | val_0_accuracy: 0.92464 |  0:00:12s\n",
      "epoch 6  | loss: 0.25718 | val_0_accuracy: 0.92501 |  0:00:14s\n",
      "epoch 7  | loss: 0.257   | val_0_accuracy: 0.92346 |  0:00:16s\n",
      "epoch 8  | loss: 0.25096 | val_0_accuracy: 0.92555 |  0:00:18s\n",
      "epoch 9  | loss: 0.25297 | val_0_accuracy: 0.92682 |  0:00:20s\n",
      "epoch 10 | loss: 0.25075 | val_0_accuracy: 0.92455 |  0:00:22s\n",
      "epoch 11 | loss: 0.25024 | val_0_accuracy: 0.9271  |  0:00:24s\n",
      "epoch 12 | loss: 0.24801 | val_0_accuracy: 0.92737 |  0:00:26s\n",
      "epoch 13 | loss: 0.25046 | val_0_accuracy: 0.9261  |  0:00:28s\n",
      "epoch 14 | loss: 0.24677 | val_0_accuracy: 0.92673 |  0:00:30s\n",
      "epoch 15 | loss: 0.24658 | val_0_accuracy: 0.92455 |  0:00:32s\n",
      "epoch 16 | loss: 0.24511 | val_0_accuracy: 0.92601 |  0:00:34s\n",
      "epoch 17 | loss: 0.24178 | val_0_accuracy: 0.92555 |  0:00:36s\n",
      "epoch 18 | loss: 0.24158 | val_0_accuracy: 0.92646 |  0:00:38s\n",
      "epoch 19 | loss: 0.2393  | val_0_accuracy: 0.92519 |  0:00:40s\n",
      "epoch 20 | loss: 0.23359 | val_0_accuracy: 0.92373 |  0:00:42s\n",
      "epoch 21 | loss: 0.23588 | val_0_accuracy: 0.92483 |  0:00:44s\n",
      "epoch 22 | loss: 0.23728 | val_0_accuracy: 0.92646 |  0:00:46s\n",
      "\n",
      "Early stopping occurred at epoch 22 with best_epoch = 12 and best_val_0_accuracy = 0.92737\n",
      "************ Val_Result ************\n",
      "AVG_acc : 0.9275520407235706\n",
      "Tabnet_result :               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.94      0.95      5909\n",
      "           1       0.90      0.94      0.92      3685\n",
      "           2       0.84      0.85      0.84      1407\n",
      "\n",
      "    accuracy                           0.93     11001\n",
      "   macro avg       0.90      0.91      0.91     11001\n",
      "weighted avg       0.93      0.93      0.93     11001\n",
      "\n",
      "************ Test_Result ************\n",
      "AVG_acc : 0.940780064480159\n",
      "Tabnet_result :               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.95      0.96      7458\n",
      "           1       0.92      0.95      0.94      4501\n",
      "           2       0.86      0.87      0.86      1792\n",
      "\n",
      "    accuracy                           0.94     13751\n",
      "   macro avg       0.92      0.92      0.92     13751\n",
      "weighted avg       0.94      0.94      0.94     13751\n",
      "\n",
      "     Tab_model  Fold 4  Training Starting       \n",
      "Using CPU\n",
      "epoch 0  | loss: 0.55834 | val_0_accuracy: 0.92101 |  0:00:01s\n",
      "epoch 1  | loss: 0.28352 | val_0_accuracy: 0.92264 |  0:00:03s\n",
      "epoch 2  | loss: 0.27437 | val_0_accuracy: 0.92637 |  0:00:05s\n",
      "epoch 3  | loss: 0.26572 | val_0_accuracy: 0.92592 |  0:00:07s\n",
      "epoch 4  | loss: 0.26446 | val_0_accuracy: 0.92455 |  0:00:09s\n",
      "epoch 5  | loss: 0.2645  | val_0_accuracy: 0.92755 |  0:00:11s\n",
      "epoch 6  | loss: 0.26065 | val_0_accuracy: 0.92873 |  0:00:13s\n",
      "epoch 7  | loss: 0.26127 | val_0_accuracy: 0.92828 |  0:00:15s\n",
      "epoch 8  | loss: 0.26105 | val_0_accuracy: 0.92692 |  0:00:17s\n",
      "epoch 9  | loss: 0.2582  | val_0_accuracy: 0.92773 |  0:00:19s\n",
      "epoch 10 | loss: 0.26317 | val_0_accuracy: 0.92819 |  0:00:21s\n",
      "epoch 11 | loss: 0.25907 | val_0_accuracy: 0.92728 |  0:00:23s\n",
      "epoch 12 | loss: 0.25654 | val_0_accuracy: 0.92792 |  0:00:25s\n",
      "epoch 13 | loss: 0.25685 | val_0_accuracy: 0.92801 |  0:00:27s\n",
      "epoch 14 | loss: 0.25696 | val_0_accuracy: 0.92692 |  0:00:29s\n",
      "epoch 15 | loss: 0.25508 | val_0_accuracy: 0.92682 |  0:00:31s\n",
      "epoch 16 | loss: 0.25342 | val_0_accuracy: 0.92837 |  0:00:33s\n",
      "\n",
      "Early stopping occurred at epoch 16 with best_epoch = 6 and best_val_0_accuracy = 0.92873\n",
      "************ Val_Result ************\n",
      "AVG_acc : 0.9278474684119625\n",
      "Tabnet_result :               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.96      0.96      5910\n",
      "           1       0.92      0.91      0.92      3684\n",
      "           2       0.83      0.86      0.85      1407\n",
      "\n",
      "    accuracy                           0.93     11001\n",
      "   macro avg       0.90      0.91      0.91     11001\n",
      "weighted avg       0.93      0.93      0.93     11001\n",
      "\n",
      "************ Test_Result ************\n",
      "AVG_acc : 0.9411497345647589\n",
      "Tabnet_result :               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.96      0.96      7458\n",
      "           1       0.94      0.93      0.94      4501\n",
      "           2       0.86      0.88      0.87      1792\n",
      "\n",
      "    accuracy                           0.94     13751\n",
      "   macro avg       0.92      0.92      0.92     13751\n",
      "weighted avg       0.94      0.94      0.94     13751\n",
      "\n",
      "     Tab_model  Fold 5  Training Starting       \n",
      "Using CPU\n",
      "epoch 0  | loss: 0.52007 | val_0_accuracy: 0.91582 |  0:00:02s\n",
      "epoch 1  | loss: 0.28904 | val_0_accuracy: 0.92245 |  0:00:04s\n",
      "epoch 2  | loss: 0.27834 | val_0_accuracy: 0.92455 |  0:00:06s\n",
      "epoch 3  | loss: 0.27322 | val_0_accuracy: 0.92391 |  0:00:07s\n",
      "epoch 4  | loss: 0.27468 | val_0_accuracy: 0.92636 |  0:00:09s\n",
      "epoch 5  | loss: 0.26823 | val_0_accuracy: 0.92782 |  0:00:11s\n",
      "epoch 6  | loss: 0.26425 | val_0_accuracy: 0.92673 |  0:00:13s\n",
      "epoch 7  | loss: 0.26206 | val_0_accuracy: 0.92627 |  0:00:15s\n",
      "epoch 8  | loss: 0.26025 | val_0_accuracy: 0.92464 |  0:00:17s\n",
      "epoch 9  | loss: 0.25717 | val_0_accuracy: 0.92309 |  0:00:20s\n",
      "epoch 10 | loss: 0.26419 | val_0_accuracy: 0.92582 |  0:00:21s\n",
      "epoch 11 | loss: 0.25506 | val_0_accuracy: 0.92736 |  0:00:24s\n",
      "epoch 12 | loss: 0.25323 | val_0_accuracy: 0.926   |  0:00:26s\n",
      "epoch 13 | loss: 0.25148 | val_0_accuracy: 0.92764 |  0:00:28s\n",
      "epoch 14 | loss: 0.25135 | val_0_accuracy: 0.92573 |  0:00:30s\n",
      "epoch 15 | loss: 0.24964 | val_0_accuracy: 0.92673 |  0:00:32s\n",
      "\n",
      "Early stopping occurred at epoch 15 with best_epoch = 5 and best_val_0_accuracy = 0.92782\n",
      "************ Val_Result ************\n",
      "AVG_acc : 0.9278416110932064\n",
      "Tabnet_result :               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.96      0.95      5910\n",
      "           1       0.93      0.91      0.92      3684\n",
      "           2       0.85      0.82      0.83      1406\n",
      "\n",
      "    accuracy                           0.93     11000\n",
      "   macro avg       0.91      0.90      0.90     11000\n",
      "weighted avg       0.93      0.93      0.93     11000\n",
      "\n",
      "************ Test_Result ************\n",
      "AVG_acc : 0.940978837902698\n",
      "Tabnet_result :               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.97      0.96      7458\n",
      "           1       0.94      0.93      0.94      4501\n",
      "           2       0.88      0.84      0.86      1792\n",
      "\n",
      "    accuracy                           0.94     13751\n",
      "   macro avg       0.92      0.91      0.92     13751\n",
      "weighted avg       0.94      0.94      0.94     13751\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#xgb预测结果作为新特征(替换困难样本)\n",
    "train_df['xgb_pre'] = xgb_train_oof_2\n",
    "test_df['xgb_pre'] = xgb_test_oof_2\n",
    "#lgb预测结果作为新特征(替换困难样本)\n",
    "train_df['lgb_pre'] = lgb_train_oof_2\n",
    "test_df['lgb_pre'] = lgb_test_oof_2\n",
    "#cat预测结果作为新特征(替换困难样本)\n",
    "train_df['cat_pre'] = cat_train_oof_2\n",
    "test_df['cat_pre'] = cat_test_oof_2\n",
    "\n",
    "#训练特征\n",
    "feature_cols = [cols for cols in train_df if cols not in ['msisdn','label','end_time','stime','times_month']]\n",
    "# 训练tabnet模型\n",
    "final_tab_train_oof,final_tab_test_oof,final_tab_model,scaler = ml_model('tabnet',train_df[feature_cols], train_df['label'],test_df[feature_cols], test_df['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>app_class_1</th>\n",
       "      <th>app_class_2</th>\n",
       "      <th>up_flow</th>\n",
       "      <th>down_flow</th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>consume</th>\n",
       "      <th>user_star</th>\n",
       "      <th>region</th>\n",
       "      <th>prov</th>\n",
       "      <th>...</th>\n",
       "      <th>app1_age_mean</th>\n",
       "      <th>app2_age_mean</th>\n",
       "      <th>app1_age_max</th>\n",
       "      <th>app2_age_max</th>\n",
       "      <th>app1_age_min</th>\n",
       "      <th>app2_age_min</th>\n",
       "      <th>up-down</th>\n",
       "      <th>up/down</th>\n",
       "      <th>up+down</th>\n",
       "      <th>tab_pre</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>31</td>\n",
       "      <td>336</td>\n",
       "      <td>427.747070</td>\n",
       "      <td>243.935547</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>100.20</td>\n",
       "      <td>3</td>\n",
       "      <td>1000250.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>...</td>\n",
       "      <td>36.728163</td>\n",
       "      <td>36.562149</td>\n",
       "      <td>75</td>\n",
       "      <td>74</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>183.811523</td>\n",
       "      <td>1.753525</td>\n",
       "      <td>671.682617</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>62</td>\n",
       "      <td>482</td>\n",
       "      <td>0.816406</td>\n",
       "      <td>3.180664</td>\n",
       "      <td>36</td>\n",
       "      <td>0</td>\n",
       "      <td>229.00</td>\n",
       "      <td>5</td>\n",
       "      <td>1000250.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>...</td>\n",
       "      <td>39.948128</td>\n",
       "      <td>39.910922</td>\n",
       "      <td>85</td>\n",
       "      <td>85</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-2.364258</td>\n",
       "      <td>0.256678</td>\n",
       "      <td>3.997070</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>90</td>\n",
       "      <td>770</td>\n",
       "      <td>68.468750</td>\n",
       "      <td>3584.277344</td>\n",
       "      <td>52</td>\n",
       "      <td>0</td>\n",
       "      <td>88.19</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>35.645078</td>\n",
       "      <td>35.374341</td>\n",
       "      <td>74</td>\n",
       "      <td>74</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-3515.808594</td>\n",
       "      <td>0.019103</td>\n",
       "      <td>3652.746094</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>119</td>\n",
       "      <td>739</td>\n",
       "      <td>846.987305</td>\n",
       "      <td>22409.771484</td>\n",
       "      <td>42</td>\n",
       "      <td>0</td>\n",
       "      <td>187.00</td>\n",
       "      <td>5</td>\n",
       "      <td>1000250.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>...</td>\n",
       "      <td>40.124583</td>\n",
       "      <td>40.116589</td>\n",
       "      <td>85</td>\n",
       "      <td>85</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-21562.784180</td>\n",
       "      <td>0.037795</td>\n",
       "      <td>23256.758789</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>61</td>\n",
       "      <td>598</td>\n",
       "      <td>13.763672</td>\n",
       "      <td>183.606445</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "      <td>154.00</td>\n",
       "      <td>4</td>\n",
       "      <td>1000250.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>...</td>\n",
       "      <td>39.846303</td>\n",
       "      <td>36.451613</td>\n",
       "      <td>85</td>\n",
       "      <td>60</td>\n",
       "      <td>-1</td>\n",
       "      <td>21</td>\n",
       "      <td>-169.842773</td>\n",
       "      <td>0.074963</td>\n",
       "      <td>197.370117</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54999</th>\n",
       "      <td>62</td>\n",
       "      <td>482</td>\n",
       "      <td>30.463867</td>\n",
       "      <td>71.309570</td>\n",
       "      <td>32</td>\n",
       "      <td>0</td>\n",
       "      <td>110.20</td>\n",
       "      <td>4</td>\n",
       "      <td>1000523.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>...</td>\n",
       "      <td>39.948128</td>\n",
       "      <td>39.910922</td>\n",
       "      <td>85</td>\n",
       "      <td>85</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-40.845703</td>\n",
       "      <td>0.427206</td>\n",
       "      <td>101.773438</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55000</th>\n",
       "      <td>62</td>\n",
       "      <td>482</td>\n",
       "      <td>1.765625</td>\n",
       "      <td>5.877930</td>\n",
       "      <td>38</td>\n",
       "      <td>0</td>\n",
       "      <td>68.00</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>39.948128</td>\n",
       "      <td>39.910922</td>\n",
       "      <td>85</td>\n",
       "      <td>85</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-4.112305</td>\n",
       "      <td>0.300382</td>\n",
       "      <td>7.643555</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55001</th>\n",
       "      <td>8</td>\n",
       "      <td>719</td>\n",
       "      <td>2.119141</td>\n",
       "      <td>1.094727</td>\n",
       "      <td>51</td>\n",
       "      <td>0</td>\n",
       "      <td>88.99</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>40.120906</td>\n",
       "      <td>41.536608</td>\n",
       "      <td>85</td>\n",
       "      <td>85</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1.024414</td>\n",
       "      <td>1.935772</td>\n",
       "      <td>3.213867</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55002</th>\n",
       "      <td>62</td>\n",
       "      <td>421</td>\n",
       "      <td>114.936523</td>\n",
       "      <td>37.320312</td>\n",
       "      <td>56</td>\n",
       "      <td>0</td>\n",
       "      <td>8.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>39.948128</td>\n",
       "      <td>40.616967</td>\n",
       "      <td>85</td>\n",
       "      <td>85</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>77.616211</td>\n",
       "      <td>3.079731</td>\n",
       "      <td>152.256836</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55003</th>\n",
       "      <td>61</td>\n",
       "      <td>617</td>\n",
       "      <td>99.207031</td>\n",
       "      <td>57.899414</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "      <td>97.22</td>\n",
       "      <td>2</td>\n",
       "      <td>1000250.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>...</td>\n",
       "      <td>39.846303</td>\n",
       "      <td>39.991061</td>\n",
       "      <td>85</td>\n",
       "      <td>85</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>41.307617</td>\n",
       "      <td>1.713438</td>\n",
       "      <td>157.106445</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>55004 rows × 64 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       app_class_1  app_class_2     up_flow     down_flow  age  sex  consume  \\\n",
       "0               31          336  427.747070    243.935547   40    0   100.20   \n",
       "1               62          482    0.816406      3.180664   36    0   229.00   \n",
       "2               90          770   68.468750   3584.277344   52    0    88.19   \n",
       "3              119          739  846.987305  22409.771484   42    0   187.00   \n",
       "4               61          598   13.763672    183.606445   32    1   154.00   \n",
       "...            ...          ...         ...           ...  ...  ...      ...   \n",
       "54999           62          482   30.463867     71.309570   32    0   110.20   \n",
       "55000           62          482    1.765625      5.877930   38    0    68.00   \n",
       "55001            8          719    2.119141      1.094727   51    0    88.99   \n",
       "55002           62          421  114.936523     37.320312   56    0     8.00   \n",
       "55003           61          617   99.207031     57.899414   32    1    97.22   \n",
       "\n",
       "       user_star     region  prov  ...  app1_age_mean  app2_age_mean  \\\n",
       "0              3  1000250.0  32.0  ...      36.728163      36.562149   \n",
       "1              5  1000250.0  32.0  ...      39.948128      39.910922   \n",
       "2              2        0.0   0.0  ...      35.645078      35.374341   \n",
       "3              5  1000250.0  32.0  ...      40.124583      40.116589   \n",
       "4              4  1000250.0  32.0  ...      39.846303      36.451613   \n",
       "...          ...        ...   ...  ...            ...            ...   \n",
       "54999          4  1000523.0  32.0  ...      39.948128      39.910922   \n",
       "55000          4        0.0   0.0  ...      39.948128      39.910922   \n",
       "55001          4        0.0   0.0  ...      40.120906      41.536608   \n",
       "55002          1        0.0   0.0  ...      39.948128      40.616967   \n",
       "55003          2  1000250.0  32.0  ...      39.846303      39.991061   \n",
       "\n",
       "       app1_age_max  app2_age_max  app1_age_min  app2_age_min       up-down  \\\n",
       "0                75            74            -1            -1    183.811523   \n",
       "1                85            85            -1            -1     -2.364258   \n",
       "2                74            74            -1            -1  -3515.808594   \n",
       "3                85            85            -1            -1 -21562.784180   \n",
       "4                85            60            -1            21   -169.842773   \n",
       "...             ...           ...           ...           ...           ...   \n",
       "54999            85            85            -1            -1    -40.845703   \n",
       "55000            85            85            -1            -1     -4.112305   \n",
       "55001            85            85            -1            -1      1.024414   \n",
       "55002            85            85            -1            -1     77.616211   \n",
       "55003            85            85            -1            -1     41.307617   \n",
       "\n",
       "        up/down       up+down  tab_pre  \n",
       "0      1.753525    671.682617        1  \n",
       "1      0.256678      3.997070        0  \n",
       "2      0.019103   3652.746094        1  \n",
       "3      0.037795  23256.758789        2  \n",
       "4      0.074963    197.370117        0  \n",
       "...         ...           ...      ...  \n",
       "54999  0.427206    101.773438        1  \n",
       "55000  0.300382      7.643555        0  \n",
       "55001  1.935772      3.213867        1  \n",
       "55002  3.079731    152.256836        1  \n",
       "55003  1.713438    157.106445        1  \n",
       "\n",
       "[55004 rows x 64 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df[[col for col in feature_cols if col not in ['xgb_pre','lgb_pre','cat_pre']]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>app_class_1</th>\n",
       "      <th>app_class_2</th>\n",
       "      <th>up_flow</th>\n",
       "      <th>down_flow</th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>consume</th>\n",
       "      <th>user_star</th>\n",
       "      <th>region</th>\n",
       "      <th>prov</th>\n",
       "      <th>...</th>\n",
       "      <th>app1_age_mean</th>\n",
       "      <th>app2_age_mean</th>\n",
       "      <th>app1_age_max</th>\n",
       "      <th>app2_age_max</th>\n",
       "      <th>app1_age_min</th>\n",
       "      <th>app2_age_min</th>\n",
       "      <th>up-down</th>\n",
       "      <th>up/down</th>\n",
       "      <th>up+down</th>\n",
       "      <th>tab_pre</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>221</td>\n",
       "      <td>0.691406</td>\n",
       "      <td>2.771484</td>\n",
       "      <td>60</td>\n",
       "      <td>0</td>\n",
       "      <td>22.56</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>37.497268</td>\n",
       "      <td>37.908537</td>\n",
       "      <td>76</td>\n",
       "      <td>76</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>-2.080078</td>\n",
       "      <td>0.249471</td>\n",
       "      <td>3.462891</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8</td>\n",
       "      <td>719</td>\n",
       "      <td>6.082031</td>\n",
       "      <td>1.983398</td>\n",
       "      <td>58</td>\n",
       "      <td>1</td>\n",
       "      <td>91.10</td>\n",
       "      <td>2</td>\n",
       "      <td>1000250.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>...</td>\n",
       "      <td>40.120906</td>\n",
       "      <td>41.536608</td>\n",
       "      <td>85</td>\n",
       "      <td>85</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>4.098633</td>\n",
       "      <td>3.066470</td>\n",
       "      <td>8.065430</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>31</td>\n",
       "      <td>697</td>\n",
       "      <td>419.723633</td>\n",
       "      <td>543.608398</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>48.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>36.728163</td>\n",
       "      <td>36.865104</td>\n",
       "      <td>75</td>\n",
       "      <td>75</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-123.884766</td>\n",
       "      <td>0.772107</td>\n",
       "      <td>963.332031</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>137</td>\n",
       "      <td>821</td>\n",
       "      <td>0.599609</td>\n",
       "      <td>0.577148</td>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "      <td>68.10</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>37.181319</td>\n",
       "      <td>38.666667</td>\n",
       "      <td>74</td>\n",
       "      <td>49</td>\n",
       "      <td>16</td>\n",
       "      <td>27</td>\n",
       "      <td>0.022461</td>\n",
       "      <td>1.038917</td>\n",
       "      <td>1.176758</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>31</td>\n",
       "      <td>697</td>\n",
       "      <td>364.347656</td>\n",
       "      <td>2061.189453</td>\n",
       "      <td>34</td>\n",
       "      <td>0</td>\n",
       "      <td>93.10</td>\n",
       "      <td>2</td>\n",
       "      <td>1000250.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>...</td>\n",
       "      <td>36.728163</td>\n",
       "      <td>36.865104</td>\n",
       "      <td>75</td>\n",
       "      <td>75</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1696.841797</td>\n",
       "      <td>0.176766</td>\n",
       "      <td>2425.537109</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13746</th>\n",
       "      <td>119</td>\n",
       "      <td>431</td>\n",
       "      <td>262.013672</td>\n",
       "      <td>207.897461</td>\n",
       "      <td>34</td>\n",
       "      <td>0</td>\n",
       "      <td>176.70</td>\n",
       "      <td>3</td>\n",
       "      <td>1000250.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>...</td>\n",
       "      <td>40.124583</td>\n",
       "      <td>44.354667</td>\n",
       "      <td>85</td>\n",
       "      <td>85</td>\n",
       "      <td>-1</td>\n",
       "      <td>17</td>\n",
       "      <td>54.116211</td>\n",
       "      <td>1.260302</td>\n",
       "      <td>469.911133</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13747</th>\n",
       "      <td>139</td>\n",
       "      <td>714</td>\n",
       "      <td>1.764648</td>\n",
       "      <td>4.765625</td>\n",
       "      <td>34</td>\n",
       "      <td>1</td>\n",
       "      <td>191.23</td>\n",
       "      <td>4</td>\n",
       "      <td>1000250.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>...</td>\n",
       "      <td>35.606897</td>\n",
       "      <td>34.975207</td>\n",
       "      <td>75</td>\n",
       "      <td>75</td>\n",
       "      <td>17</td>\n",
       "      <td>17</td>\n",
       "      <td>-3.000977</td>\n",
       "      <td>0.370287</td>\n",
       "      <td>6.530273</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13748</th>\n",
       "      <td>119</td>\n",
       "      <td>213</td>\n",
       "      <td>1.821289</td>\n",
       "      <td>1.342773</td>\n",
       "      <td>42</td>\n",
       "      <td>0</td>\n",
       "      <td>263.60</td>\n",
       "      <td>6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>40.124583</td>\n",
       "      <td>36.940822</td>\n",
       "      <td>85</td>\n",
       "      <td>85</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.478516</td>\n",
       "      <td>1.356364</td>\n",
       "      <td>3.164062</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13749</th>\n",
       "      <td>61</td>\n",
       "      <td>617</td>\n",
       "      <td>99.733398</td>\n",
       "      <td>55.399414</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "      <td>39.10</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>39.846303</td>\n",
       "      <td>39.991061</td>\n",
       "      <td>85</td>\n",
       "      <td>85</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>44.333984</td>\n",
       "      <td>1.800261</td>\n",
       "      <td>155.132812</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13750</th>\n",
       "      <td>4</td>\n",
       "      <td>718</td>\n",
       "      <td>3.070312</td>\n",
       "      <td>145.499023</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "      <td>59.00</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>40.108586</td>\n",
       "      <td>40.329373</td>\n",
       "      <td>77</td>\n",
       "      <td>77</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-142.428711</td>\n",
       "      <td>0.021102</td>\n",
       "      <td>148.569336</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>13751 rows × 64 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       app_class_1  app_class_2     up_flow    down_flow  age  sex  consume  \\\n",
       "0                3          221    0.691406     2.771484   60    0    22.56   \n",
       "1                8          719    6.082031     1.983398   58    1    91.10   \n",
       "2               31          697  419.723633   543.608398   29    0    48.00   \n",
       "3              137          821    0.599609     0.577148   27    0    68.10   \n",
       "4               31          697  364.347656  2061.189453   34    0    93.10   \n",
       "...            ...          ...         ...          ...  ...  ...      ...   \n",
       "13746          119          431  262.013672   207.897461   34    0   176.70   \n",
       "13747          139          714    1.764648     4.765625   34    1   191.23   \n",
       "13748          119          213    1.821289     1.342773   42    0   263.60   \n",
       "13749           61          617   99.733398    55.399414   32    1    39.10   \n",
       "13750            4          718    3.070312   145.499023   50    1    59.00   \n",
       "\n",
       "       user_star     region  prov  ...  app1_age_mean  app2_age_mean  \\\n",
       "0              1        0.0   0.0  ...      37.497268      37.908537   \n",
       "1              2  1000250.0  32.0  ...      40.120906      41.536608   \n",
       "2              1        0.0   0.0  ...      36.728163      36.865104   \n",
       "3              2        0.0   0.0  ...      37.181319      38.666667   \n",
       "4              2  1000250.0  32.0  ...      36.728163      36.865104   \n",
       "...          ...        ...   ...  ...            ...            ...   \n",
       "13746          3  1000250.0  32.0  ...      40.124583      44.354667   \n",
       "13747          4  1000250.0  32.0  ...      35.606897      34.975207   \n",
       "13748          6        0.0   0.0  ...      40.124583      36.940822   \n",
       "13749          1        0.0   0.0  ...      39.846303      39.991061   \n",
       "13750          2        0.0   0.0  ...      40.108586      40.329373   \n",
       "\n",
       "       app1_age_max  app2_age_max  app1_age_min  app2_age_min      up-down  \\\n",
       "0                76            76            16            16    -2.080078   \n",
       "1                85            85            -1            -1     4.098633   \n",
       "2                75            75            -1            -1  -123.884766   \n",
       "3                74            49            16            27     0.022461   \n",
       "4                75            75            -1            -1 -1696.841797   \n",
       "...             ...           ...           ...           ...          ...   \n",
       "13746            85            85            -1            17    54.116211   \n",
       "13747            75            75            17            17    -3.000977   \n",
       "13748            85            85            -1            -1     0.478516   \n",
       "13749            85            85            -1            -1    44.333984   \n",
       "13750            77            77            -1            -1  -142.428711   \n",
       "\n",
       "        up/down      up+down  tab_pre  \n",
       "0      0.249471     3.462891        0  \n",
       "1      3.066470     8.065430        0  \n",
       "2      0.772107   963.332031        0  \n",
       "3      1.038917     1.176758        0  \n",
       "4      0.176766  2425.537109        1  \n",
       "...         ...          ...      ...  \n",
       "13746  1.260302   469.911133        2  \n",
       "13747  0.370287     6.530273        0  \n",
       "13748  1.356364     3.164062        0  \n",
       "13749  1.800261   155.132812        1  \n",
       "13750  0.021102   148.569336        0  \n",
       "\n",
       "[13751 rows x 64 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df[[col for col in feature_cols if col not in ['xgb_pre','lgb_pre','cat_pre']]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "#模型保存\n",
    "def save_model(mdoel,save_path):\n",
    "    # save_path = save_path + '/' +  mdoel_name +'.pkl'\n",
    "    joblib.dump(mdoel, save_path)\n",
    "#模型保存路径\n",
    "xgb_model_path = './model/xgb.pkl'\n",
    "lgb_model_path = './model/lgb.pkl'\n",
    "cat_model_path = './model/cat.pkl'\n",
    "svm_model_path = './model/svm.pkl'\n",
    "tabnet_model_path = './model/tabnet.pkl'\n",
    "final_tab_model_path = './model/final_tab.pkl'\n",
    "#保存\n",
    "save_model(xgb_model,xgb_model_path)\n",
    "save_model(lgb_model, lgb_model_path)\n",
    "save_model(cat_model,  cat_model_path)\n",
    "save_model(svm_model,svm_model_path)\n",
    "save_model(tab_model, tabnet_model_path)\n",
    "save_model(final_tab_model,  final_tab_model_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machine_learning_39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
