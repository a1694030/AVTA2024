{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "#Read the data\n",
    "df = pd.read_csv('../data/app_use_info_label.csv', sep=',', header=0)\n",
    "df2 = pd.read_csv('../data/user_portrait.csv', sep=',', header=0)\n",
    "df3 = pd.read_csv('../data/user_trajectory2.csv', sep=',', header=0)\n",
    "#Merge data\n",
    "df_all = pd.merge(df, df2, left_on='msisdn', right_on='userid', how='left').drop(columns = ['userid'])\n",
    "df_all = df_all.merge(df3, on = 'msisdn', how = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Date feature processing\n",
    "def get_time_feature(df, col):\n",
    "    \n",
    "    df_copy = df.copy()\n",
    "    prefix = col + \"_\"\n",
    "    df_copy['new_'+col] = df_copy[col].astype(str)\n",
    "    \n",
    "    col = 'new_'+col\n",
    "    df_copy[col] = pd.to_datetime(df_copy[col])\n",
    "    df_copy[prefix + 'year'] = df_copy[col].dt.year\n",
    "    df_copy[prefix + 'month'] = df_copy[col].dt.month\n",
    "    df_copy[prefix + 'day'] = df_copy[col].dt.day\n",
    "    # df_copy[prefix + 'dayofweek'] = df_copy[col].dt.dayofweek\n",
    "    # df_copy[prefix + 'is_wknd'] = df_copy[col].dt.dayofweek // 6\n",
    "    # df_copy[prefix + 'quarter'] = df_copy[col].dt.quarter\n",
    "    # df_copy[prefix + 'is_month_start'] = df_copy[col].dt.is_month_start.astype(int)\n",
    "    # df_copy[prefix + 'is_month_end'] = df_copy[col].dt.is_month_end.astype(int)\n",
    "    del df_copy[col]\n",
    "    \n",
    "    return df_copy   \n",
    "    \n",
    "df_all = get_time_feature(df_all, 'stime')\n",
    "df_all = get_time_feature(df_all, 'end_time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#label encoding\n",
    "from sklearn import preprocessing\n",
    " \n",
    "enc=preprocessing.LabelEncoder() \n",
    "enc=enc.fit(df_all['app_class_1']) \n",
    "df_all['app_class_1']=enc.transform(df_all['app_class_1'])\n",
    "\n",
    "enc2=preprocessing.LabelEncoder() \n",
    "enc2=enc2.fit(df_all['app_class_2']) \n",
    "df_all['app_class_2']=enc2.transform(df_all['app_class_2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt # 导入作图包\n",
    "import seaborn as sns    # 导入作图包\n",
    "\n",
    "plt.figure(figsize=(20,20),dpi=600)\n",
    "mask = np.triu(np.ones_like(df_all.fillna(0).corr().round(2), dtype=np.bool))\n",
    "ax = sns.heatmap(df_all.fillna(0).corr().round(2), linewidths=.5, cmap='YlGnBu',annot=True,vmax=2.0,mask=mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#特征衍生\n",
    "df_all['up_flow']= df_all['up_flow']/1024\n",
    "df_all['down_flow']= df_all['down_flow']/1024\n",
    "\n",
    "df_all['app1_upflow_mean'] = df_all['app_class_1'].map(df_all.groupby('app_class_1')['up_flow'].mean())\n",
    "df_all['app2_upflow_mean'] = df_all['app_class_2'].map(df_all.groupby('app_class_2')['up_flow'].mean())\n",
    "df_all['app1_upflow_max'] = df_all['app_class_1'].map(df_all.groupby('app_class_1')['up_flow'].max())\n",
    "df_all['app2_upflow_max'] = df_all['app_class_2'].map(df_all.groupby('app_class_2')['up_flow'].max())\n",
    "df_all['app1_upflow_min'] = df_all['app_class_1'].map(df_all.groupby('app_class_1')['up_flow'].min())\n",
    "df_all['app2_upflow_min'] = df_all['app_class_2'].map(df_all.groupby('app_class_2')['up_flow'].min())\n",
    "\n",
    "df_all['app1_downflow_mean'] = df_all['app_class_1'].map(df_all.groupby('app_class_1')['down_flow'].mean())\n",
    "df_all['app2_downflow_mean'] = df_all['app_class_2'].map(df_all.groupby('app_class_2')['down_flow'].mean())\n",
    "df_all['app1_downflow_max'] = df_all['app_class_1'].map(df_all.groupby('app_class_1')['down_flow'].max())\n",
    "df_all['app2_downflow_max'] = df_all['app_class_2'].map(df_all.groupby('app_class_2')['down_flow'].max())\n",
    "df_all['app1_downflow_min'] = df_all['app_class_1'].map(df_all.groupby('app_class_1')['down_flow'].min())\n",
    "df_all['app2_downflow_min'] = df_all['app_class_2'].map(df_all.groupby('app_class_2')['down_flow'].min())\n",
    "\n",
    "df_all['app1_consume_mean'] = df_all['app_class_1'].map(df_all.groupby('app_class_1')['consume'].mean())\n",
    "df_all['app2_consume_mean'] = df_all['app_class_2'].map(df_all.groupby('app_class_2')['consume'].mean())\n",
    "df_all['app1_consume_max'] = df_all['app_class_1'].map(df_all.groupby('app_class_1')['consume'].max())\n",
    "df_all['app2_consume_max'] = df_all['app_class_2'].map(df_all.groupby('app_class_2')['consume'].max())\n",
    "df_all['app1_consume_min'] = df_all['app_class_1'].map(df_all.groupby('app_class_1')['consume'].min())\n",
    "df_all['app2_consume_min'] = df_all['app_class_2'].map(df_all.groupby('app_class_2')['consume'].min())\n",
    "\n",
    "df_all['app1_age_mean'] = df_all['app_class_1'].map(df_all.groupby('app_class_1')['age'].mean())\n",
    "df_all['app2_age_mean'] = df_all['app_class_2'].map(df_all.groupby('app_class_2')['age'].mean())\n",
    "df_all['app1_age_max'] = df_all['app_class_1'].map(df_all.groupby('app_class_1')['age'].max())\n",
    "df_all['app2_age_max'] = df_all['app_class_2'].map(df_all.groupby('app_class_2')['age'].max())\n",
    "df_all['app1_age_min'] = df_all['app_class_1'].map(df_all.groupby('app_class_1')['age'].min())\n",
    "df_all['app2_age_min'] = df_all['app_class_2'].map(df_all.groupby('app_class_2')['age'].min())\n",
    "\n",
    "df_all['up-down'] = df_all['up_flow']-df_all['down_flow']\n",
    "df_all['up/down'] = df_all['up_flow']/df_all['down_flow']\n",
    "df_all['up+down'] = df_all['up_flow']+df_all['down_flow']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "53"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#分离训练集，测试集\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df_all=df_all.replace([np.inf, -np.inf], 0)\n",
    "df_all.fillna(0,inplace=True)\n",
    "train_df,test_df = train_test_split(df_all,train_size=0.8,shuffle=True,random_state=42)\n",
    "train_df = train_df.reset_index(drop=True)\n",
    "test_df = test_df.reset_index(drop=True)\n",
    "feature_cols = [cols for cols in df_all if cols not in ['msisdn','label','end_time','stime','times_month']]\n",
    "len(feature_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed: 888\n",
      "|  dt  Fold  1  Training Start           |\n",
      "************ Val_Result ************\n",
      "AVG_acc : 0.7224797745659486\n",
      "DT_result :               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.84      0.84      5910\n",
      "           1       0.61      0.61      0.61      3685\n",
      "           2       0.53      0.53      0.53      1406\n",
      "\n",
      "    accuracy                           0.72     11001\n",
      "   macro avg       0.66      0.66      0.66     11001\n",
      "weighted avg       0.72      0.72      0.72     11001\n",
      "\n",
      "************ Test_Result ************\n",
      "AVG_acc : 0.7233655734128427\n",
      "DT_result :               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.83      0.84      7458\n",
      "           1       0.60      0.62      0.61      4501\n",
      "           2       0.54      0.52      0.53      1792\n",
      "\n",
      "    accuracy                           0.72     13751\n",
      "   macro avg       0.66      0.66      0.66     13751\n",
      "weighted avg       0.72      0.72      0.72     13751\n",
      "\n",
      "|  dt  Fold  2  Training Start           |\n",
      "************ Val_Result ************\n",
      "AVG_acc : 0.7195254976820289\n",
      "DT_result :               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.84      0.84      5909\n",
      "           1       0.60      0.60      0.60      3685\n",
      "           2       0.51      0.51      0.51      1407\n",
      "\n",
      "    accuracy                           0.72     11001\n",
      "   macro avg       0.65      0.65      0.65     11001\n",
      "weighted avg       0.72      0.72      0.72     11001\n",
      "\n",
      "************ Test_Result ************\n",
      "AVG_acc : 0.7251472620173078\n",
      "DT_result :               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.84      0.84      7458\n",
      "           1       0.60      0.62      0.61      4501\n",
      "           2       0.55      0.54      0.54      1792\n",
      "\n",
      "    accuracy                           0.73     13751\n",
      "   macro avg       0.67      0.67      0.67     13751\n",
      "weighted avg       0.73      0.73      0.73     13751\n",
      "\n",
      "|  dt  Fold  3  Training Start           |\n",
      "************ Val_Result ************\n",
      "AVG_acc : 0.7176923310002121\n",
      "DT_result :               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.83      0.83      5909\n",
      "           1       0.59      0.60      0.60      3685\n",
      "           2       0.52      0.52      0.52      1407\n",
      "\n",
      "    accuracy                           0.71     11001\n",
      "   macro avg       0.65      0.65      0.65     11001\n",
      "weighted avg       0.72      0.71      0.71     11001\n",
      "\n",
      "************ Test_Result ************\n",
      "AVG_acc : 0.7251593823479504\n",
      "DT_result :               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.84      0.84      7458\n",
      "           1       0.60      0.61      0.61      4501\n",
      "           2       0.54      0.53      0.54      1792\n",
      "\n",
      "    accuracy                           0.73     13751\n",
      "   macro avg       0.66      0.66      0.66     13751\n",
      "weighted avg       0.73      0.73      0.73     13751\n",
      "\n",
      "|  dt  Fold  4  Training Start           |\n",
      "************ Val_Result ************\n",
      "AVG_acc : 0.7179120079992728\n",
      "DT_result :               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.83      0.83      5910\n",
      "           1       0.60      0.61      0.61      3684\n",
      "           2       0.53      0.54      0.54      1407\n",
      "\n",
      "    accuracy                           0.72     11001\n",
      "   macro avg       0.66      0.66      0.66     11001\n",
      "weighted avg       0.72      0.72      0.72     11001\n",
      "\n",
      "************ Test_Result ************\n",
      "AVG_acc : 0.7249472765617047\n",
      "DT_result :               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.84      0.84      7458\n",
      "           1       0.60      0.60      0.60      4501\n",
      "           2       0.55      0.54      0.55      1792\n",
      "\n",
      "    accuracy                           0.72     13751\n",
      "   macro avg       0.66      0.66      0.66     13751\n",
      "weighted avg       0.72      0.72      0.72     13751\n",
      "\n",
      "|  dt  Fold  5  Training Start           |\n",
      "************ Val_Result ************\n",
      "AVG_acc : 0.7181114245812364\n",
      "DT_result :               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.84      0.84      5910\n",
      "           1       0.60      0.61      0.61      3684\n",
      "           2       0.50      0.50      0.50      1406\n",
      "\n",
      "    accuracy                           0.72     11000\n",
      "   macro avg       0.65      0.65      0.65     11000\n",
      "weighted avg       0.72      0.72      0.72     11000\n",
      "\n",
      "************ Test_Result ************\n",
      "AVG_acc : 0.7254308777543452\n",
      "DT_result :               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.84      0.84      7458\n",
      "           1       0.61      0.62      0.61      4501\n",
      "           2       0.55      0.54      0.54      1792\n",
      "\n",
      "    accuracy                           0.73     13751\n",
      "   macro avg       0.67      0.67      0.67     13751\n",
      "weighted avg       0.73      0.73      0.73     13751\n",
      "\n",
      "Seed: 888\n",
      "|  Ada  Fold  1  Training Start           |\n",
      "************ Val_Result ************\n",
      "AVG_acc : 0.7457503863285155\n",
      "Ada_result :               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.87      0.85      5910\n",
      "           1       0.63      0.65      0.64      3685\n",
      "           2       0.65      0.48      0.55      1406\n",
      "\n",
      "    accuracy                           0.75     11001\n",
      "   macro avg       0.70      0.67      0.68     11001\n",
      "weighted avg       0.74      0.75      0.74     11001\n",
      "\n",
      "************ Test_Result ************\n",
      "AVG_acc : 0.7426368991346084\n",
      "Ada_result :               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.86      0.85      7458\n",
      "           1       0.62      0.65      0.63      4501\n",
      "           2       0.66      0.47      0.55      1792\n",
      "\n",
      "    accuracy                           0.74     13751\n",
      "   macro avg       0.70      0.66      0.68     13751\n",
      "weighted avg       0.74      0.74      0.74     13751\n",
      "\n",
      "|  Ada  Fold  2  Training Start           |\n",
      "************ Val_Result ************\n",
      "AVG_acc : 0.7469320970820834\n",
      "Ada_result :               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.87      0.85      5909\n",
      "           1       0.63      0.67      0.65      3685\n",
      "           2       0.69      0.46      0.55      1407\n",
      "\n",
      "    accuracy                           0.75     11001\n",
      "   macro avg       0.72      0.66      0.68     11001\n",
      "weighted avg       0.75      0.75      0.74     11001\n",
      "\n",
      "************ Test_Result ************\n",
      "AVG_acc : 0.7431823140135263\n",
      "Ada_result :               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.87      0.85      7458\n",
      "           1       0.61      0.66      0.64      4501\n",
      "           2       0.69      0.44      0.53      1792\n",
      "\n",
      "    accuracy                           0.74     13751\n",
      "   macro avg       0.71      0.66      0.67     13751\n",
      "weighted avg       0.74      0.74      0.74     13751\n",
      "\n",
      "|  Ada  Fold  3  Training Start           |\n",
      "************ Val_Result ************\n",
      "AVG_acc : 0.7444777747477502\n",
      "Ada_result :               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.86      0.84      5909\n",
      "           1       0.62      0.65      0.63      3685\n",
      "           2       0.65      0.46      0.54      1407\n",
      "\n",
      "    accuracy                           0.74     11001\n",
      "   macro avg       0.70      0.66      0.67     11001\n",
      "weighted avg       0.74      0.74      0.74     11001\n",
      "\n",
      "************ Test_Result ************\n",
      "AVG_acc : 0.7438004508763\n",
      "Ada_result :               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.86      0.85      7458\n",
      "           1       0.62      0.66      0.64      4501\n",
      "           2       0.68      0.48      0.56      1792\n",
      "\n",
      "    accuracy                           0.75     13751\n",
      "   macro avg       0.71      0.67      0.68     13751\n",
      "weighted avg       0.74      0.75      0.74     13751\n",
      "\n",
      "|  Ada  Fold  4  Training Start           |\n",
      "************ Val_Result ************\n",
      "AVG_acc : 0.7441596218525589\n",
      "Ada_result :               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.87      0.85      5910\n",
      "           1       0.63      0.64      0.63      3684\n",
      "           2       0.63      0.47      0.54      1407\n",
      "\n",
      "    accuracy                           0.74     11001\n",
      "   macro avg       0.70      0.66      0.67     11001\n",
      "weighted avg       0.74      0.74      0.74     11001\n",
      "\n",
      "************ Test_Result ************\n",
      "AVG_acc : 0.744327685259254\n",
      "Ada_result :               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.87      0.85      7458\n",
      "           1       0.62      0.64      0.63      4501\n",
      "           2       0.67      0.48      0.56      1792\n",
      "\n",
      "    accuracy                           0.75     13751\n",
      "   macro avg       0.71      0.67      0.68     13751\n",
      "weighted avg       0.74      0.75      0.74     13751\n",
      "\n",
      "|  Ada  Fold  5  Training Start           |\n",
      "************ Val_Result ************\n",
      "AVG_acc : 0.7441095156638653\n",
      "Ada_result :               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.87      0.85      5910\n",
      "           1       0.63      0.65      0.64      3684\n",
      "           2       0.66      0.46      0.54      1406\n",
      "\n",
      "    accuracy                           0.74     11000\n",
      "   macro avg       0.70      0.66      0.68     11000\n",
      "weighted avg       0.74      0.74      0.74     11000\n",
      "\n",
      "************ Test_Result ************\n",
      "AVG_acc : 0.7444404043342303\n",
      "Ada_result :               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.87      0.85      7458\n",
      "           1       0.62      0.64      0.63      4501\n",
      "           2       0.69      0.48      0.56      1792\n",
      "\n",
      "    accuracy                           0.74     13751\n",
      "   macro avg       0.71      0.66      0.68     13751\n",
      "weighted avg       0.74      0.74      0.74     13751\n",
      "\n",
      "Seed: 888\n",
      "|  rf  Fold  1  Training Start           |\n",
      "************ Val_Result ************\n",
      "AVG_acc : 0.7856558494682302\n",
      "RF_result :               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.89      0.88      5910\n",
      "           1       0.68      0.71      0.70      3685\n",
      "           2       0.66      0.56      0.60      1406\n",
      "\n",
      "    accuracy                           0.79     11001\n",
      "   macro avg       0.74      0.72      0.73     11001\n",
      "weighted avg       0.78      0.79      0.78     11001\n",
      "\n",
      "************ Test_Result ************\n",
      "AVG_acc : 0.7947785615591594\n",
      "RF_result :               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.89      0.89      7458\n",
      "           1       0.68      0.73      0.70      4501\n",
      "           2       0.69      0.58      0.63      1792\n",
      "\n",
      "    accuracy                           0.79     13751\n",
      "   macro avg       0.75      0.73      0.74     13751\n",
      "weighted avg       0.79      0.79      0.79     13751\n",
      "\n",
      "|  rf  Fold  2  Training Start           |\n",
      "************ Val_Result ************\n",
      "AVG_acc : 0.7883374238705572\n",
      "RF_result :               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.89      0.89      5909\n",
      "           1       0.69      0.72      0.70      3685\n",
      "           2       0.67      0.58      0.62      1407\n",
      "\n",
      "    accuracy                           0.79     11001\n",
      "   macro avg       0.75      0.73      0.74     11001\n",
      "weighted avg       0.79      0.79      0.79     11001\n",
      "\n",
      "************ Test_Result ************\n",
      "AVG_acc : 0.7939058977528908\n",
      "RF_result :               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.89      0.89      7458\n",
      "           1       0.68      0.72      0.70      4501\n",
      "           2       0.69      0.58      0.63      1792\n",
      "\n",
      "    accuracy                           0.79     13751\n",
      "   macro avg       0.75      0.73      0.74     13751\n",
      "weighted avg       0.79      0.79      0.79     13751\n",
      "\n",
      "|  rf  Fold  3  Training Start           |\n",
      "************ Val_Result ************\n",
      "AVG_acc : 0.7880798715268309\n",
      "RF_result :               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.89      0.88      5909\n",
      "           1       0.68      0.72      0.70      3685\n",
      "           2       0.68      0.56      0.62      1407\n",
      "\n",
      "    accuracy                           0.79     11001\n",
      "   macro avg       0.75      0.72      0.73     11001\n",
      "weighted avg       0.79      0.79      0.79     11001\n",
      "\n",
      "************ Test_Result ************\n",
      "AVG_acc : 0.7936877318013237\n",
      "RF_result :               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.89      0.89      7458\n",
      "           1       0.68      0.72      0.70      4501\n",
      "           2       0.69      0.57      0.62      1792\n",
      "\n",
      "    accuracy                           0.79     13751\n",
      "   macro avg       0.75      0.73      0.74     13751\n",
      "weighted avg       0.79      0.79      0.79     13751\n",
      "\n",
      "|  rf  Fold  4  Training Start           |\n",
      "************ Val_Result ************\n",
      "AVG_acc : 0.788087446595764\n",
      "RF_result :               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.88      0.88      5910\n",
      "           1       0.68      0.72      0.70      3684\n",
      "           2       0.67      0.56      0.61      1407\n",
      "\n",
      "    accuracy                           0.79     11001\n",
      "   macro avg       0.74      0.72      0.73     11001\n",
      "weighted avg       0.79      0.79      0.79     11001\n",
      "\n",
      "************ Test_Result ************\n",
      "AVG_acc : 0.7928877899789107\n",
      "RF_result :               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.88      0.88      7458\n",
      "           1       0.68      0.72      0.70      4501\n",
      "           2       0.69      0.58      0.63      1792\n",
      "\n",
      "    accuracy                           0.79     13751\n",
      "   macro avg       0.75      0.73      0.74     13751\n",
      "weighted avg       0.79      0.79      0.79     13751\n",
      "\n",
      "|  rf  Fold  5  Training Start           |\n",
      "************ Val_Result ************\n",
      "AVG_acc : 0.7887972300038839\n",
      "RF_result :               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.88      0.89      5910\n",
      "           1       0.69      0.73      0.71      3684\n",
      "           2       0.66      0.57      0.61      1406\n",
      "\n",
      "    accuracy                           0.79     11000\n",
      "   macro avg       0.75      0.73      0.73     11000\n",
      "weighted avg       0.79      0.79      0.79     11000\n",
      "\n",
      "************ Test_Result ************\n",
      "AVG_acc : 0.7925096356628609\n",
      "RF_result :               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.89      0.89      7458\n",
      "           1       0.68      0.72      0.70      4501\n",
      "           2       0.69      0.57      0.62      1792\n",
      "\n",
      "    accuracy                           0.79     13751\n",
      "   macro avg       0.75      0.73      0.74     13751\n",
      "weighted avg       0.79      0.79      0.79     13751\n",
      "\n",
      "Seed: 888\n",
      "|  Gnb  Fold  1  Training Start           |\n",
      "************ Val_Result ************\n",
      "AVG_acc : 0.5835833106081265\n",
      "Gnb_result :               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.94      0.74      5910\n",
      "           1       0.54      0.02      0.03      3685\n",
      "           2       0.44      0.56      0.49      1406\n",
      "\n",
      "    accuracy                           0.58     11001\n",
      "   macro avg       0.53      0.51      0.42     11001\n",
      "weighted avg       0.56      0.58      0.47     11001\n",
      "\n",
      "************ Test_Result ************\n",
      "AVG_acc : 0.591302450730856\n",
      "Gnb_result :               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.94      0.74      7458\n",
      "           1       0.55      0.02      0.03      4501\n",
      "           2       0.47      0.57      0.52      1792\n",
      "\n",
      "    accuracy                           0.59     13751\n",
      "   macro avg       0.54      0.51      0.43     13751\n",
      "weighted avg       0.57      0.59      0.48     13751\n",
      "\n",
      "|  Gnb  Fold  2  Training Start           |\n",
      "************ Val_Result ************\n",
      "AVG_acc : 0.5769020998091082\n",
      "Gnb_result :               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.89      0.75      5909\n",
      "           1       0.48      0.01      0.02      3685\n",
      "           2       0.36      0.69      0.47      1407\n",
      "\n",
      "    accuracy                           0.57     11001\n",
      "   macro avg       0.49      0.53      0.41     11001\n",
      "weighted avg       0.55      0.57      0.47     11001\n",
      "\n",
      "************ Test_Result ************\n",
      "AVG_acc : 0.5833757544905825\n",
      "Gnb_result :               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.89      0.74      7458\n",
      "           1       0.48      0.01      0.02      4501\n",
      "           2       0.38      0.69      0.49      1792\n",
      "\n",
      "    accuracy                           0.58     13751\n",
      "   macro avg       0.50      0.53      0.42     13751\n",
      "weighted avg       0.55      0.58      0.47     13751\n",
      "\n",
      "|  Gnb  Fold  3  Training Start           |\n",
      "************ Val_Result ************\n",
      "AVG_acc : 0.5803714813804804\n",
      "Gnb_result :               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.94      0.75      5909\n",
      "           1       0.63      0.01      0.03      3685\n",
      "           2       0.43      0.61      0.51      1407\n",
      "\n",
      "    accuracy                           0.59     11001\n",
      "   macro avg       0.56      0.52      0.43     11001\n",
      "weighted avg       0.60      0.59      0.48     11001\n",
      "\n",
      "************ Test_Result ************\n",
      "AVG_acc : 0.5856058953288246\n",
      "Gnb_result :               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.93      0.75      7458\n",
      "           1       0.51      0.01      0.02      4501\n",
      "           2       0.45      0.62      0.52      1792\n",
      "\n",
      "    accuracy                           0.59     13751\n",
      "   macro avg       0.52      0.52      0.43     13751\n",
      "weighted avg       0.56      0.59      0.48     13751\n",
      "\n",
      "|  Gnb  Fold  4  Training Start           |\n",
      "************ Val_Result ************\n",
      "AVG_acc : 0.5807426597582037\n",
      "Gnb_result :               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.94      0.74      5910\n",
      "           1       0.59      0.02      0.04      3684\n",
      "           2       0.43      0.56      0.49      1407\n",
      "\n",
      "    accuracy                           0.58     11001\n",
      "   macro avg       0.55      0.50      0.42     11001\n",
      "weighted avg       0.58      0.58      0.47     11001\n",
      "\n",
      "************ Test_Result ************\n",
      "AVG_acc : 0.5868482292196932\n",
      "Gnb_result :               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.94      0.74      7458\n",
      "           1       0.54      0.02      0.04      4501\n",
      "           2       0.46      0.57      0.51      1792\n",
      "\n",
      "    accuracy                           0.59     13751\n",
      "   macro avg       0.54      0.51      0.43     13751\n",
      "weighted avg       0.57      0.59      0.48     13751\n",
      "\n",
      "|  Gnb  Fold  5  Training Start           |\n",
      "************ Val_Result ************\n",
      "AVG_acc : 0.583085036897472\n",
      "Gnb_result :               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.92      0.75      5910\n",
      "           1       0.50      0.07      0.13      3684\n",
      "           2       0.44      0.57      0.50      1406\n",
      "\n",
      "    accuracy                           0.59     11000\n",
      "   macro avg       0.52      0.52      0.46     11000\n",
      "weighted avg       0.56      0.59      0.51     11000\n",
      "\n",
      "************ Test_Result ************\n",
      "AVG_acc : 0.5888735364700749\n",
      "Gnb_result :               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.92      0.75      7458\n",
      "           1       0.48      0.07      0.13      4501\n",
      "           2       0.46      0.56      0.50      1792\n",
      "\n",
      "    accuracy                           0.60     13751\n",
      "   macro avg       0.52      0.52      0.46     13751\n",
      "weighted avg       0.56      0.60      0.51     13751\n",
      "\n",
      "Seed: 888\n",
      "|  knn  Fold  1  Training Start           |\n",
      "************ Val_Result ************\n",
      "AVG_acc : 0.5994909553676938\n",
      "Knn_result :               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.78      0.72      5910\n",
      "           1       0.45      0.38      0.41      3685\n",
      "           2       0.58      0.40      0.47      1406\n",
      "\n",
      "    accuracy                           0.60     11001\n",
      "   macro avg       0.57      0.52      0.54     11001\n",
      "weighted avg       0.58      0.60      0.59     11001\n",
      "\n",
      "************ Test_Result ************\n",
      "AVG_acc : 0.6022107483092138\n",
      "Knn_result :               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.78      0.73      7458\n",
      "           1       0.44      0.39      0.41      4501\n",
      "           2       0.60      0.40      0.48      1792\n",
      "\n",
      "    accuracy                           0.60     13751\n",
      "   macro avg       0.57      0.52      0.54     13751\n",
      "weighted avg       0.59      0.60      0.59     13751\n",
      "\n",
      "|  knn  Fold  2  Training Start           |\n",
      "************ Val_Result ************\n",
      "AVG_acc : 0.601763476047632\n",
      "Knn_result :               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.79      0.73      5909\n",
      "           1       0.46      0.39      0.42      3685\n",
      "           2       0.59      0.40      0.47      1407\n",
      "\n",
      "    accuracy                           0.60     11001\n",
      "   macro avg       0.57      0.52      0.54     11001\n",
      "weighted avg       0.59      0.60      0.59     11001\n",
      "\n",
      "************ Test_Result ************\n",
      "AVG_acc : 0.6055923205585048\n",
      "Knn_result :               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.79      0.73      7458\n",
      "           1       0.45      0.39      0.42      4501\n",
      "           2       0.61      0.41      0.49      1792\n",
      "\n",
      "    accuracy                           0.61     13751\n",
      "   macro avg       0.58      0.53      0.55     13751\n",
      "weighted avg       0.60      0.61      0.60     13751\n",
      "\n",
      "|  knn  Fold  3  Training Start           |\n",
      "************ Val_Result ************\n",
      "AVG_acc : 0.6039147956246401\n",
      "Knn_result :               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.79      0.73      5909\n",
      "           1       0.46      0.39      0.43      3685\n",
      "           2       0.60      0.42      0.49      1407\n",
      "\n",
      "    accuracy                           0.61     11001\n",
      "   macro avg       0.58      0.53      0.55     11001\n",
      "weighted avg       0.59      0.61      0.60     11001\n",
      "\n",
      "************ Test_Result ************\n",
      "AVG_acc : 0.6054105155988655\n",
      "Knn_result :               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.79      0.73      7458\n",
      "           1       0.44      0.38      0.41      4501\n",
      "           2       0.60      0.40      0.48      1792\n",
      "\n",
      "    accuracy                           0.61     13751\n",
      "   macro avg       0.57      0.52      0.54     13751\n",
      "weighted avg       0.59      0.61      0.59     13751\n",
      "\n",
      "|  knn  Fold  4  Training Start           |\n",
      "************ Val_Result ************\n",
      "AVG_acc : 0.6012635214980456\n",
      "Knn_result :               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.78      0.72      5910\n",
      "           1       0.44      0.37      0.40      3684\n",
      "           2       0.58      0.37      0.45      1407\n",
      "\n",
      "    accuracy                           0.59     11001\n",
      "   macro avg       0.56      0.51      0.53     11001\n",
      "weighted avg       0.58      0.59      0.58     11001\n",
      "\n",
      "************ Test_Result ************\n",
      "AVG_acc : 0.6061922769253145\n",
      "Knn_result :               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.79      0.73      7458\n",
      "           1       0.45      0.40      0.42      4501\n",
      "           2       0.61      0.41      0.49      1792\n",
      "\n",
      "    accuracy                           0.61     13751\n",
      "   macro avg       0.58      0.53      0.55     13751\n",
      "weighted avg       0.60      0.61      0.60     13751\n",
      "\n",
      "|  knn  Fold  5  Training Start           |\n",
      "************ Val_Result ************\n",
      "AVG_acc : 0.6023380899257093\n",
      "Knn_result :               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.79      0.73      5910\n",
      "           1       0.46      0.40      0.43      3684\n",
      "           2       0.59      0.39      0.47      1406\n",
      "\n",
      "    accuracy                           0.61     11000\n",
      "   macro avg       0.58      0.53      0.54     11000\n",
      "weighted avg       0.59      0.61      0.59     11000\n",
      "\n",
      "************ Test_Result ************\n",
      "AVG_acc : 0.6063559013889899\n",
      "Knn_result :               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.79      0.73      7458\n",
      "           1       0.45      0.40      0.42      4501\n",
      "           2       0.60      0.40      0.48      1792\n",
      "\n",
      "    accuracy                           0.61     13751\n",
      "   macro avg       0.58      0.53      0.54     13751\n",
      "weighted avg       0.59      0.61      0.60     13751\n",
      "\n",
      "Seed: 888\n",
      "|  Mlp Fold  1  Training Start           |\n",
      "************ Val_Result ************\n",
      "AVG_acc : 0.7444777747477502\n",
      "Mlp_result :               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.86      0.84      5910\n",
      "           1       0.63      0.63      0.63      3685\n",
      "           2       0.68      0.55      0.61      1406\n",
      "\n",
      "    accuracy                           0.74     11001\n",
      "   macro avg       0.71      0.68      0.69     11001\n",
      "weighted avg       0.74      0.74      0.74     11001\n",
      "\n",
      "************ Test_Result ************\n",
      "AVG_acc : 0.7414733473929169\n",
      "Mlp_result :               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.86      0.84      7458\n",
      "           1       0.61      0.62      0.62      4501\n",
      "           2       0.70      0.56      0.62      1792\n",
      "\n",
      "    accuracy                           0.74     13751\n",
      "   macro avg       0.71      0.68      0.69     13751\n",
      "weighted avg       0.74      0.74      0.74     13751\n",
      "\n",
      "|  Mlp Fold  2  Training Start           |\n",
      "************ Val_Result ************\n",
      "AVG_acc : 0.732978820107263\n",
      "Mlp_result :               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.83      0.82      5909\n",
      "           1       0.59      0.61      0.60      3685\n",
      "           2       0.68      0.54      0.60      1407\n",
      "\n",
      "    accuracy                           0.72     11001\n",
      "   macro avg       0.69      0.66      0.68     11001\n",
      "weighted avg       0.72      0.72      0.72     11001\n",
      "\n",
      "************ Test_Result ************\n",
      "AVG_acc : 0.7323467384190241\n",
      "Mlp_result :               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.84      0.83      7458\n",
      "           1       0.58      0.60      0.59      4501\n",
      "           2       0.69      0.54      0.61      1792\n",
      "\n",
      "    accuracy                           0.72     13751\n",
      "   macro avg       0.70      0.66      0.68     13751\n",
      "weighted avg       0.72      0.72      0.72     13751\n",
      "\n",
      "|  Mlp Fold  3  Training Start           |\n",
      "************ Val_Result ************\n",
      "AVG_acc : 0.7315395570099689\n",
      "Mlp_result :               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.86      0.84      5909\n",
      "           1       0.61      0.58      0.59      3685\n",
      "           2       0.64      0.57      0.60      1407\n",
      "\n",
      "    accuracy                           0.73     11001\n",
      "   macro avg       0.69      0.67      0.68     11001\n",
      "weighted avg       0.72      0.73      0.72     11001\n",
      "\n",
      "************ Test_Result ************\n",
      "AVG_acc : 0.73269822800766\n",
      "Mlp_result :               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.86      0.83      7458\n",
      "           1       0.61      0.57      0.59      4501\n",
      "           2       0.67      0.61      0.64      1792\n",
      "\n",
      "    accuracy                           0.73     13751\n",
      "   macro avg       0.70      0.68      0.69     13751\n",
      "weighted avg       0.73      0.73      0.73     13751\n",
      "\n",
      "|  Mlp Fold  4  Training Start           |\n",
      "************ Val_Result ************\n",
      "AVG_acc : 0.7307972002545223\n",
      "Mlp_result :               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.87      0.83      5910\n",
      "           1       0.62      0.57      0.59      3684\n",
      "           2       0.67      0.56      0.61      1407\n",
      "\n",
      "    accuracy                           0.73     11001\n",
      "   macro avg       0.69      0.66      0.68     11001\n",
      "weighted avg       0.72      0.73      0.72     11001\n",
      "\n",
      "************ Test_Result ************\n",
      "AVG_acc : 0.7340375245436696\n",
      "Mlp_result :               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.87      0.84      7458\n",
      "           1       0.62      0.58      0.60      4501\n",
      "           2       0.70      0.57      0.63      1792\n",
      "\n",
      "    accuracy                           0.74     13751\n",
      "   macro avg       0.71      0.68      0.69     13751\n",
      "weighted avg       0.73      0.74      0.73     13751\n",
      "\n",
      "|  Mlp Fold  5  Training Start           |\n",
      "************ Val_Result ************\n",
      "AVG_acc : 0.7311104874763451\n",
      "Mlp_result :               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.87      0.84      5910\n",
      "           1       0.63      0.55      0.59      3684\n",
      "           2       0.64      0.62      0.63      1406\n",
      "\n",
      "    accuracy                           0.73     11000\n",
      "   macro avg       0.69      0.68      0.68     11000\n",
      "weighted avg       0.72      0.73      0.73     11000\n",
      "\n",
      "************ Test_Result ************\n",
      "AVG_acc : 0.734099338229947\n",
      "Mlp_result :               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.87      0.84      7458\n",
      "           1       0.62      0.55      0.58      4501\n",
      "           2       0.66      0.62      0.64      1792\n",
      "\n",
      "    accuracy                           0.73     13751\n",
      "   macro avg       0.69      0.68      0.69     13751\n",
      "weighted avg       0.73      0.73      0.73     13751\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#构建模型\n",
    "import optuna\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from catboost import CatBoostClassifier \n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from pytorch_tabnet import tab_model\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold,KFold\n",
    "from sklearn.metrics import roc_auc_score,accuracy_score,classification_report\n",
    "from sklearn.preprocessing import StandardScaler,MinMaxScaler\n",
    "import torch\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# # 标准化\n",
    "# def scale(train_features,test_features):\n",
    "#     scaler=StandardScaler()\n",
    "#     scaler.fit(train_features)\n",
    "#     train_features=pd.DataFrame(scaler.transform(train_features),columns=test_features.keys())\n",
    "#     test_features=pd.DataFrame(scaler.transform(test_features),columns=test_features.keys())\n",
    "#     return train_features,test_features\n",
    "\n",
    "# 模型定义\n",
    "def ml_model(clf,train_x,train_y,test_x=[],test_y=[]):\n",
    "    seeds=[888]\n",
    "    train_oof = np.zeros([train_x.shape[0],3])\n",
    "    feat_imp_df = pd.DataFrame()\n",
    "    feat_imp_df['feature'] = train_x.columns\n",
    "    feat_imp_df['imp'] = 0\n",
    "    #标准化\n",
    "    scaler=StandardScaler()\n",
    "    scaler.fit(train_x)\n",
    "    train_x = scaler.transform(train_x)\n",
    "    if (len(test_x)!=0) and (len(test_x)!=0):\n",
    "        test_oof = np.zeros([test_x.shape[0],3])\n",
    "        test_x = scaler.transform(test_x)\n",
    "    for seed in seeds:\n",
    "        print('Seed:',seed)\n",
    "        folds = 5\n",
    "        kf = StratifiedKFold(n_splits=folds, shuffle=True, random_state=seed)\n",
    "        acc_scores_val = []\n",
    "        acc_scores_test = []\n",
    "        for i, (train_index, valid_index) in enumerate(kf.split(train_x, train_y)):\n",
    "            trn_x, trn_y, val_x, val_y = train_x[train_index], train_y[train_index], train_x[valid_index], \\\n",
    "                                        train_y[valid_index] \n",
    "            if clf == 'xgb':\n",
    "                print(\"|  XGB  Fold  {}  Training Start           |\".format(str(i + 1)))\n",
    "                xgb_params = {'booster': 'gbtree','objective': 'multi:softprob','eval_metric':'mlogloss','num_class':3,\n",
    "                    'n_estimators':500,'max_depth': 8,'lambda': 10,'subsample': 0.7,'colsample_bytree': 0.8,'eta': 0.1,\n",
    "                    'colsample_bylevel': 0.7,'tree_method': 'hist','seed': seed,'nthread': 16}\n",
    "                #训练模型\n",
    "                model = xgb.XGBClassifier(*xgb_params)\n",
    "                model.fit(trn_x,trn_y,eval_set=[(trn_x, trn_y),(val_x,val_y)],early_stopping_rounds=50,verbose=100)\n",
    "                #验证集\n",
    "                print('************ Val_Result ************')\n",
    "                val_pred  = model.predict_proba(val_x)\n",
    "                acc_score_val = accuracy_score(val_y, np.argmax(val_pred, axis=1))\n",
    "                acc_scores_val.append(acc_score_val)\n",
    "                print('AVG_acc :',sum(acc_scores_val)/len(acc_scores_val))\n",
    "                print('XGB_result :',classification_report(val_y, np.argmax(val_pred, axis=1)))\n",
    "                #测试集\n",
    "                if (len(test_x)!=0) and (len(test_x)!=0):\n",
    "                    print('************ Test_Result ************')\n",
    "                    test_pred  = model.predict_proba(test_x)\n",
    "                    acc_score_tset = accuracy_score(test_y, np.argmax(test_pred, axis=1))\n",
    "                    acc_scores_test.append(acc_score_tset)\n",
    "                    print('AVG_acc :',sum(acc_scores_test)/len(acc_scores_test))\n",
    "                    print('XGB_result :',classification_report(test_y, np.argmax(test_pred, axis=1)))\n",
    "                    #保存测试集结果\n",
    "                    test_oof += test_pred / kf.n_splits / len(seeds)\n",
    "                \n",
    "                #保存训练集结果\n",
    "                train_oof[valid_index] = val_pred / kf.n_splits / len(seeds)\n",
    "                \n",
    "                #模型特征重要性\n",
    "                feat_imp_df['imp'] += model.feature_importances_ / folds/ len(seeds)\n",
    "                feat_imp_df = feat_imp_df.sort_values(by='imp', ascending=False).reset_index(drop=True)\n",
    "                feat_imp_df['rank'] = range(feat_imp_df.shape[0])\n",
    "            if clf == 'lgb':\n",
    "                lgb_params = {'boosting_type': 'gbdt','n_estimators':500,'min_child_weight': 4,'num_leaves': 64,\n",
    "                    'feature_fraction': 0.8,'bagging_fraction': 0.8,'bagging_freq': 4,'learning_rate': 0.02,\n",
    "                    'seed': seed,'nthread': 32,'n_jobs':8,'verbose': -1}\n",
    "                print(\"|  LGB  Fold  {}  Training Start           |\".format(str(i + 1)))\n",
    "                #训练模型\n",
    "                model = lgb.LGBMClassifier(**lgb_params)\n",
    "                model.fit(trn_x,trn_y)\n",
    "                \n",
    "                #验证集\n",
    "                print('************ Val_Result ************')\n",
    "                val_pred  = model.predict_proba(val_x)\n",
    "                acc_score_val = accuracy_score(val_y, np.argmax(val_pred, axis=1))\n",
    "                acc_scores_val.append(acc_score_val)\n",
    "                print('AVG_acc :',sum(acc_scores_val)/len(acc_scores_val))\n",
    "                print('Cat_result :',classification_report(val_y, np.argmax(val_pred, axis=1)))\n",
    "                #测试集\n",
    "                if (len(test_x)!=0) and (len(test_x)!=0):\n",
    "                    print('************ Test_Result ************')\n",
    "                    test_pred  = model.predict_proba(test_x)\n",
    "                    acc_score_tset = accuracy_score(test_y, np.argmax(test_pred, axis=1))\n",
    "                    acc_scores_test.append(acc_score_tset)\n",
    "                    print('AVG_acc :',sum(acc_scores_test)/len(acc_scores_test))\n",
    "                    print('Cat_result :',classification_report(test_y, np.argmax(test_pred, axis=1))) \n",
    "                    #保存测试集结果\n",
    "                    test_oof += test_pred / kf.n_splits / len(seeds)\n",
    "                #保存训练集结果\n",
    "                train_oof[valid_index] = val_pred / kf.n_splits / len(seeds)\n",
    "            if clf == 'cat':\n",
    "                print(\"|  CAT  Fold  {}  Training Start           |\".format(str(i + 1)))\n",
    "                #训练模型\n",
    "                model = CatBoostClassifier(verbose=False)\n",
    "                model.fit(trn_x,trn_y)\n",
    "                    \n",
    "                #验证集\n",
    "                print('************ Val_Result ************')\n",
    "                val_pred  = model.predict_proba(val_x)\n",
    "                acc_score_val = accuracy_score(val_y, np.argmax(val_pred, axis=1))\n",
    "                acc_scores_val.append(acc_score_val)\n",
    "                print('AVG_acc :',sum(acc_scores_val)/len(acc_scores_val))\n",
    "                print('Cat_result :',classification_report(val_y, np.argmax(val_pred, axis=1)))\n",
    "                #测试集\n",
    "                if (len(test_x)!=0) and (len(test_x)!=0):\n",
    "                    print('************ Test_Result ************')\n",
    "                    test_pred  = model.predict_proba(test_x)\n",
    "                    acc_score_tset = accuracy_score(test_y, np.argmax(test_pred, axis=1))\n",
    "                    acc_scores_test.append(acc_score_tset)\n",
    "                    print('AVG_acc :',sum(acc_scores_test)/len(acc_scores_test))\n",
    "                    print('Cat_result :',classification_report(test_y, np.argmax(test_pred, axis=1))) \n",
    "                    #保存测试集结果\n",
    "                    test_oof += test_pred / kf.n_splits / len(seeds)\n",
    "                    \n",
    "                #保存结果\n",
    "                train_oof[valid_index] = val_pred / kf.n_splits / len(seeds)\n",
    "            if clf == 'dt':\n",
    "                print(\"|  DT  Fold  {}  Training Start           |\".format(str(i + 1)))\n",
    "                #训练模型\n",
    "                model = DecisionTreeClassifier()\n",
    "                model.fit(trn_x,trn_y)\n",
    "                    \n",
    "                #验证集\n",
    "                print('************ Val_Result ************')\n",
    "                val_pred  = model.predict_proba(val_x)\n",
    "                acc_score_val = accuracy_score(val_y, np.argmax(val_pred, axis=1))\n",
    "                acc_scores_val.append(acc_score_val)\n",
    "                print('AVG_acc :',sum(acc_scores_val)/len(acc_scores_val))\n",
    "                print('DT_result :',classification_report(val_y, np.argmax(val_pred, axis=1)))\n",
    "                #测试集\n",
    "                if (len(test_x)!=0) and (len(test_x)!=0):\n",
    "                    print('************ Test_Result ************')\n",
    "                    test_pred  = model.predict_proba(test_x)\n",
    "                    acc_score_tset = accuracy_score(test_y, np.argmax(test_pred, axis=1))\n",
    "                    acc_scores_test.append(acc_score_tset)\n",
    "                    print('AVG_acc :',sum(acc_scores_test)/len(acc_scores_test))\n",
    "                    print('DT_result :',classification_report(test_y, np.argmax(test_pred, axis=1))) \n",
    "                    #保存测试集结果\n",
    "                    test_oof += test_pred / kf.n_splits / len(seeds)\n",
    "            if clf == 'Ada':\n",
    "                print(\"|  ADA  Fold  {}  Training Start           |\".format(str(i + 1)))\n",
    "                #训练模型\n",
    "                model = AdaBoostClassifier()\n",
    "                model.fit(trn_x,trn_y)\n",
    "                    \n",
    "                #验证集\n",
    "                print('************ Val_Result ************')\n",
    "                val_pred  = model.predict_proba(val_x)\n",
    "                acc_score_val = accuracy_score(val_y, np.argmax(val_pred, axis=1))\n",
    "                acc_scores_val.append(acc_score_val)\n",
    "                print('AVG_acc :',sum(acc_scores_val)/len(acc_scores_val))\n",
    "                print('Ada_result :',classification_report(val_y, np.argmax(val_pred, axis=1)))\n",
    "                #测试集\n",
    "                if (len(test_x)!=0) and (len(test_x)!=0):\n",
    "                    print('************ Test_Result ************')\n",
    "                    test_pred  = model.predict_proba(test_x)\n",
    "                    acc_score_tset = accuracy_score(test_y, np.argmax(test_pred, axis=1))\n",
    "                    acc_scores_test.append(acc_score_tset)\n",
    "                    print('AVG_acc :',sum(acc_scores_test)/len(acc_scores_test))\n",
    "                    print('Ada_result :',classification_report(test_y, np.argmax(test_pred, axis=1))) \n",
    "                    #保存测试集结果\n",
    "                    test_oof += test_pred / kf.n_splits / len(seeds)\n",
    "            if clf == 'rf':\n",
    "                print(\"|  RF  Fold  {}  Training Start           |\".format(str(i + 1)))\n",
    "                #训练模型\n",
    "                model = RandomForestClassifier()\n",
    "                model.fit(trn_x,trn_y)\n",
    "                    \n",
    "                #验证集\n",
    "                print('************ Val_Result ************')\n",
    "                val_pred  = model.predict_proba(val_x)\n",
    "                acc_score_val = accuracy_score(val_y, np.argmax(val_pred, axis=1))\n",
    "                acc_scores_val.append(acc_score_val)\n",
    "                print('AVG_acc :',sum(acc_scores_val)/len(acc_scores_val))\n",
    "                print('RF_result :',classification_report(val_y, np.argmax(val_pred, axis=1)))\n",
    "                #测试集\n",
    "                if (len(test_x)!=0) and (len(test_x)!=0):\n",
    "                    print('************ Test_Result ************')\n",
    "                    test_pred  = model.predict_proba(test_x)\n",
    "                    acc_score_tset = accuracy_score(test_y, np.argmax(test_pred, axis=1))\n",
    "                    acc_scores_test.append(acc_score_tset)\n",
    "                    print('AVG_acc :',sum(acc_scores_test)/len(acc_scores_test))\n",
    "                    print('RF_result :',classification_report(test_y, np.argmax(test_pred, axis=1))) \n",
    "                    #保存测试集结果\n",
    "                    test_oof += test_pred / kf.n_splits / len(seeds)    \n",
    "                #保存结果\n",
    "                train_oof[valid_index] = val_pred / kf.n_splits / len(seeds)\n",
    "            if clf == 'Gnb':\n",
    "                print(\"|  GNB  Fold  {}  Training Start           |\".format(str(i + 1)))\n",
    "                #训练模型\n",
    "                model = GaussianNB()\n",
    "                model.fit(trn_x,trn_y)\n",
    "                    \n",
    "                #验证集\n",
    "                print('************ Val_Result ************')\n",
    "                val_pred  = model.predict_proba(val_x)\n",
    "                acc_score_val = accuracy_score(val_y, np.argmax(val_pred, axis=1))\n",
    "                acc_scores_val.append(acc_score_val)\n",
    "                print('AVG_acc :',sum(acc_scores_val)/len(acc_scores_val))\n",
    "                print('Gnb_result :',classification_report(val_y, np.argmax(val_pred, axis=1)))\n",
    "                #测试集\n",
    "                if (len(test_x)!=0) and (len(test_x)!=0):\n",
    "                    print('************ Test_Result ************')\n",
    "                    test_pred  = model.predict_proba(test_x)\n",
    "                    acc_score_tset = accuracy_score(test_y, np.argmax(test_pred, axis=1))\n",
    "                    acc_scores_test.append(acc_score_tset)\n",
    "                    print('AVG_acc :',sum(acc_scores_test)/len(acc_scores_test))\n",
    "                    print('Gnb_result :',classification_report(test_y, np.argmax(test_pred, axis=1))) \n",
    "                    #保存测试集结果\n",
    "                    test_oof += test_pred / kf.n_splits / len(seeds)\n",
    "            if clf == 'svm':\n",
    "                print(\"|  SVM  Fold  {}  Training Start           |\".format(str(i + 1)))\n",
    "                #训练模型\n",
    "                model = SVC(kernel='rbf', C=1, gamma='auto', probability=True,max_iter=1000)\n",
    "                model.fit(trn_x,trn_y)\n",
    "                \n",
    "                #验证集\n",
    "                print('************ Val_Result ************')\n",
    "                val_pred  = model.predict_proba(val_x)\n",
    "                acc_score_val = accuracy_score(val_y, np.argmax(val_pred, axis=1))\n",
    "                acc_scores_val.append(acc_score_val)\n",
    "                print('AVG_acc :',sum(acc_scores_val)/len(acc_scores_val))\n",
    "                print('Svm_result :',classification_report(val_y, np.argmax(val_pred, axis=1)))\n",
    "                #测试集\n",
    "                print('************ Test_Result ************')\n",
    "                test_pred  = model.predict_proba(test_x)\n",
    "                acc_score_tset = accuracy_score(test_y, np.argmax(test_pred, axis=1))\n",
    "                acc_scores_test.append(acc_score_tset)\n",
    "                print('AVG_acc :',sum(acc_scores_test)/len(acc_scores_test))\n",
    "                print('Svm_result :',classification_report(test_y, np.argmax(test_pred, axis=1))) \n",
    "                #保存结果\n",
    "                train_oof[valid_index] = val_pred / kf.n_splits / len(seeds)\n",
    "                test_oof += test_pred / kf.n_splits / len(seeds)   \n",
    "            if clf == 'knn':\n",
    "                print(\"|  KNN  Fold  {}  Training Start           |\".format(str(i + 1)))\n",
    "                #训练模型\n",
    "                model = KNeighborsClassifier()\n",
    "                model.fit(trn_x,trn_y)\n",
    "                \n",
    "                #验证集\n",
    "                print('************ Val_Result ************')\n",
    "                val_pred  = model.predict_proba(val_x)\n",
    "                acc_score_val = accuracy_score(val_y, np.argmax(val_pred, axis=1))\n",
    "                acc_scores_val.append(acc_score_val)\n",
    "                print('AVG_acc :',sum(acc_scores_val)/len(acc_scores_val))\n",
    "                print('Knn_result :',classification_report(val_y, np.argmax(val_pred, axis=1)))\n",
    "                #测试集\n",
    "                print('************ Test_Result ************')\n",
    "                test_pred  = model.predict_proba(test_x)\n",
    "                acc_score_tset = accuracy_score(test_y, np.argmax(test_pred, axis=1))\n",
    "                acc_scores_test.append(acc_score_tset)\n",
    "                print('AVG_acc :',sum(acc_scores_test)/len(acc_scores_test))\n",
    "                print('Knn_result :',classification_report(test_y, np.argmax(test_pred, axis=1))) \n",
    "                #保存结果\n",
    "                train_oof[valid_index] = val_pred / kf.n_splits / len(seeds)\n",
    "                test_oof += test_pred / kf.n_splits / len(seeds)    \n",
    "            if clf == 'tabnet':\n",
    "                print(f\"     Tab_model  Fold {i+1}  Training Starting       \")\n",
    "                if torch.cuda.is_available():\n",
    "                    print(\"Using GPU\")\n",
    "                    device = \"cuda\"\n",
    "                else:\n",
    "                    print(\"Using CPU\")\n",
    "                    device = \"cpu\"\n",
    "                    \n",
    "                torch.manual_seed(seed)\n",
    "                np.random.seed(seed)\n",
    "                model = tab_model.TabNetClassifier()\n",
    "\n",
    "                model.fit(trn_x, trn_y,eval_set=[(val_x, val_y)],eval_metric=['accuracy'])\n",
    "                #验证集\n",
    "                print('************ Val_Result ************')\n",
    "                val_pred  = model.predict_proba(val_x)\n",
    "                acc_score_val = accuracy_score(val_y, np.argmax(val_pred, axis=1))\n",
    "                acc_scores_val.append(acc_score_val)\n",
    "                print('AVG_acc :',sum(acc_scores_val)/len(acc_scores_val))\n",
    "                print('Tabnet_result :',classification_report(val_y, np.argmax(val_pred, axis=1)))\n",
    "                #测试集\n",
    "                print('************ Test_Result ************')\n",
    "                test_pred  = model.predict_proba(test_x)\n",
    "                acc_score_tset = accuracy_score(test_y, np.argmax(test_pred, axis=1))\n",
    "                acc_scores_test.append(acc_score_tset)\n",
    "                print('AVG_acc :',sum(acc_scores_test)/len(acc_scores_test))\n",
    "                print('Tabnet_result :',classification_report(test_y, np.argmax(test_pred, axis=1))) \n",
    "                #保存结果\n",
    "                train_oof[valid_index] = val_pred / kf.n_splits / len(seeds)\n",
    "                test_oof += test_pred / kf.n_splits / len(seeds)  \n",
    "            if clf == 'Mlp':\n",
    "                print(\"|  MLP Fold  {}  Training Start           |\".format(str(i + 1)))\n",
    "                #训练模型\n",
    "                model = MLPClassifier()\n",
    "                model.fit(trn_x,trn_y)\n",
    "                \n",
    "                #验证集\n",
    "                print('************ Val_Result ************')\n",
    "                val_pred  = model.predict_proba(val_x)\n",
    "                acc_score_val = accuracy_score(val_y, np.argmax(val_pred, axis=1))\n",
    "                acc_scores_val.append(acc_score_val)\n",
    "                print('AVG_acc :',sum(acc_scores_val)/len(acc_scores_val))\n",
    "                print('Mlp_result :',classification_report(val_y, np.argmax(val_pred, axis=1)))\n",
    "                #测试集\n",
    "                print('************ Test_Result ************')\n",
    "                test_pred  = model.predict_proba(test_x)\n",
    "                acc_score_tset = accuracy_score(test_y, np.argmax(test_pred, axis=1))\n",
    "                acc_scores_test.append(acc_score_tset)\n",
    "                print('AVG_acc :',sum(acc_scores_test)/len(acc_scores_test))\n",
    "                print('Mlp_result :',classification_report(test_y, np.argmax(test_pred, axis=1))) \n",
    "                #保存结果\n",
    "                train_oof[valid_index] = val_pred / kf.n_splits / len(seeds)\n",
    "                test_oof += test_pred / kf.n_splits / len(seeds)  \n",
    "        if (len(test_x)!=0) and (len(test_x)!=0):\n",
    "            return train_oof,test_oof,model,scaler\n",
    "        else:\n",
    "            return train_oof,model,scaler\n",
    "\n",
    "# # 训练 XGB模型\n",
    "# xgb_train_oof_1,xgb_test_oof_1, xgb_model_1,scaler_1 = ml_model('xgb',train_df[feature_cols], train_df['label'],test_df[feature_cols], test_df['label'])\n",
    "\n",
    "# # 训练 LGB模型\n",
    "# lgb_train_oof_1,lgb_test_oof_1,lgb_model_1,scaler_1 = ml_model('lgb',train_df[feature_cols], train_df['label'],test_df[feature_cols], test_df['label'])\n",
    "\n",
    "# # 训练 CAT模型\n",
    "# cat_train_oof_1,cat_test_oof_1,cat_model_1,scaler_1 = ml_model('cat',train_df[feature_cols], train_df['label'],test_df[feature_cols], test_df['label'])\n",
    "\n",
    "# # 训练 SVM模型\n",
    "# svm_train_oof_1,svm_test_oof_1,svm_model_1,scaler_1 = ml_model('svm',train_df[feature_cols], train_df['label'],test_df[feature_cols], test_df['label'])\n",
    "\n",
    "# # 训练 Tabnet模型\n",
    "# tab_train_oof_1,tab_test_oof_1,tab_model_1,scaler_1 = ml_model('tabnet',train_df[feature_cols], train_df['label'],test_df[feature_cols], test_df['label'])\n",
    "\n",
    "# 训练 DT模型\n",
    "dt_train_oof_1,dt_test_oof_1, dt_model_1,scaler_1 = ml_model('dt',train_df[feature_cols], train_df['label'],test_df[feature_cols], test_df['label'])\n",
    "\n",
    "# 训练 Ada模型\n",
    "Ada_train_oof_1,Ada_test_oof_1,Ada_model_1,scaler_1 = ml_model('Ada',train_df[feature_cols], train_df['label'],test_df[feature_cols], test_df['label'])\n",
    "\n",
    "# 训练 RF模型\n",
    "rf_train_oof_1,rf_test_oof_1,rf_model_1,scaler_1 = ml_model('rf',train_df[feature_cols], train_df['label'],test_df[feature_cols], test_df['label'])\n",
    "\n",
    "# 训练 Gnb模型\n",
    "Gnb_train_oof_1,Gnb_test_oof_1,Gnb_model_1,scaler_1 = ml_model('Gnb',train_df[feature_cols], train_df['label'],test_df[feature_cols], test_df['label'])\n",
    "\n",
    "# 训练 Knn模型\n",
    "knn_train_oof_1,knn_test_oof_1,knn_model_1,scaler_1 = ml_model('knn',train_df[feature_cols], train_df['label'],test_df[feature_cols], test_df['label'])\n",
    "\n",
    "# 训练 Mlp模型\n",
    "Mlp_train_oof_1,Mlp_test_oof_1,Mlp_model_1,scaler_1 = ml_model('Mlp',train_df[feature_cols], train_df['label'],test_df[feature_cols], test_df['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed: 888\n",
      "|  XGB  Fold  1  Training Start           |\n",
      "[0]\tvalidation_0-mlogloss:0.96997\tvalidation_1-mlogloss:0.98364\n",
      "[99]\tvalidation_0-mlogloss:0.20272\tvalidation_1-mlogloss:0.62778\n",
      "************ Val_Result ************\n",
      "AVG_acc : 0.7344444444444445\n",
      "XGB_result :               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.68      0.67       550\n",
      "           1       0.84      0.84      0.84       772\n",
      "           2       0.65      0.62      0.64       478\n",
      "\n",
      "    accuracy                           0.73      1800\n",
      "   macro avg       0.72      0.71      0.72      1800\n",
      "weighted avg       0.73      0.73      0.73      1800\n",
      "\n",
      "|  XGB  Fold  2  Training Start           |\n",
      "[0]\tvalidation_0-mlogloss:0.97274\tvalidation_1-mlogloss:0.98713\n",
      "[99]\tvalidation_0-mlogloss:0.20816\tvalidation_1-mlogloss:0.62838\n",
      "************ Val_Result ************\n",
      "AVG_acc : 0.7380555555555556\n",
      "XGB_result :               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.65      0.68       549\n",
      "           1       0.81      0.85      0.83       773\n",
      "           2       0.66      0.67      0.67       478\n",
      "\n",
      "    accuracy                           0.74      1800\n",
      "   macro avg       0.73      0.72      0.72      1800\n",
      "weighted avg       0.74      0.74      0.74      1800\n",
      "\n",
      "|  XGB  Fold  3  Training Start           |\n",
      "[0]\tvalidation_0-mlogloss:0.96537\tvalidation_1-mlogloss:0.97914\n",
      "[99]\tvalidation_0-mlogloss:0.19456\tvalidation_1-mlogloss:0.60322\n",
      "************ Val_Result ************\n",
      "AVG_acc : 0.7396296296296296\n",
      "XGB_result :               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.68      0.69       550\n",
      "           1       0.82      0.85      0.83       773\n",
      "           2       0.66      0.63      0.65       477\n",
      "\n",
      "    accuracy                           0.74      1800\n",
      "   macro avg       0.73      0.72      0.72      1800\n",
      "weighted avg       0.74      0.74      0.74      1800\n",
      "\n",
      "|  XGB  Fold  4  Training Start           |\n",
      "[0]\tvalidation_0-mlogloss:0.96640\tvalidation_1-mlogloss:0.98179\n",
      "[99]\tvalidation_0-mlogloss:0.20520\tvalidation_1-mlogloss:0.63447\n",
      "************ Val_Result ************\n",
      "AVG_acc : 0.7433333333333334\n",
      "XGB_result :               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.69      0.70       550\n",
      "           1       0.83      0.84      0.84       773\n",
      "           2       0.68      0.68      0.68       477\n",
      "\n",
      "    accuracy                           0.75      1800\n",
      "   macro avg       0.74      0.74      0.74      1800\n",
      "weighted avg       0.75      0.75      0.75      1800\n",
      "\n",
      "|  XGB  Fold  5  Training Start           |\n",
      "[0]\tvalidation_0-mlogloss:0.96742\tvalidation_1-mlogloss:0.98601\n",
      "[99]\tvalidation_0-mlogloss:0.20029\tvalidation_1-mlogloss:0.62611\n",
      "************ Val_Result ************\n",
      "AVG_acc : 0.7400807856216417\n",
      "XGB_result :               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.65      0.66       550\n",
      "           1       0.83      0.84      0.83       772\n",
      "           2       0.63      0.64      0.64       477\n",
      "\n",
      "    accuracy                           0.73      1799\n",
      "   macro avg       0.71      0.71      0.71      1799\n",
      "weighted avg       0.73      0.73      0.73      1799\n",
      "\n",
      "Seed: 888\n",
      "|  LGB  Fold  1  Training Start           |\n",
      "************ Val_Result ************\n",
      "AVG_acc : 0.7455555555555555\n",
      "Cat_result :               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.68      0.68       550\n",
      "           1       0.84      0.85      0.85       772\n",
      "           2       0.67      0.65      0.66       478\n",
      "\n",
      "    accuracy                           0.75      1800\n",
      "   macro avg       0.73      0.73      0.73      1800\n",
      "weighted avg       0.74      0.75      0.75      1800\n",
      "\n",
      "|  LGB  Fold  2  Training Start           |\n",
      "************ Val_Result ************\n",
      "AVG_acc : 0.7494444444444444\n",
      "Cat_result :               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.69      0.69       549\n",
      "           1       0.82      0.87      0.85       773\n",
      "           2       0.69      0.64      0.66       478\n",
      "\n",
      "    accuracy                           0.75      1800\n",
      "   macro avg       0.74      0.73      0.73      1800\n",
      "weighted avg       0.75      0.75      0.75      1800\n",
      "\n",
      "|  LGB  Fold  3  Training Start           |\n",
      "************ Val_Result ************\n",
      "AVG_acc : 0.7498148148148148\n",
      "Cat_result :               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.70      0.70       550\n",
      "           1       0.82      0.87      0.84       773\n",
      "           2       0.67      0.62      0.65       477\n",
      "\n",
      "    accuracy                           0.75      1800\n",
      "   macro avg       0.73      0.73      0.73      1800\n",
      "weighted avg       0.75      0.75      0.75      1800\n",
      "\n",
      "|  LGB  Fold  4  Training Start           |\n",
      "************ Val_Result ************\n",
      "AVG_acc : 0.7509722222222222\n",
      "Cat_result :               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.68      0.70       550\n",
      "           1       0.82      0.85      0.83       773\n",
      "           2       0.69      0.69      0.69       477\n",
      "\n",
      "    accuracy                           0.75      1800\n",
      "   macro avg       0.74      0.74      0.74      1800\n",
      "weighted avg       0.75      0.75      0.75      1800\n",
      "\n",
      "|  LGB  Fold  5  Training Start           |\n",
      "************ Val_Result ************\n",
      "AVG_acc : 0.7484153542091285\n",
      "Cat_result :               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.68      0.68       550\n",
      "           1       0.82      0.85      0.84       772\n",
      "           2       0.65      0.63      0.64       477\n",
      "\n",
      "    accuracy                           0.74      1799\n",
      "   macro avg       0.72      0.72      0.72      1799\n",
      "weighted avg       0.74      0.74      0.74      1799\n",
      "\n",
      "Seed: 888\n",
      "|  cat  Fold  1  Training Start           |\n",
      "************ Val_Result ************\n",
      "AVG_acc : 0.74\n",
      "Cat_result :               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.68      0.67       550\n",
      "           1       0.83      0.85      0.84       772\n",
      "           2       0.67      0.63      0.65       478\n",
      "\n",
      "    accuracy                           0.74      1800\n",
      "   macro avg       0.72      0.72      0.72      1800\n",
      "weighted avg       0.74      0.74      0.74      1800\n",
      "\n",
      "|  cat  Fold  2  Training Start           |\n",
      "************ Val_Result ************\n",
      "AVG_acc : 0.745\n",
      "Cat_result :               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.67      0.68       549\n",
      "           1       0.82      0.87      0.84       773\n",
      "           2       0.69      0.65      0.67       478\n",
      "\n",
      "    accuracy                           0.75      1800\n",
      "   macro avg       0.73      0.73      0.73      1800\n",
      "weighted avg       0.75      0.75      0.75      1800\n",
      "\n",
      "|  cat  Fold  3  Training Start           |\n",
      "************ Val_Result ************\n",
      "AVG_acc : 0.7437037037037038\n",
      "Cat_result :               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.70      0.69       550\n",
      "           1       0.82      0.85      0.83       773\n",
      "           2       0.66      0.62      0.64       477\n",
      "\n",
      "    accuracy                           0.74      1800\n",
      "   macro avg       0.72      0.72      0.72      1800\n",
      "weighted avg       0.74      0.74      0.74      1800\n",
      "\n",
      "|  cat  Fold  4  Training Start           |\n",
      "************ Val_Result ************\n",
      "AVG_acc : 0.7445833333333334\n",
      "Cat_result :               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.68      0.70       550\n",
      "           1       0.80      0.84      0.82       773\n",
      "           2       0.69      0.67      0.68       477\n",
      "\n",
      "    accuracy                           0.75      1800\n",
      "   macro avg       0.73      0.73      0.73      1800\n",
      "weighted avg       0.74      0.75      0.75      1800\n",
      "\n",
      "|  cat  Fold  5  Training Start           |\n",
      "************ Val_Result ************\n",
      "AVG_acc : 0.7429707244765611\n",
      "Cat_result :               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.67      0.67       550\n",
      "           1       0.82      0.85      0.83       772\n",
      "           2       0.66      0.63      0.64       477\n",
      "\n",
      "    accuracy                           0.74      1799\n",
      "   macro avg       0.72      0.72      0.72      1799\n",
      "weighted avg       0.73      0.74      0.73      1799\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#训练集困难样本处理\n",
    "df_pre = pd.DataFrame()\n",
    "df_pre['xgb_pre'] = np.argmax(xgb_train_oof_1,axis=1)\n",
    "df_pre['lgb_pre'] = np.argmax(lgb_train_oof_1,axis=1)\n",
    "df_pre['cat_pre'] = np.argmax(cat_train_oof_1,axis=1)\n",
    "df_pre['label'] = train_df['label']\n",
    "\n",
    "grade_list = []\n",
    "for row in df_pre.itertuples():\n",
    "    grade = 0\n",
    "    if getattr(row,'xgb_pre') == getattr(row,'label'):\n",
    "        grade += 1\n",
    "    if getattr(row,'lgb_pre') == getattr(row,'label'):\n",
    "        grade += 1\n",
    "    if getattr(row,'cat_pre') == getattr(row,'label'):\n",
    "        grade += 1\n",
    "    grade_list.append(grade)\n",
    "\n",
    "#困难样本识别\n",
    "df_pre['grade'] = grade_list\n",
    "train_hard_index = df_pre.loc[(df_pre['grade']==0)].index\n",
    "\n",
    "train_hard_df = train_df.loc[train_hard_index].reset_index(drop=True)\n",
    "xgb_train_hard_oof, xgb_train_hard_model,scaler_2 = ml_model('xgb',train_hard_df[feature_cols],train_hard_df['label'])\n",
    "lgb_train_hard_oof, lgb_train_hard_model,scaler_2 = ml_model('lgb',train_hard_df[feature_cols],train_hard_df['label'])\n",
    "cat_train_hard_oof, cat_train_hard_model,scaler_2 = ml_model('cat',train_hard_df[feature_cols],train_hard_df['label'])\n",
    "\n",
    "#训练集替换困难样本结果\n",
    "xgb_train_oof_2 = np.argmax(xgb_train_oof_1,axis=1)\n",
    "xgb_train_oof_2[train_hard_index]=np.argmax(xgb_train_hard_oof,axis=1)\n",
    "\n",
    "lgb_train_oof_2 = np.argmax(lgb_train_oof_1,axis=1)\n",
    "lgb_train_oof_2[train_hard_index]=np.argmax(lgb_train_hard_oof,axis=1)\n",
    "\n",
    "cat_train_oof_2 = np.argmax(cat_train_oof_1,axis=1)\n",
    "cat_train_oof_2[train_hard_index]=np.argmax(cat_train_hard_oof,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test_hard result :\n",
      "xgb_test_hard result: 0.7598797250859106\n",
      "lgb_test_hard result: 0.7800687285223368\n",
      "cat_test_hard result: 0.7736254295532646\n"
     ]
    }
   ],
   "source": [
    "#测试集困难样本处理\n",
    "df_pre = pd.DataFrame()\n",
    "df_pre['xgb_pre'] = np.argmax(xgb_test_oof_1,axis=1)\n",
    "df_pre['lgb_pre'] = np.argmax(lgb_test_oof_1,axis=1)\n",
    "df_pre['cat_pre'] = np.argmax(cat_test_oof_1,axis=1).flatten()\n",
    "df_pre['label'] = test_df['label']\n",
    "\n",
    "grade_list = []\n",
    "for row in df_pre.itertuples():\n",
    "    grade = 0\n",
    "    if getattr(row,'xgb_pre') == getattr(row,'label'):\n",
    "        grade += 1\n",
    "    if getattr(row,'lgb_pre') == getattr(row,'label'):\n",
    "        grade += 1\n",
    "    if getattr(row,'cat_pre') == getattr(row,'label'):\n",
    "        grade += 1\n",
    "    grade_list.append(grade)\n",
    "\n",
    "#测试集替换困难样本结果\n",
    "df_pre['grade'] = grade_list\n",
    "test_hard_index = df_pre.loc[(df_pre['grade']==0)].index\n",
    "test_hard_df = test_df.loc[test_hard_index].reset_index(drop=True)\n",
    "\n",
    "print('Test_hard result :')\n",
    "xgb_test_hard_oof = xgb_train_hard_model.predict(scaler_2.transform(test_hard_df[feature_cols]))\n",
    "print('xgb_test_hard result:',accuracy_score(xgb_test_hard_oof,test_hard_df['label']))\n",
    "lgb_test_hard_oof = lgb_train_hard_model.predict(scaler_2.transform(test_hard_df[feature_cols]))\n",
    "print('lgb_test_hard result:',accuracy_score(lgb_test_hard_oof,test_hard_df['label']))\n",
    "cat_test_hard_oof = cat_train_hard_model.predict(scaler_2.transform(test_hard_df[feature_cols]))\n",
    "print('cat_test_hard result:',accuracy_score(cat_test_hard_oof,test_hard_df['label']))\n",
    "\n",
    "\n",
    "xgb_test_oof_2 = np.argmax(xgb_test_oof_1,axis=1)\n",
    "xgb_test_oof_2[test_hard_index]=xgb_test_hard_oof\n",
    "\n",
    "lgb_test_oof_2 = np.argmax(lgb_test_oof_1,axis=1)\n",
    "lgb_test_oof_2[test_hard_index]=lgb_test_hard_oof\n",
    "\n",
    "cat_test_oof_2 = np.argmax(cat_test_oof_1,axis=1)\n",
    "cat_test_oof_2[test_hard_index]=cat_test_hard_oof.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed: 888\n",
      "     Tab_model  Fold 1  Training Starting       \n",
      "Using CPU\n",
      "epoch 0  | loss: 0.54512 | val_0_accuracy: 0.91737 |  0:00:03s\n",
      "epoch 1  | loss: 0.28215 | val_0_accuracy: 0.92328 |  0:00:07s\n",
      "epoch 2  | loss: 0.2617  | val_0_accuracy: 0.92237 |  0:00:10s\n",
      "epoch 3  | loss: 0.24948 | val_0_accuracy: 0.92383 |  0:00:14s\n",
      "epoch 4  | loss: 0.24828 | val_0_accuracy: 0.9231  |  0:00:18s\n",
      "epoch 5  | loss: 0.24792 | val_0_accuracy: 0.92292 |  0:00:21s\n",
      "epoch 6  | loss: 0.24348 | val_0_accuracy: 0.92264 |  0:00:25s\n",
      "epoch 7  | loss: 0.24308 | val_0_accuracy: 0.92283 |  0:00:28s\n",
      "epoch 8  | loss: 0.24088 | val_0_accuracy: 0.92364 |  0:00:32s\n",
      "epoch 9  | loss: 0.24043 | val_0_accuracy: 0.92373 |  0:00:35s\n",
      "epoch 10 | loss: 0.23849 | val_0_accuracy: 0.92455 |  0:00:39s\n",
      "epoch 11 | loss: 0.23753 | val_0_accuracy: 0.92446 |  0:00:43s\n",
      "epoch 12 | loss: 0.23354 | val_0_accuracy: 0.92473 |  0:00:46s\n",
      "epoch 13 | loss: 0.23128 | val_0_accuracy: 0.92383 |  0:00:50s\n",
      "epoch 14 | loss: 0.23101 | val_0_accuracy: 0.92428 |  0:00:53s\n",
      "epoch 15 | loss: 0.22951 | val_0_accuracy: 0.9231  |  0:00:57s\n",
      "epoch 16 | loss: 0.22903 | val_0_accuracy: 0.92373 |  0:01:00s\n",
      "epoch 17 | loss: 0.22679 | val_0_accuracy: 0.9241  |  0:01:04s\n",
      "epoch 18 | loss: 0.23069 | val_0_accuracy: 0.92083 |  0:01:07s\n",
      "epoch 19 | loss: 0.23827 | val_0_accuracy: 0.92446 |  0:01:11s\n",
      "epoch 20 | loss: 0.22725 | val_0_accuracy: 0.92183 |  0:01:15s\n",
      "epoch 21 | loss: 0.22788 | val_0_accuracy: 0.9231  |  0:01:18s\n",
      "epoch 22 | loss: 0.22465 | val_0_accuracy: 0.92401 |  0:01:22s\n",
      "\n",
      "Early stopping occurred at epoch 22 with best_epoch = 12 and best_val_0_accuracy = 0.92473\n",
      "************ Val_Result ************\n",
      "AVG_acc : 0.9247341150804472\n",
      "Tabnet_result :               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.95      0.95      5910\n",
      "           1       0.90      0.93      0.92      3685\n",
      "           2       0.84      0.81      0.83      1406\n",
      "\n",
      "    accuracy                           0.92     11001\n",
      "   macro avg       0.90      0.90      0.90     11001\n",
      "weighted avg       0.92      0.92      0.92     11001\n",
      "\n",
      "************ Test_Result ************\n",
      "AVG_acc : 0.9387680895934841\n",
      "Tabnet_result :               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.96      0.96      7458\n",
      "           1       0.92      0.95      0.93      4501\n",
      "           2       0.88      0.84      0.86      1792\n",
      "\n",
      "    accuracy                           0.94     13751\n",
      "   macro avg       0.92      0.92      0.92     13751\n",
      "weighted avg       0.94      0.94      0.94     13751\n",
      "\n",
      "     Tab_model  Fold 2  Training Starting       \n",
      "Using CPU\n",
      "epoch 0  | loss: 0.54327 | val_0_accuracy: 0.92464 |  0:00:03s\n",
      "epoch 1  | loss: 0.27113 | val_0_accuracy: 0.92492 |  0:00:07s\n",
      "epoch 2  | loss: 0.25439 | val_0_accuracy: 0.92619 |  0:00:10s\n",
      "epoch 3  | loss: 0.24845 | val_0_accuracy: 0.92473 |  0:00:14s\n",
      "epoch 4  | loss: 0.24415 | val_0_accuracy: 0.92483 |  0:00:17s\n",
      "epoch 5  | loss: 0.24221 | val_0_accuracy: 0.92446 |  0:00:21s\n",
      "epoch 6  | loss: 0.2402  | val_0_accuracy: 0.92582 |  0:00:24s\n",
      "epoch 7  | loss: 0.24235 | val_0_accuracy: 0.92619 |  0:00:28s\n",
      "epoch 8  | loss: 0.23794 | val_0_accuracy: 0.92292 |  0:00:32s\n",
      "epoch 9  | loss: 0.23526 | val_0_accuracy: 0.92464 |  0:00:35s\n",
      "epoch 10 | loss: 0.23243 | val_0_accuracy: 0.92383 |  0:00:39s\n",
      "epoch 11 | loss: 0.22973 | val_0_accuracy: 0.92437 |  0:00:42s\n",
      "epoch 12 | loss: 0.22766 | val_0_accuracy: 0.92419 |  0:00:46s\n",
      "\n",
      "Early stopping occurred at epoch 12 with best_epoch = 2 and best_val_0_accuracy = 0.92619\n",
      "************ Val_Result ************\n",
      "AVG_acc : 0.9254613216980274\n",
      "Tabnet_result :               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.96      0.96      5909\n",
      "           1       0.92      0.91      0.92      3685\n",
      "           2       0.83      0.83      0.83      1407\n",
      "\n",
      "    accuracy                           0.93     11001\n",
      "   macro avg       0.90      0.90      0.90     11001\n",
      "weighted avg       0.93      0.93      0.93     11001\n",
      "\n",
      "************ Test_Result ************\n",
      "AVG_acc : 0.9390953385208349\n",
      "Tabnet_result :               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.96      0.96      7458\n",
      "           1       0.93      0.94      0.93      4501\n",
      "           2       0.86      0.85      0.86      1792\n",
      "\n",
      "    accuracy                           0.94     13751\n",
      "   macro avg       0.92      0.92      0.92     13751\n",
      "weighted avg       0.94      0.94      0.94     13751\n",
      "\n",
      "     Tab_model  Fold 3  Training Starting       \n",
      "Using CPU\n",
      "epoch 0  | loss: 0.51906 | val_0_accuracy: 0.92164 |  0:00:03s\n",
      "epoch 1  | loss: 0.28345 | val_0_accuracy: 0.92337 |  0:00:07s\n",
      "epoch 2  | loss: 0.26261 | val_0_accuracy: 0.92801 |  0:00:10s\n",
      "epoch 3  | loss: 0.25602 | val_0_accuracy: 0.92573 |  0:00:14s\n",
      "epoch 4  | loss: 0.25283 | val_0_accuracy: 0.92919 |  0:00:17s\n",
      "epoch 5  | loss: 0.24944 | val_0_accuracy: 0.92546 |  0:00:21s\n",
      "epoch 6  | loss: 0.24326 | val_0_accuracy: 0.92501 |  0:00:25s\n",
      "epoch 7  | loss: 0.24422 | val_0_accuracy: 0.92437 |  0:00:28s\n",
      "epoch 8  | loss: 0.24307 | val_0_accuracy: 0.92664 |  0:00:32s\n",
      "epoch 9  | loss: 0.24731 | val_0_accuracy: 0.92673 |  0:00:35s\n",
      "epoch 10 | loss: 0.24202 | val_0_accuracy: 0.92555 |  0:00:39s\n",
      "epoch 11 | loss: 0.23974 | val_0_accuracy: 0.92764 |  0:00:43s\n",
      "epoch 12 | loss: 0.2351  | val_0_accuracy: 0.9261  |  0:00:46s\n",
      "epoch 13 | loss: 0.23545 | val_0_accuracy: 0.92646 |  0:00:50s\n",
      "epoch 14 | loss: 0.23408 | val_0_accuracy: 0.92601 |  0:00:53s\n",
      "\n",
      "Early stopping occurred at epoch 14 with best_epoch = 4 and best_val_0_accuracy = 0.92919\n",
      "************ Val_Result ************\n",
      "AVG_acc : 0.9267036330030604\n",
      "Tabnet_result :               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.95      0.95      5909\n",
      "           1       0.92      0.92      0.92      3685\n",
      "           2       0.84      0.84      0.84      1407\n",
      "\n",
      "    accuracy                           0.93     11001\n",
      "   macro avg       0.91      0.91      0.91     11001\n",
      "weighted avg       0.93      0.93      0.93     11001\n",
      "\n",
      "************ Test_Result ************\n",
      "AVG_acc : 0.939761956706179\n",
      "Tabnet_result :               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.96      0.96      7458\n",
      "           1       0.93      0.94      0.94      4501\n",
      "           2       0.87      0.85      0.86      1792\n",
      "\n",
      "    accuracy                           0.94     13751\n",
      "   macro avg       0.92      0.92      0.92     13751\n",
      "weighted avg       0.94      0.94      0.94     13751\n",
      "\n",
      "     Tab_model  Fold 4  Training Starting       \n",
      "Using CPU\n",
      "epoch 0  | loss: 0.51792 | val_0_accuracy: 0.92555 |  0:00:03s\n",
      "epoch 1  | loss: 0.28873 | val_0_accuracy: 0.92537 |  0:00:07s\n",
      "epoch 2  | loss: 0.2601  | val_0_accuracy: 0.92764 |  0:00:10s\n",
      "epoch 3  | loss: 0.25338 | val_0_accuracy: 0.92737 |  0:00:14s\n",
      "epoch 4  | loss: 0.24948 | val_0_accuracy: 0.92682 |  0:00:17s\n",
      "epoch 5  | loss: 0.25187 | val_0_accuracy: 0.92601 |  0:00:21s\n",
      "epoch 6  | loss: 0.24899 | val_0_accuracy: 0.92582 |  0:00:24s\n",
      "epoch 7  | loss: 0.24515 | val_0_accuracy: 0.92619 |  0:00:28s\n",
      "epoch 8  | loss: 0.24325 | val_0_accuracy: 0.92782 |  0:00:32s\n",
      "epoch 9  | loss: 0.24203 | val_0_accuracy: 0.92646 |  0:00:36s\n",
      "epoch 10 | loss: 0.24279 | val_0_accuracy: 0.92619 |  0:00:39s\n",
      "epoch 11 | loss: 0.24514 | val_0_accuracy: 0.92728 |  0:00:43s\n",
      "epoch 12 | loss: 0.2443  | val_0_accuracy: 0.92673 |  0:00:46s\n",
      "epoch 13 | loss: 0.24197 | val_0_accuracy: 0.92828 |  0:00:50s\n",
      "epoch 14 | loss: 0.23909 | val_0_accuracy: 0.92682 |  0:00:54s\n",
      "epoch 15 | loss: 0.23794 | val_0_accuracy: 0.92764 |  0:00:57s\n",
      "epoch 16 | loss: 0.23881 | val_0_accuracy: 0.92728 |  0:01:01s\n",
      "epoch 17 | loss: 0.23937 | val_0_accuracy: 0.92782 |  0:01:04s\n",
      "epoch 18 | loss: 0.2372  | val_0_accuracy: 0.92746 |  0:01:08s\n",
      "epoch 19 | loss: 0.23653 | val_0_accuracy: 0.92873 |  0:01:12s\n",
      "epoch 20 | loss: 0.23637 | val_0_accuracy: 0.92655 |  0:01:15s\n",
      "epoch 21 | loss: 0.23513 | val_0_accuracy: 0.92792 |  0:01:19s\n",
      "epoch 22 | loss: 0.23335 | val_0_accuracy: 0.92855 |  0:01:22s\n",
      "epoch 23 | loss: 0.23257 | val_0_accuracy: 0.92801 |  0:01:26s\n",
      "epoch 24 | loss: 0.23227 | val_0_accuracy: 0.92701 |  0:01:30s\n",
      "epoch 25 | loss: 0.23226 | val_0_accuracy: 0.92901 |  0:01:33s\n",
      "epoch 26 | loss: 0.2364  | val_0_accuracy: 0.92746 |  0:01:37s\n",
      "epoch 27 | loss: 0.23608 | val_0_accuracy: 0.92864 |  0:01:40s\n",
      "epoch 28 | loss: 0.23642 | val_0_accuracy: 0.92592 |  0:01:44s\n",
      "epoch 29 | loss: 0.2355  | val_0_accuracy: 0.92801 |  0:01:47s\n",
      "epoch 30 | loss: 0.23381 | val_0_accuracy: 0.92728 |  0:01:51s\n",
      "epoch 31 | loss: 0.23084 | val_0_accuracy: 0.92892 |  0:01:55s\n",
      "epoch 32 | loss: 0.23104 | val_0_accuracy: 0.92846 |  0:01:58s\n",
      "epoch 33 | loss: 0.23035 | val_0_accuracy: 0.92628 |  0:02:02s\n",
      "epoch 34 | loss: 0.22927 | val_0_accuracy: 0.92946 |  0:02:05s\n",
      "epoch 35 | loss: 0.23005 | val_0_accuracy: 0.92973 |  0:02:09s\n",
      "epoch 36 | loss: 0.23016 | val_0_accuracy: 0.92564 |  0:02:12s\n",
      "epoch 37 | loss: 0.22785 | val_0_accuracy: 0.92882 |  0:02:16s\n",
      "epoch 38 | loss: 0.22636 | val_0_accuracy: 0.92837 |  0:02:20s\n",
      "epoch 39 | loss: 0.2256  | val_0_accuracy: 0.92837 |  0:02:23s\n",
      "epoch 40 | loss: 0.22835 | val_0_accuracy: 0.92828 |  0:02:27s\n",
      "epoch 41 | loss: 0.22888 | val_0_accuracy: 0.92801 |  0:02:31s\n",
      "epoch 42 | loss: 0.22967 | val_0_accuracy: 0.92782 |  0:02:34s\n",
      "epoch 43 | loss: 0.22726 | val_0_accuracy: 0.92628 |  0:02:38s\n",
      "epoch 44 | loss: 0.22572 | val_0_accuracy: 0.92846 |  0:02:41s\n",
      "epoch 45 | loss: 0.22638 | val_0_accuracy: 0.92855 |  0:02:45s\n",
      "\n",
      "Early stopping occurred at epoch 45 with best_epoch = 35 and best_val_0_accuracy = 0.92973\n",
      "************ Val_Result ************\n",
      "AVG_acc : 0.927461139896373\n",
      "Tabnet_result :               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.96      0.96      5910\n",
      "           1       0.93      0.91      0.92      3684\n",
      "           2       0.85      0.84      0.84      1407\n",
      "\n",
      "    accuracy                           0.93     11001\n",
      "   macro avg       0.91      0.90      0.91     11001\n",
      "weighted avg       0.93      0.93      0.93     11001\n",
      "\n",
      "************ Test_Result ************\n",
      "AVG_acc : 0.9397134753836085\n",
      "Tabnet_result :               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.97      0.96      7458\n",
      "           1       0.94      0.93      0.93      4501\n",
      "           2       0.86      0.85      0.86      1792\n",
      "\n",
      "    accuracy                           0.94     13751\n",
      "   macro avg       0.92      0.92      0.92     13751\n",
      "weighted avg       0.94      0.94      0.94     13751\n",
      "\n",
      "     Tab_model  Fold 5  Training Starting       \n",
      "Using CPU\n",
      "epoch 0  | loss: 0.54548 | val_0_accuracy: 0.92209 |  0:00:03s\n",
      "epoch 1  | loss: 0.2814  | val_0_accuracy: 0.922   |  0:00:07s\n",
      "epoch 2  | loss: 0.25741 | val_0_accuracy: 0.92064 |  0:00:10s\n",
      "epoch 3  | loss: 0.24399 | val_0_accuracy: 0.92409 |  0:00:14s\n",
      "epoch 4  | loss: 0.23867 | val_0_accuracy: 0.92427 |  0:00:18s\n",
      "epoch 5  | loss: 0.23736 | val_0_accuracy: 0.92173 |  0:00:21s\n",
      "epoch 6  | loss: 0.2396  | val_0_accuracy: 0.92309 |  0:00:25s\n",
      "epoch 7  | loss: 0.23805 | val_0_accuracy: 0.925   |  0:00:28s\n",
      "epoch 8  | loss: 0.23592 | val_0_accuracy: 0.92473 |  0:00:32s\n",
      "epoch 9  | loss: 0.23438 | val_0_accuracy: 0.92491 |  0:00:36s\n",
      "epoch 10 | loss: 0.23234 | val_0_accuracy: 0.92436 |  0:00:39s\n",
      "epoch 11 | loss: 0.23072 | val_0_accuracy: 0.92473 |  0:00:43s\n",
      "epoch 12 | loss: 0.23222 | val_0_accuracy: 0.92509 |  0:00:46s\n",
      "epoch 13 | loss: 0.23053 | val_0_accuracy: 0.92491 |  0:00:50s\n",
      "epoch 14 | loss: 0.22898 | val_0_accuracy: 0.92536 |  0:00:54s\n",
      "epoch 15 | loss: 0.22773 | val_0_accuracy: 0.92427 |  0:00:57s\n",
      "epoch 16 | loss: 0.22813 | val_0_accuracy: 0.92264 |  0:01:01s\n",
      "epoch 17 | loss: 0.22816 | val_0_accuracy: 0.92418 |  0:01:04s\n",
      "epoch 18 | loss: 0.22745 | val_0_accuracy: 0.92427 |  0:01:08s\n",
      "epoch 19 | loss: 0.22817 | val_0_accuracy: 0.924   |  0:01:11s\n",
      "epoch 20 | loss: 0.22584 | val_0_accuracy: 0.92327 |  0:01:15s\n",
      "epoch 21 | loss: 0.22542 | val_0_accuracy: 0.92409 |  0:01:19s\n",
      "epoch 22 | loss: 0.22474 | val_0_accuracy: 0.923   |  0:01:22s\n",
      "epoch 23 | loss: 0.22452 | val_0_accuracy: 0.92455 |  0:01:26s\n",
      "epoch 24 | loss: 0.22574 | val_0_accuracy: 0.92536 |  0:01:30s\n",
      "\n",
      "Early stopping occurred at epoch 24 with best_epoch = 14 and best_val_0_accuracy = 0.92536\n",
      "************ Val_Result ************\n",
      "AVG_acc : 0.9270416391898257\n",
      "Tabnet_result :               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.94      0.95      5910\n",
      "           1       0.90      0.94      0.92      3684\n",
      "           2       0.83      0.83      0.83      1406\n",
      "\n",
      "    accuracy                           0.93     11000\n",
      "   macro avg       0.90      0.90      0.90     11000\n",
      "weighted avg       0.93      0.93      0.93     11000\n",
      "\n",
      "************ Test_Result ************\n",
      "AVG_acc : 0.939669842193295\n",
      "Tabnet_result :               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.96      0.96      7458\n",
      "           1       0.92      0.95      0.94      4501\n",
      "           2       0.88      0.84      0.86      1792\n",
      "\n",
      "    accuracy                           0.94     13751\n",
      "   macro avg       0.92      0.92      0.92     13751\n",
      "weighted avg       0.94      0.94      0.94     13751\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#xgb预测结果作为新特征(替换困难样本)\n",
    "train_df['xgb_pre'] = xgb_train_oof_2\n",
    "test_df['xgb_pre'] = xgb_test_oof_2\n",
    "#lgb预测结果作为新特征(替换困难样本)\n",
    "train_df['lgb_pre'] = lgb_train_oof_2\n",
    "test_df['lgb_pre'] = lgb_test_oof_2\n",
    "#cat预测结果作为新特征(替换困难样本)\n",
    "train_df['cat_pre'] = cat_train_oof_2\n",
    "test_df['cat_pre'] = cat_test_oof_2\n",
    "\n",
    "#训练特征\n",
    "feature_cols = [cols for cols in train_df if cols not in ['msisdn','label','end_time','stime','times_month']]\n",
    "# 训练tabnet模型\n",
    "final_tab_train_oof,final_tab_test_oof,final_tab_model,scaler = ml_model('tabnet',train_df[feature_cols], train_df['label'],test_df[feature_cols], test_df['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "#模型保存\n",
    "def save_model(mdoel,save_path):\n",
    "    # save_path = save_path + '/' +  mdoel_name +'.pkl'\n",
    "    joblib.dump(mdoel, save_path)\n",
    "#模型保存路径\n",
    "xgb_model_path = './model/xgb.pkl'\n",
    "lgb_model_path = './model/lgb.pkl'\n",
    "cat_model_path = './model/cat.pkl'\n",
    "svm_model_path = './model/svm.pkl'\n",
    "tabnet_model_path = './model/tabnet.pkl'\n",
    "final_tab_model_path = './model/final_tab.pkl'\n",
    "#保存\n",
    "save_model(xgb_model,xgb_model_path)\n",
    "save_model(lgb_model, lgb_model_path)\n",
    "save_model(cat_model,  cat_model_path)\n",
    "save_model(svm_model,svm_model_path)\n",
    "save_model(tab_model, tabnet_model_path)\n",
    "save_model(final_tab_model,  final_tab_model_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machine_learning_39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
