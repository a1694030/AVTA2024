{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "#读取数据\n",
    "df = pd.read_csv(r'C:\\Users\\86130\\Desktop\\论文\\日本会议发表\\data\\app_use_info_label.csv', sep=',', header=0)\n",
    "df2 = pd.read_csv(r'C:\\Users\\86130\\Desktop\\论文\\日本会议发表\\data\\user_portrait.csv', sep=',', header=0)\n",
    "df3 = pd.read_csv(r'C:\\Users\\86130\\Desktop\\论文\\日本会议发表\\data\\user_trajectory2.csv', sep=',', header=0)\n",
    "#合并数据\n",
    "df_all = pd.merge(df, df2, left_on='msisdn', right_on='userid', how='left').drop(columns = ['userid'])\n",
    "df_all = df_all.merge(df3, on = 'msisdn', how = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#日期特征处理\n",
    "def get_time_feature(df, col):\n",
    "    \n",
    "    df_copy = df.copy()\n",
    "    prefix = col + \"_\"\n",
    "    df_copy['new_'+col] = df_copy[col].astype(str)\n",
    "    \n",
    "    col = 'new_'+col\n",
    "    df_copy[col] = pd.to_datetime(df_copy[col])\n",
    "    df_copy[prefix + 'year'] = df_copy[col].dt.year\n",
    "    df_copy[prefix + 'month'] = df_copy[col].dt.month\n",
    "    df_copy[prefix + 'day'] = df_copy[col].dt.day\n",
    "    # df_copy[prefix + 'dayofweek'] = df_copy[col].dt.dayofweek\n",
    "    # df_copy[prefix + 'is_wknd'] = df_copy[col].dt.dayofweek // 6\n",
    "    # df_copy[prefix + 'quarter'] = df_copy[col].dt.quarter\n",
    "    # df_copy[prefix + 'is_month_start'] = df_copy[col].dt.is_month_start.astype(int)\n",
    "    # df_copy[prefix + 'is_month_end'] = df_copy[col].dt.is_month_end.astype(int)\n",
    "    del df_copy[col]\n",
    "    \n",
    "    return df_copy   \n",
    "    \n",
    "df_all = get_time_feature(df_all, 'stime')\n",
    "df_all = get_time_feature(df_all, 'end_time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#label编码\n",
    "from sklearn import preprocessing\n",
    " \n",
    "enc=preprocessing.LabelEncoder() \n",
    "enc=enc.fit(df_all['app_class_1']) \n",
    "df_all['app_class_1']=enc.transform(df_all['app_class_1'])\n",
    "\n",
    "enc2=preprocessing.LabelEncoder() \n",
    "enc2=enc2.fit(df_all['app_class_2']) \n",
    "df_all['app_class_2']=enc2.transform(df_all['app_class_2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt # 导入作图包\n",
    "import seaborn as sns    # 导入作图包\n",
    "\n",
    "plt.figure(figsize=(20,20),dpi=600)\n",
    "mask = np.triu(np.ones_like(df_all.fillna(0).corr().round(2), dtype=np.bool))\n",
    "ax = sns.heatmap(df_all.fillna(0).corr().round(2), linewidths=.5, cmap='YlGnBu',annot=True,vmax=2.0,mask=mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#特征衍生\n",
    "df_all['up_flow']= df_all['up_flow']/1024\n",
    "df_all['down_flow']= df_all['down_flow']/1024\n",
    "\n",
    "df_all['app1_upflow_mean'] = df_all['app_class_1'].map(df_all.groupby('app_class_1')['up_flow'].mean())\n",
    "df_all['app2_upflow_mean'] = df_all['app_class_2'].map(df_all.groupby('app_class_2')['up_flow'].mean())\n",
    "df_all['app1_upflow_max'] = df_all['app_class_1'].map(df_all.groupby('app_class_1')['up_flow'].max())\n",
    "df_all['app2_upflow_max'] = df_all['app_class_2'].map(df_all.groupby('app_class_2')['up_flow'].max())\n",
    "df_all['app1_upflow_min'] = df_all['app_class_1'].map(df_all.groupby('app_class_1')['up_flow'].min())\n",
    "df_all['app2_upflow_min'] = df_all['app_class_2'].map(df_all.groupby('app_class_2')['up_flow'].min())\n",
    "\n",
    "df_all['app1_downflow_mean'] = df_all['app_class_1'].map(df_all.groupby('app_class_1')['down_flow'].mean())\n",
    "df_all['app2_downflow_mean'] = df_all['app_class_2'].map(df_all.groupby('app_class_2')['down_flow'].mean())\n",
    "df_all['app1_downflow_max'] = df_all['app_class_1'].map(df_all.groupby('app_class_1')['down_flow'].max())\n",
    "df_all['app2_downflow_max'] = df_all['app_class_2'].map(df_all.groupby('app_class_2')['down_flow'].max())\n",
    "df_all['app1_downflow_min'] = df_all['app_class_1'].map(df_all.groupby('app_class_1')['down_flow'].min())\n",
    "df_all['app2_downflow_min'] = df_all['app_class_2'].map(df_all.groupby('app_class_2')['down_flow'].min())\n",
    "\n",
    "df_all['app1_consume_mean'] = df_all['app_class_1'].map(df_all.groupby('app_class_1')['consume'].mean())\n",
    "df_all['app2_consume_mean'] = df_all['app_class_2'].map(df_all.groupby('app_class_2')['consume'].mean())\n",
    "df_all['app1_consume_max'] = df_all['app_class_1'].map(df_all.groupby('app_class_1')['consume'].max())\n",
    "df_all['app2_consume_max'] = df_all['app_class_2'].map(df_all.groupby('app_class_2')['consume'].max())\n",
    "df_all['app1_consume_min'] = df_all['app_class_1'].map(df_all.groupby('app_class_1')['consume'].min())\n",
    "df_all['app2_consume_min'] = df_all['app_class_2'].map(df_all.groupby('app_class_2')['consume'].min())\n",
    "\n",
    "df_all['app1_age_mean'] = df_all['app_class_1'].map(df_all.groupby('app_class_1')['age'].mean())\n",
    "df_all['app2_age_mean'] = df_all['app_class_2'].map(df_all.groupby('app_class_2')['age'].mean())\n",
    "df_all['app1_age_max'] = df_all['app_class_1'].map(df_all.groupby('app_class_1')['age'].max())\n",
    "df_all['app2_age_max'] = df_all['app_class_2'].map(df_all.groupby('app_class_2')['age'].max())\n",
    "df_all['app1_age_min'] = df_all['app_class_1'].map(df_all.groupby('app_class_1')['age'].min())\n",
    "df_all['app2_age_min'] = df_all['app_class_2'].map(df_all.groupby('app_class_2')['age'].min())\n",
    "\n",
    "df_all['up-down'] = df_all['up_flow']-df_all['down_flow']\n",
    "df_all['up/down'] = df_all['up_flow']/df_all['down_flow']\n",
    "df_all['up+down'] = df_all['up_flow']+df_all['down_flow']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "53"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#分离训练集，测试集\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df_all=df_all.replace([np.inf, -np.inf], 0)\n",
    "df_all.fillna(0,inplace=True)\n",
    "train_df,test_df = train_test_split(df_all,train_size=0.8,shuffle=True,random_state=42)\n",
    "train_df = train_df.reset_index(drop=True)\n",
    "test_df = test_df.reset_index(drop=True)\n",
    "feature_cols = [cols for cols in df_all if cols not in ['msisdn','label','end_time','stime','times_month']]\n",
    "len(feature_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed: 888\n",
      "|  XGB  Fold  1  Training Start           |\n",
      "[0]\tvalidation_0-mlogloss:0.89551\tvalidation_1-mlogloss:0.89819\n",
      "[99]\tvalidation_0-mlogloss:0.31382\tvalidation_1-mlogloss:0.44397\n",
      "************ Val_Result ************\n",
      "AVG_acc : 0.8014725934006\n",
      "XGB_result :               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.89      0.89      5910\n",
      "           1       0.70      0.73      0.72      3685\n",
      "           2       0.71      0.62      0.66      1406\n",
      "\n",
      "    accuracy                           0.80     11001\n",
      "   macro avg       0.77      0.75      0.75     11001\n",
      "weighted avg       0.80      0.80      0.80     11001\n",
      "\n",
      "************ Test_Result ************\n",
      "AVG_acc : 0.8061231910406516\n",
      "XGB_result :               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.89      0.89      7458\n",
      "           1       0.70      0.74      0.72      4501\n",
      "           2       0.73      0.63      0.68      1792\n",
      "\n",
      "    accuracy                           0.81     13751\n",
      "   macro avg       0.77      0.75      0.76     13751\n",
      "weighted avg       0.81      0.81      0.81     13751\n",
      "\n",
      "|  XGB  Fold  2  Training Start           |\n",
      "[0]\tvalidation_0-mlogloss:0.89519\tvalidation_1-mlogloss:0.89720\n",
      "[99]\tvalidation_0-mlogloss:0.31243\tvalidation_1-mlogloss:0.44124\n",
      "************ Val_Result ************\n",
      "AVG_acc : 0.8018361967093901\n",
      "XGB_result :               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.89      0.89      5909\n",
      "           1       0.70      0.73      0.72      3685\n",
      "           2       0.70      0.62      0.66      1407\n",
      "\n",
      "    accuracy                           0.80     11001\n",
      "   macro avg       0.76      0.75      0.75     11001\n",
      "weighted avg       0.80      0.80      0.80     11001\n",
      "\n",
      "************ Test_Result ************\n",
      "AVG_acc : 0.80546869318595\n",
      "XGB_result :               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.89      0.89      7458\n",
      "           1       0.70      0.74      0.72      4501\n",
      "           2       0.72      0.62      0.67      1792\n",
      "\n",
      "    accuracy                           0.80     13751\n",
      "   macro avg       0.77      0.75      0.76     13751\n",
      "weighted avg       0.81      0.80      0.80     13751\n",
      "\n",
      "|  XGB  Fold  3  Training Start           |\n",
      "[0]\tvalidation_0-mlogloss:0.89519\tvalidation_1-mlogloss:0.89878\n",
      "[99]\tvalidation_0-mlogloss:0.31246\tvalidation_1-mlogloss:0.45190\n",
      "************ Val_Result ************\n",
      "AVG_acc : 0.8015937945035301\n",
      "XGB_result :               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.89      0.89      5909\n",
      "           1       0.70      0.74      0.72      3685\n",
      "           2       0.72      0.61      0.66      1407\n",
      "\n",
      "    accuracy                           0.80     11001\n",
      "   macro avg       0.77      0.74      0.75     11001\n",
      "weighted avg       0.80      0.80      0.80     11001\n",
      "\n",
      "************ Test_Result ************\n",
      "AVG_acc : 0.806074709718081\n",
      "XGB_result :               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.89      0.89      7458\n",
      "           1       0.70      0.75      0.72      4501\n",
      "           2       0.73      0.62      0.67      1792\n",
      "\n",
      "    accuracy                           0.81     13751\n",
      "   macro avg       0.77      0.75      0.76     13751\n",
      "weighted avg       0.81      0.81      0.81     13751\n",
      "\n",
      "|  XGB  Fold  4  Training Start           |\n",
      "[0]\tvalidation_0-mlogloss:0.89526\tvalidation_1-mlogloss:0.89901\n",
      "[99]\tvalidation_0-mlogloss:0.31702\tvalidation_1-mlogloss:0.44542\n",
      "************ Val_Result ************\n",
      "AVG_acc : 0.8026543041541678\n",
      "XGB_result :               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.89      0.89      5910\n",
      "           1       0.70      0.75      0.73      3684\n",
      "           2       0.71      0.62      0.66      1407\n",
      "\n",
      "    accuracy                           0.81     11001\n",
      "   macro avg       0.77      0.75      0.76     11001\n",
      "weighted avg       0.81      0.81      0.81     11001\n",
      "\n",
      "************ Test_Result ************\n",
      "AVG_acc : 0.8063958984801105\n",
      "XGB_result :               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.89      0.89      7458\n",
      "           1       0.70      0.75      0.72      4501\n",
      "           2       0.73      0.62      0.67      1792\n",
      "\n",
      "    accuracy                           0.81     13751\n",
      "   macro avg       0.78      0.75      0.76     13751\n",
      "weighted avg       0.81      0.81      0.81     13751\n",
      "\n",
      "|  XGB  Fold  5  Training Start           |\n",
      "[0]\tvalidation_0-mlogloss:0.89399\tvalidation_1-mlogloss:0.89763\n",
      "[99]\tvalidation_0-mlogloss:0.31531\tvalidation_1-mlogloss:0.44502\n",
      "************ Val_Result ************\n",
      "AVG_acc : 0.8031598069596979\n",
      "XGB_result :               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.89      0.89      5910\n",
      "           1       0.70      0.75      0.73      3684\n",
      "           2       0.71      0.60      0.65      1406\n",
      "\n",
      "    accuracy                           0.81     11000\n",
      "   macro avg       0.77      0.75      0.76     11000\n",
      "weighted avg       0.81      0.81      0.80     11000\n",
      "\n",
      "************ Test_Result ************\n",
      "AVG_acc : 0.8066613337211839\n",
      "XGB_result :               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.89      0.89      7458\n",
      "           1       0.70      0.75      0.72      4501\n",
      "           2       0.74      0.62      0.68      1792\n",
      "\n",
      "    accuracy                           0.81     13751\n",
      "   macro avg       0.78      0.75      0.76     13751\n",
      "weighted avg       0.81      0.81      0.81     13751\n",
      "\n",
      "Seed: 888\n",
      "|  LGB  Fold  1  Training Start           |\n",
      "************ Val_Result ************\n",
      "AVG_acc : 0.7988364694118717\n",
      "Cat_result :               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.89      0.88      5910\n",
      "           1       0.70      0.73      0.71      3685\n",
      "           2       0.71      0.61      0.66      1406\n",
      "\n",
      "    accuracy                           0.80     11001\n",
      "   macro avg       0.76      0.74      0.75     11001\n",
      "weighted avg       0.80      0.80      0.80     11001\n",
      "\n",
      "************ Test_Result ************\n",
      "AVG_acc : 0.8084502945240346\n",
      "Cat_result :               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.89      0.89      7458\n",
      "           1       0.70      0.75      0.72      4501\n",
      "           2       0.74      0.62      0.68      1792\n",
      "\n",
      "    accuracy                           0.81     13751\n",
      "   macro avg       0.78      0.75      0.76     13751\n",
      "weighted avg       0.81      0.81      0.81     13751\n",
      "\n",
      "|  LGB  Fold  2  Training Start           |\n",
      "************ Val_Result ************\n",
      "AVG_acc : 0.8023816016725752\n",
      "Cat_result :               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.89      0.89      5909\n",
      "           1       0.71      0.74      0.72      3685\n",
      "           2       0.70      0.62      0.66      1407\n",
      "\n",
      "    accuracy                           0.81     11001\n",
      "   macro avg       0.77      0.75      0.76     11001\n",
      "weighted avg       0.81      0.81      0.81     11001\n",
      "\n",
      "************ Test_Result ************\n",
      "AVG_acc : 0.807832157661261\n",
      "Cat_result :               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.89      0.89      7458\n",
      "           1       0.70      0.74      0.72      4501\n",
      "           2       0.73      0.63      0.68      1792\n",
      "\n",
      "    accuracy                           0.81     13751\n",
      "   macro avg       0.77      0.75      0.76     13751\n",
      "weighted avg       0.81      0.81      0.81     13751\n",
      "\n",
      "|  LGB  Fold  3  Training Start           |\n",
      "************ Val_Result ************\n",
      "AVG_acc : 0.8035633124261431\n",
      "Cat_result :               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.89      0.89      5909\n",
      "           1       0.71      0.74      0.72      3685\n",
      "           2       0.73      0.61      0.67      1407\n",
      "\n",
      "    accuracy                           0.81     11001\n",
      "   macro avg       0.78      0.75      0.76     11001\n",
      "weighted avg       0.81      0.81      0.81     11001\n",
      "\n",
      "************ Test_Result ************\n",
      "AVG_acc : 0.807771556008048\n",
      "Cat_result :               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.89      0.89      7458\n",
      "           1       0.70      0.75      0.72      4501\n",
      "           2       0.73      0.62      0.67      1792\n",
      "\n",
      "    accuracy                           0.81     13751\n",
      "   macro avg       0.77      0.75      0.76     13751\n",
      "weighted avg       0.81      0.81      0.81     13751\n",
      "\n",
      "|  LGB  Fold  4  Training Start           |\n",
      "************ Val_Result ************\n",
      "AVG_acc : 0.8045632215253159\n",
      "Cat_result :               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.89      0.89      5910\n",
      "           1       0.71      0.75      0.73      3684\n",
      "           2       0.72      0.62      0.66      1407\n",
      "\n",
      "    accuracy                           0.81     11001\n",
      "   macro avg       0.77      0.75      0.76     11001\n",
      "weighted avg       0.81      0.81      0.81     11001\n",
      "\n",
      "************ Test_Result ************\n",
      "AVG_acc : 0.8081957675805397\n",
      "Cat_result :               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.89      0.89      7458\n",
      "           1       0.70      0.75      0.73      4501\n",
      "           2       0.74      0.62      0.68      1792\n",
      "\n",
      "    accuracy                           0.81     13751\n",
      "   macro avg       0.78      0.75      0.76     13751\n",
      "weighted avg       0.81      0.81      0.81     13751\n",
      "\n",
      "|  LGB  Fold  5  Training Start           |\n",
      "************ Val_Result ************\n",
      "AVG_acc : 0.804814213583889\n",
      "Cat_result :               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.89      0.89      5910\n",
      "           1       0.71      0.75      0.73      3684\n",
      "           2       0.72      0.61      0.66      1406\n",
      "\n",
      "    accuracy                           0.81     11000\n",
      "   macro avg       0.77      0.75      0.76     11000\n",
      "weighted avg       0.81      0.81      0.81     11000\n",
      "\n",
      "************ Test_Result ************\n",
      "AVG_acc : 0.8083339393498654\n",
      "Cat_result :               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.89      0.89      7458\n",
      "           1       0.70      0.75      0.72      4501\n",
      "           2       0.74      0.62      0.68      1792\n",
      "\n",
      "    accuracy                           0.81     13751\n",
      "   macro avg       0.78      0.75      0.76     13751\n",
      "weighted avg       0.81      0.81      0.81     13751\n",
      "\n",
      "Seed: 888\n",
      "|  cat  Fold  1  Training Start           |\n",
      "************ Val_Result ************\n",
      "AVG_acc : 0.7932006181256249\n",
      "Cat_result :               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.89      0.88      5910\n",
      "           1       0.69      0.72      0.70      3685\n",
      "           2       0.70      0.59      0.64      1406\n",
      "\n",
      "    accuracy                           0.79     11001\n",
      "   macro avg       0.76      0.73      0.74     11001\n",
      "weighted avg       0.79      0.79      0.79     11001\n",
      "\n",
      "************ Test_Result ************\n",
      "AVG_acc : 0.8006690422514726\n",
      "Cat_result :               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.89      0.89      7458\n",
      "           1       0.69      0.73      0.71      4501\n",
      "           2       0.72      0.60      0.65      1792\n",
      "\n",
      "    accuracy                           0.80     13751\n",
      "   macro avg       0.77      0.74      0.75     13751\n",
      "weighted avg       0.80      0.80      0.80     13751\n",
      "\n",
      "|  cat  Fold  2  Training Start           |\n",
      "************ Val_Result ************\n",
      "AVG_acc : 0.7963366966639396\n",
      "Cat_result :               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.89      0.89      5909\n",
      "           1       0.70      0.73      0.71      3685\n",
      "           2       0.69      0.61      0.65      1407\n",
      "\n",
      "    accuracy                           0.80     11001\n",
      "   macro avg       0.76      0.74      0.75     11001\n",
      "weighted avg       0.80      0.80      0.80     11001\n",
      "\n",
      "************ Test_Result ************\n",
      "AVG_acc : 0.8014326230819577\n",
      "Cat_result :               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.89      0.89      7458\n",
      "           1       0.69      0.74      0.71      4501\n",
      "           2       0.72      0.61      0.66      1792\n",
      "\n",
      "    accuracy                           0.80     13751\n",
      "   macro avg       0.77      0.74      0.75     13751\n",
      "weighted avg       0.80      0.80      0.80     13751\n",
      "\n",
      "|  cat  Fold  3  Training Start           |\n",
      "************ Val_Result ************\n",
      "AVG_acc : 0.7966851498348634\n",
      "Cat_result :               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.89      0.88      5909\n",
      "           1       0.70      0.73      0.71      3685\n",
      "           2       0.72      0.59      0.65      1407\n",
      "\n",
      "    accuracy                           0.80     11001\n",
      "   macro avg       0.76      0.74      0.75     11001\n",
      "weighted avg       0.80      0.80      0.80     11001\n",
      "\n",
      "************ Test_Result ************\n",
      "AVG_acc : 0.8012265774610331\n",
      "Cat_result :               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.89      0.89      7458\n",
      "           1       0.69      0.73      0.71      4501\n",
      "           2       0.72      0.60      0.66      1792\n",
      "\n",
      "    accuracy                           0.80     13751\n",
      "   macro avg       0.77      0.74      0.75     13751\n",
      "weighted avg       0.80      0.80      0.80     13751\n",
      "\n",
      "|  cat  Fold  4  Training Start           |\n",
      "************ Val_Result ************\n",
      "AVG_acc : 0.797132078901918\n",
      "Cat_result :               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.89      0.89      5910\n",
      "           1       0.69      0.74      0.71      3684\n",
      "           2       0.69      0.59      0.64      1407\n",
      "\n",
      "    accuracy                           0.80     11001\n",
      "   macro avg       0.76      0.74      0.75     11001\n",
      "weighted avg       0.80      0.80      0.80     11001\n",
      "\n",
      "************ Test_Result ************\n",
      "AVG_acc : 0.8017235110173806\n",
      "Cat_result :               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.89      0.89      7458\n",
      "           1       0.69      0.74      0.72      4501\n",
      "           2       0.72      0.60      0.66      1792\n",
      "\n",
      "    accuracy                           0.80     13751\n",
      "   macro avg       0.77      0.74      0.75     13751\n",
      "weighted avg       0.80      0.80      0.80     13751\n",
      "\n",
      "|  cat  Fold  5  Training Start           |\n",
      "************ Val_Result ************\n",
      "AVG_acc : 0.797742026757898\n",
      "Cat_result :               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.89      0.89      5910\n",
      "           1       0.70      0.74      0.72      3684\n",
      "           2       0.70      0.59      0.64      1406\n",
      "\n",
      "    accuracy                           0.80     11000\n",
      "   macro avg       0.76      0.74      0.75     11000\n",
      "weighted avg       0.80      0.80      0.80     11000\n",
      "\n",
      "************ Test_Result ************\n",
      "AVG_acc : 0.8017453276125373\n",
      "Cat_result :               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.89      0.89      7458\n",
      "           1       0.69      0.73      0.71      4501\n",
      "           2       0.72      0.61      0.66      1792\n",
      "\n",
      "    accuracy                           0.80     13751\n",
      "   macro avg       0.77      0.74      0.75     13751\n",
      "weighted avg       0.80      0.80      0.80     13751\n",
      "\n",
      "Seed: 888\n",
      "|  SVM  Fold  1  Training Start           |\n",
      "************ Val_Result ************\n",
      "AVG_acc : 0.5635851286246705\n",
      "Svm_result :               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.96      0.71      5910\n",
      "           1       0.41      0.07      0.11      3685\n",
      "           2       0.70      0.21      0.32      1406\n",
      "\n",
      "    accuracy                           0.56     11001\n",
      "   macro avg       0.56      0.41      0.38     11001\n",
      "weighted avg       0.53      0.56      0.46     11001\n",
      "\n",
      "************ Test_Result ************\n",
      "AVG_acc : 0.5661406443167769\n",
      "Svm_result :               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.96      0.72      7458\n",
      "           1       0.39      0.06      0.11      4501\n",
      "           2       0.71      0.20      0.31      1792\n",
      "\n",
      "    accuracy                           0.57     13751\n",
      "   macro avg       0.56      0.41      0.38     13751\n",
      "weighted avg       0.53      0.57      0.46     13751\n",
      "\n",
      "|  SVM  Fold  2  Training Start           |\n",
      "************ Val_Result ************\n",
      "AVG_acc : 0.5484955913098809\n",
      "Svm_result :               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.88      0.70      5909\n",
      "           1       0.00      0.00      0.00      3685\n",
      "           2       0.31      0.46      0.37      1407\n",
      "\n",
      "    accuracy                           0.53     11001\n",
      "   macro avg       0.30      0.45      0.36     11001\n",
      "weighted avg       0.35      0.53      0.43     11001\n",
      "\n",
      "************ Test_Result ************\n",
      "AVG_acc : 0.5542142389644389\n",
      "Svm_result :               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.89      0.71      7458\n",
      "           1       0.00      0.00      0.00      4501\n",
      "           2       0.33      0.45      0.38      1792\n",
      "\n",
      "    accuracy                           0.54     13751\n",
      "   macro avg       0.31      0.45      0.36     13751\n",
      "weighted avg       0.36      0.54      0.43     13751\n",
      "\n",
      "|  SVM  Fold  3  Training Start           |\n",
      "************ Val_Result ************\n",
      "AVG_acc : 0.5184377177832319\n",
      "Svm_result :               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.74      0.65      5909\n",
      "           1       0.24      0.02      0.04      3685\n",
      "           2       0.18      0.41      0.25      1407\n",
      "\n",
      "    accuracy                           0.46     11001\n",
      "   macro avg       0.33      0.39      0.31     11001\n",
      "weighted avg       0.42      0.46      0.40     11001\n",
      "\n",
      "************ Test_Result ************\n",
      "AVG_acc : 0.5206166824230964\n",
      "Svm_result :               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.73      0.65      7458\n",
      "           1       0.21      0.02      0.03      4501\n",
      "           2       0.17      0.38      0.24      1792\n",
      "\n",
      "    accuracy                           0.45     13751\n",
      "   macro avg       0.32      0.38      0.31     13751\n",
      "weighted avg       0.41      0.45      0.39     13751\n",
      "\n",
      "|  SVM  Fold  4  Training Start           |\n",
      "************ Val_Result ************\n",
      "AVG_acc : 0.5306790291791655\n",
      "Svm_result :               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.95      0.73      5910\n",
      "           1       0.39      0.00      0.01      3684\n",
      "           2       0.41      0.45      0.43      1407\n",
      "\n",
      "    accuracy                           0.57     11001\n",
      "   macro avg       0.46      0.47      0.39     11001\n",
      "weighted avg       0.50      0.57      0.45     11001\n",
      "\n",
      "************ Test_Result ************\n",
      "AVG_acc : 0.533961166460621\n",
      "Svm_result :               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.95      0.73      7458\n",
      "           1       0.48      0.00      0.01      4501\n",
      "           2       0.43      0.45      0.44      1792\n",
      "\n",
      "    accuracy                           0.57     13751\n",
      "   macro avg       0.50      0.47      0.39     13751\n",
      "weighted avg       0.54      0.57      0.46     13751\n",
      "\n",
      "|  SVM  Fold  5  Training Start           |\n",
      "************ Val_Result ************\n",
      "AVG_acc : 0.5420523142524234\n",
      "Svm_result :               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.94      0.75      5910\n",
      "           1       0.53      0.02      0.03      3684\n",
      "           2       0.45      0.59      0.51      1406\n",
      "\n",
      "    accuracy                           0.59     11000\n",
      "   macro avg       0.53      0.52      0.43     11000\n",
      "weighted avg       0.56      0.59      0.48     11000\n",
      "\n",
      "************ Test_Result ************\n",
      "AVG_acc : 0.545633044869464\n",
      "Svm_result :               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.95      0.75      7458\n",
      "           1       0.50      0.01      0.02      4501\n",
      "           2       0.46      0.58      0.51      1792\n",
      "\n",
      "    accuracy                           0.59     13751\n",
      "   macro avg       0.53      0.51      0.43     13751\n",
      "weighted avg       0.56      0.59      0.48     13751\n",
      "\n",
      "Seed: 888\n",
      "     Tab_model  Fold 1  Training Starting       \n",
      "Using CPU\n",
      "epoch 0  | loss: 0.95332 | val_0_accuracy: 0.56658 |  0:00:03s\n",
      "epoch 1  | loss: 0.86596 | val_0_accuracy: 0.5884  |  0:00:06s\n",
      "epoch 2  | loss: 0.84065 | val_0_accuracy: 0.59576 |  0:00:10s\n",
      "epoch 3  | loss: 0.81914 | val_0_accuracy: 0.57422 |  0:00:13s\n",
      "epoch 4  | loss: 0.80001 | val_0_accuracy: 0.46668 |  0:00:17s\n",
      "epoch 5  | loss: 0.78335 | val_0_accuracy: 0.4855  |  0:00:20s\n",
      "epoch 6  | loss: 0.77061 | val_0_accuracy: 0.43514 |  0:00:24s\n",
      "epoch 7  | loss: 0.75008 | val_0_accuracy: 0.40278 |  0:00:27s\n",
      "epoch 8  | loss: 0.73414 | val_0_accuracy: 0.39124 |  0:00:31s\n",
      "epoch 9  | loss: 0.71975 | val_0_accuracy: 0.38533 |  0:00:34s\n",
      "epoch 10 | loss: 0.72714 | val_0_accuracy: 0.40324 |  0:00:37s\n",
      "epoch 11 | loss: 0.72143 | val_0_accuracy: 0.40796 |  0:00:41s\n",
      "epoch 12 | loss: 0.71801 | val_0_accuracy: 0.37878 |  0:00:45s\n",
      "\n",
      "Early stopping occurred at epoch 12 with best_epoch = 2 and best_val_0_accuracy = 0.59576\n",
      "************ Val_Result ************\n",
      "AVG_acc : 0.5957640214525952\n",
      "Tabnet_result :               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.93      0.74      5910\n",
      "           1       0.48      0.12      0.19      3685\n",
      "           2       0.52      0.44      0.48      1406\n",
      "\n",
      "    accuracy                           0.60     11001\n",
      "   macro avg       0.54      0.50      0.47     11001\n",
      "weighted avg       0.56      0.60      0.52     11001\n",
      "\n",
      "************ Test_Result ************\n",
      "AVG_acc : 0.5985019271325722\n",
      "Tabnet_result :               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.93      0.74      7458\n",
      "           1       0.45      0.11      0.18      4501\n",
      "           2       0.54      0.44      0.49      1792\n",
      "\n",
      "    accuracy                           0.60     13751\n",
      "   macro avg       0.54      0.50      0.47     13751\n",
      "weighted avg       0.56      0.60      0.53     13751\n",
      "\n",
      "     Tab_model  Fold 2  Training Starting       \n",
      "Using CPU\n",
      "epoch 0  | loss: 0.96761 | val_0_accuracy: 0.56031 |  0:00:03s\n",
      "epoch 1  | loss: 0.87483 | val_0_accuracy: 0.57522 |  0:00:06s\n",
      "epoch 2  | loss: 0.85297 | val_0_accuracy: 0.57149 |  0:00:10s\n",
      "epoch 3  | loss: 0.83693 | val_0_accuracy: 0.57495 |  0:00:13s\n",
      "epoch 4  | loss: 0.82596 | val_0_accuracy: 0.60822 |  0:00:17s\n",
      "epoch 5  | loss: 0.81419 | val_0_accuracy: 0.60367 |  0:00:20s\n",
      "epoch 6  | loss: 0.81472 | val_0_accuracy: 0.61722 |  0:00:24s\n",
      "epoch 7  | loss: 0.79189 | val_0_accuracy: 0.6464  |  0:00:27s\n",
      "epoch 8  | loss: 0.76438 | val_0_accuracy: 0.52213 |  0:00:31s\n",
      "epoch 9  | loss: 0.72482 | val_0_accuracy: 0.43214 |  0:00:34s\n",
      "epoch 10 | loss: 0.67869 | val_0_accuracy: 0.4715  |  0:00:38s\n",
      "epoch 11 | loss: 0.65658 | val_0_accuracy: 0.38696 |  0:00:41s\n",
      "epoch 12 | loss: 0.64138 | val_0_accuracy: 0.37751 |  0:00:45s\n",
      "epoch 13 | loss: 0.63074 | val_0_accuracy: 0.37488 |  0:00:48s\n",
      "epoch 14 | loss: 0.6276  | val_0_accuracy: 0.4935  |  0:00:52s\n",
      "epoch 15 | loss: 0.62127 | val_0_accuracy: 0.38533 |  0:00:55s\n",
      "epoch 16 | loss: 0.61899 | val_0_accuracy: 0.3756  |  0:00:58s\n",
      "epoch 17 | loss: 0.61476 | val_0_accuracy: 0.38151 |  0:01:02s\n",
      "\n",
      "Early stopping occurred at epoch 17 with best_epoch = 7 and best_val_0_accuracy = 0.6464\n",
      "************ Val_Result ************\n",
      "AVG_acc : 0.6210799018271067\n",
      "Tabnet_result :               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.87      0.77      5909\n",
      "           1       0.50      0.39      0.44      3685\n",
      "           2       0.73      0.37      0.49      1407\n",
      "\n",
      "    accuracy                           0.65     11001\n",
      "   macro avg       0.64      0.54      0.57     11001\n",
      "weighted avg       0.63      0.65      0.62     11001\n",
      "\n",
      "************ Test_Result ************\n",
      "AVG_acc : 0.621627517998691\n",
      "Tabnet_result :               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.87      0.77      7458\n",
      "           1       0.49      0.38      0.43      4501\n",
      "           2       0.71      0.36      0.48      1792\n",
      "\n",
      "    accuracy                           0.64     13751\n",
      "   macro avg       0.63      0.54      0.56     13751\n",
      "weighted avg       0.63      0.64      0.62     13751\n",
      "\n",
      "     Tab_model  Fold 3  Training Starting       \n",
      "Using CPU\n",
      "epoch 0  | loss: 0.95861 | val_0_accuracy: 0.5744  |  0:00:03s\n",
      "epoch 1  | loss: 0.86572 | val_0_accuracy: 0.58504 |  0:00:06s\n",
      "epoch 2  | loss: 0.84517 | val_0_accuracy: 0.59776 |  0:00:10s\n",
      "epoch 3  | loss: 0.8381  | val_0_accuracy: 0.60495 |  0:00:13s\n",
      "epoch 4  | loss: 0.83154 | val_0_accuracy: 0.61004 |  0:00:17s\n",
      "epoch 5  | loss: 0.82867 | val_0_accuracy: 0.60067 |  0:00:20s\n",
      "epoch 6  | loss: 0.81844 | val_0_accuracy: 0.61767 |  0:00:24s\n",
      "epoch 7  | loss: 0.80908 | val_0_accuracy: 0.61585 |  0:00:27s\n",
      "epoch 8  | loss: 0.80206 | val_0_accuracy: 0.63494 |  0:00:31s\n",
      "epoch 9  | loss: 0.78805 | val_0_accuracy: 0.6324  |  0:00:34s\n",
      "epoch 10 | loss: 0.77832 | val_0_accuracy: 0.61694 |  0:00:38s\n",
      "epoch 11 | loss: 0.76048 | val_0_accuracy: 0.6204  |  0:00:41s\n",
      "epoch 12 | loss: 0.74828 | val_0_accuracy: 0.61858 |  0:00:45s\n",
      "epoch 13 | loss: 0.74019 | val_0_accuracy: 0.63067 |  0:00:48s\n",
      "epoch 14 | loss: 0.72197 | val_0_accuracy: 0.59922 |  0:00:52s\n",
      "epoch 15 | loss: 0.70547 | val_0_accuracy: 0.4725  |  0:00:55s\n",
      "epoch 16 | loss: 0.7039  | val_0_accuracy: 0.50686 |  0:00:58s\n",
      "epoch 17 | loss: 0.68664 | val_0_accuracy: 0.43469 |  0:01:02s\n",
      "epoch 18 | loss: 0.67511 | val_0_accuracy: 0.43741 |  0:01:05s\n",
      "\n",
      "Early stopping occurred at epoch 18 with best_epoch = 8 and best_val_0_accuracy = 0.63494\n",
      "************ Val_Result ************\n",
      "AVG_acc : 0.6257006938763143\n",
      "Tabnet_result :               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.88      0.76      5909\n",
      "           1       0.50      0.34      0.40      3685\n",
      "           2       0.71      0.38      0.49      1407\n",
      "\n",
      "    accuracy                           0.63     11001\n",
      "   macro avg       0.63      0.53      0.55     11001\n",
      "weighted avg       0.62      0.63      0.61     11001\n",
      "\n",
      "************ Test_Result ************\n",
      "AVG_acc : 0.6244879160303493\n",
      "Tabnet_result :               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.87      0.76      7458\n",
      "           1       0.47      0.33      0.39      4501\n",
      "           2       0.72      0.38      0.50      1792\n",
      "\n",
      "    accuracy                           0.63     13751\n",
      "   macro avg       0.62      0.53      0.55     13751\n",
      "weighted avg       0.61      0.63      0.60     13751\n",
      "\n",
      "     Tab_model  Fold 4  Training Starting       \n",
      "Using CPU\n",
      "epoch 0  | loss: 0.96297 | val_0_accuracy: 0.56558 |  0:00:03s\n",
      "epoch 1  | loss: 0.87067 | val_0_accuracy: 0.58295 |  0:00:07s\n",
      "epoch 2  | loss: 0.84737 | val_0_accuracy: 0.58995 |  0:00:10s\n",
      "epoch 3  | loss: 0.83298 | val_0_accuracy: 0.60122 |  0:00:14s\n",
      "epoch 4  | loss: 0.81684 | val_0_accuracy: 0.55604 |  0:00:17s\n",
      "epoch 5  | loss: 0.80189 | val_0_accuracy: 0.57277 |  0:00:21s\n",
      "epoch 6  | loss: 0.79323 | val_0_accuracy: 0.54231 |  0:00:24s\n",
      "epoch 7  | loss: 0.78291 | val_0_accuracy: 0.50895 |  0:00:28s\n",
      "epoch 8  | loss: 0.77864 | val_0_accuracy: 0.54622 |  0:00:31s\n",
      "epoch 9  | loss: 0.77172 | val_0_accuracy: 0.50504 |  0:00:34s\n",
      "epoch 10 | loss: 0.76065 | val_0_accuracy: 0.40751 |  0:00:38s\n",
      "epoch 11 | loss: 0.74952 | val_0_accuracy: 0.45759 |  0:00:41s\n",
      "epoch 12 | loss: 0.74898 | val_0_accuracy: 0.4615  |  0:00:45s\n",
      "epoch 13 | loss: 0.72882 | val_0_accuracy: 0.3596  |  0:00:48s\n",
      "\n",
      "Early stopping occurred at epoch 13 with best_epoch = 3 and best_val_0_accuracy = 0.60122\n",
      "************ Val_Result ************\n",
      "AVG_acc : 0.6195800381783475\n",
      "Tabnet_result :               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.90      0.75      5910\n",
      "           1       0.45      0.28      0.34      3684\n",
      "           2       0.68      0.21      0.33      1407\n",
      "\n",
      "    accuracy                           0.60     11001\n",
      "   macro avg       0.59      0.46      0.47     11001\n",
      "weighted avg       0.58      0.60      0.56     11001\n",
      "\n",
      "************ Test_Result ************\n",
      "AVG_acc : 0.6177550723583739\n",
      "Tabnet_result :               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.89      0.74      7458\n",
      "           1       0.43      0.25      0.32      4501\n",
      "           2       0.75      0.24      0.37      1792\n",
      "\n",
      "    accuracy                           0.60     13751\n",
      "   macro avg       0.60      0.46      0.48     13751\n",
      "weighted avg       0.58      0.60      0.55     13751\n",
      "\n",
      "     Tab_model  Fold 5  Training Starting       \n",
      "Using CPU\n",
      "epoch 0  | loss: 0.9663  | val_0_accuracy: 0.57018 |  0:00:03s\n",
      "epoch 1  | loss: 0.85282 | val_0_accuracy: 0.61591 |  0:00:06s\n",
      "epoch 2  | loss: 0.7855  | val_0_accuracy: 0.41518 |  0:00:10s\n",
      "epoch 3  | loss: 0.72144 | val_0_accuracy: 0.36873 |  0:00:14s\n",
      "epoch 4  | loss: 0.70848 | val_0_accuracy: 0.37009 |  0:00:17s\n",
      "epoch 5  | loss: 0.69384 | val_0_accuracy: 0.38436 |  0:00:21s\n",
      "epoch 6  | loss: 0.67372 | val_0_accuracy: 0.38882 |  0:00:24s\n",
      "epoch 7  | loss: 0.66464 | val_0_accuracy: 0.38745 |  0:00:27s\n",
      "epoch 8  | loss: 0.65216 | val_0_accuracy: 0.38364 |  0:00:31s\n",
      "epoch 9  | loss: 0.64322 | val_0_accuracy: 0.38382 |  0:00:34s\n",
      "epoch 10 | loss: 0.63999 | val_0_accuracy: 0.38345 |  0:00:38s\n",
      "epoch 11 | loss: 0.6318  | val_0_accuracy: 0.36664 |  0:00:41s\n",
      "\n",
      "Early stopping occurred at epoch 11 with best_epoch = 1 and best_val_0_accuracy = 0.61591\n",
      "************ Val_Result ************\n",
      "AVG_acc : 0.6188458487244961\n",
      "Tabnet_result :               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.93      0.75      5910\n",
      "           1       0.48      0.22      0.30      3684\n",
      "           2       0.74      0.32      0.45      1406\n",
      "\n",
      "    accuracy                           0.62     11000\n",
      "   macro avg       0.62      0.49      0.50     11000\n",
      "weighted avg       0.60      0.62      0.56     11000\n",
      "\n",
      "************ Test_Result ************\n",
      "AVG_acc : 0.6174969093156861\n",
      "Tabnet_result :               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.93      0.76      7458\n",
      "           1       0.46      0.22      0.30      4501\n",
      "           2       0.74      0.31      0.43      1792\n",
      "\n",
      "    accuracy                           0.62     13751\n",
      "   macro avg       0.61      0.49      0.50     13751\n",
      "weighted avg       0.59      0.62      0.56     13751\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#构建模型\n",
    "import optuna\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from catboost import CatBoostClassifier \n",
    "from sklearn.model_selection import StratifiedKFold,KFold\n",
    "from sklearn.metrics import roc_auc_score,accuracy_score,classification_report\n",
    "from sklearn.preprocessing import StandardScaler,MinMaxScaler\n",
    "from pytorch_tabnet import tab_model\n",
    "from sklearn.svm import SVC\n",
    "import torch\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# # 标准化\n",
    "# def scale(train_features,test_features):\n",
    "#     scaler=StandardScaler()\n",
    "#     scaler.fit(train_features)\n",
    "#     train_features=pd.DataFrame(scaler.transform(train_features),columns=test_features.keys())\n",
    "#     test_features=pd.DataFrame(scaler.transform(test_features),columns=test_features.keys())\n",
    "#     return train_features,test_features\n",
    "\n",
    "# 模型定义\n",
    "def ml_model(clf,train_x,train_y,test_x=[],test_y=[]):\n",
    "    seeds=[888]\n",
    "    train_oof = np.zeros([train_x.shape[0],3])\n",
    "    feat_imp_df = pd.DataFrame()\n",
    "    feat_imp_df['feature'] = train_x.columns\n",
    "    feat_imp_df['imp'] = 0\n",
    "    #标准化\n",
    "    scaler=StandardScaler()\n",
    "    scaler.fit(train_x)\n",
    "    train_x = scaler.transform(train_x)\n",
    "    if (len(test_x)!=0) and (len(test_x)!=0):\n",
    "        test_oof = np.zeros([test_x.shape[0],3])\n",
    "        test_x = scaler.transform(test_x)\n",
    "    for seed in seeds:\n",
    "        print('Seed:',seed)\n",
    "        folds = 5\n",
    "        kf = StratifiedKFold(n_splits=folds, shuffle=True, random_state=seed)\n",
    "        acc_scores_val = []\n",
    "        acc_scores_test = []\n",
    "        for i, (train_index, valid_index) in enumerate(kf.split(train_x, train_y)):\n",
    "            trn_x, trn_y, val_x, val_y = train_x[train_index], train_y[train_index], train_x[valid_index], \\\n",
    "                                        train_y[valid_index] \n",
    "            if clf == 'xgb':\n",
    "                print(\"|  XGB  Fold  {}  Training Start           |\".format(str(i + 1)))\n",
    "                xgb_params = {'booster': 'gbtree','objective': 'multi:softprob','eval_metric':'mlogloss','num_class':3,\n",
    "                    'n_estimators':500,'max_depth': 8,'lambda': 10,'subsample': 0.7,'colsample_bytree': 0.8,'eta': 0.1,\n",
    "                    'colsample_bylevel': 0.7,'tree_method': 'hist','seed': seed,'nthread': 16}\n",
    "                #训练模型\n",
    "                model = xgb.XGBClassifier(*xgb_params)\n",
    "                model.fit(trn_x,trn_y,eval_set=[(trn_x, trn_y),(val_x,val_y)],early_stopping_rounds=50,verbose=100)\n",
    "                #验证集\n",
    "                print('************ Val_Result ************')\n",
    "                val_pred  = model.predict_proba(val_x)\n",
    "                acc_score_val = accuracy_score(val_y, np.argmax(val_pred, axis=1))\n",
    "                acc_scores_val.append(acc_score_val)\n",
    "                print('AVG_acc :',sum(acc_scores_val)/len(acc_scores_val))\n",
    "                print('XGB_result :',classification_report(val_y, np.argmax(val_pred, axis=1)))\n",
    "                #测试集\n",
    "                if (len(test_x)!=0) and (len(test_x)!=0):\n",
    "                    print('************ Test_Result ************')\n",
    "                    test_pred  = model.predict_proba(test_x)\n",
    "                    acc_score_tset = accuracy_score(test_y, np.argmax(test_pred, axis=1))\n",
    "                    acc_scores_test.append(acc_score_tset)\n",
    "                    print('AVG_acc :',sum(acc_scores_test)/len(acc_scores_test))\n",
    "                    print('XGB_result :',classification_report(test_y, np.argmax(test_pred, axis=1)))\n",
    "                    #保存测试集结果\n",
    "                    test_oof += test_pred / kf.n_splits / len(seeds)\n",
    "                \n",
    "                #保存训练集结果\n",
    "                train_oof[valid_index] = val_pred / kf.n_splits / len(seeds)\n",
    "                \n",
    "                #模型特征重要性\n",
    "                feat_imp_df['imp'] += model.feature_importances_ / folds/ len(seeds)\n",
    "                feat_imp_df = feat_imp_df.sort_values(by='imp', ascending=False).reset_index(drop=True)\n",
    "                feat_imp_df['rank'] = range(feat_imp_df.shape[0])\n",
    "            if clf == 'lgb':\n",
    "                lgb_params = {'boosting_type': 'gbdt','n_estimators':500,'min_child_weight': 4,'num_leaves': 64,\n",
    "                    'feature_fraction': 0.8,'bagging_fraction': 0.8,'bagging_freq': 4,'learning_rate': 0.02,\n",
    "                    'seed': seed,'nthread': 32,'n_jobs':8,'verbose': -1}\n",
    "                print(\"|  LGB  Fold  {}  Training Start           |\".format(str(i + 1)))\n",
    "                #训练模型\n",
    "                model = lgb.LGBMClassifier(**lgb_params)\n",
    "                model.fit(trn_x,trn_y)\n",
    "                \n",
    "                #验证集\n",
    "                print('************ Val_Result ************')\n",
    "                val_pred  = model.predict_proba(val_x)\n",
    "                acc_score_val = accuracy_score(val_y, np.argmax(val_pred, axis=1))\n",
    "                acc_scores_val.append(acc_score_val)\n",
    "                print('AVG_acc :',sum(acc_scores_val)/len(acc_scores_val))\n",
    "                print('Cat_result :',classification_report(val_y, np.argmax(val_pred, axis=1)))\n",
    "                #测试集\n",
    "                if (len(test_x)!=0) and (len(test_x)!=0):\n",
    "                    print('************ Test_Result ************')\n",
    "                    test_pred  = model.predict_proba(test_x)\n",
    "                    acc_score_tset = accuracy_score(test_y, np.argmax(test_pred, axis=1))\n",
    "                    acc_scores_test.append(acc_score_tset)\n",
    "                    print('AVG_acc :',sum(acc_scores_test)/len(acc_scores_test))\n",
    "                    print('Cat_result :',classification_report(test_y, np.argmax(test_pred, axis=1))) \n",
    "                    #保存测试集结果\n",
    "                    test_oof += test_pred / kf.n_splits / len(seeds)\n",
    "                #保存训练集结果\n",
    "                train_oof[valid_index] = val_pred / kf.n_splits / len(seeds)\n",
    "            if clf == 'cat':\n",
    "                print(\"|  cat  Fold  {}  Training Start           |\".format(str(i + 1)))\n",
    "                #训练模型\n",
    "                model = CatBoostClassifier(verbose=False)\n",
    "                model.fit(trn_x,trn_y)\n",
    "                    \n",
    "                #验证集\n",
    "                print('************ Val_Result ************')\n",
    "                val_pred  = model.predict_proba(val_x)\n",
    "                acc_score_val = accuracy_score(val_y, np.argmax(val_pred, axis=1))\n",
    "                acc_scores_val.append(acc_score_val)\n",
    "                print('AVG_acc :',sum(acc_scores_val)/len(acc_scores_val))\n",
    "                print('Cat_result :',classification_report(val_y, np.argmax(val_pred, axis=1)))\n",
    "                #测试集\n",
    "                if (len(test_x)!=0) and (len(test_x)!=0):\n",
    "                    print('************ Test_Result ************')\n",
    "                    test_pred  = model.predict_proba(test_x)\n",
    "                    acc_score_tset = accuracy_score(test_y, np.argmax(test_pred, axis=1))\n",
    "                    acc_scores_test.append(acc_score_tset)\n",
    "                    print('AVG_acc :',sum(acc_scores_test)/len(acc_scores_test))\n",
    "                    print('Cat_result :',classification_report(test_y, np.argmax(test_pred, axis=1))) \n",
    "                    #保存测试集结果\n",
    "                    test_oof += test_pred / kf.n_splits / len(seeds)\n",
    "                    \n",
    "                #保存结果\n",
    "                train_oof[valid_index] = val_pred / kf.n_splits / len(seeds)\n",
    "            if clf == 'svm':\n",
    "                print(\"|  SVM  Fold  {}  Training Start           |\".format(str(i + 1)))\n",
    "                #训练模型\n",
    "                model = SVC(kernel='rbf', C=1, gamma='auto', probability=True,max_iter=1000)\n",
    "                model.fit(trn_x,trn_y)\n",
    "                \n",
    "                #验证集\n",
    "                print('************ Val_Result ************')\n",
    "                val_pred  = model.predict_proba(val_x)\n",
    "                acc_score_val = accuracy_score(val_y, np.argmax(val_pred, axis=1))\n",
    "                acc_scores_val.append(acc_score_val)\n",
    "                print('AVG_acc :',sum(acc_scores_val)/len(acc_scores_val))\n",
    "                print('Svm_result :',classification_report(val_y, np.argmax(val_pred, axis=1)))\n",
    "                #测试集\n",
    "                print('************ Test_Result ************')\n",
    "                test_pred  = model.predict_proba(test_x)\n",
    "                acc_score_tset = accuracy_score(test_y, np.argmax(test_pred, axis=1))\n",
    "                acc_scores_test.append(acc_score_tset)\n",
    "                print('AVG_acc :',sum(acc_scores_test)/len(acc_scores_test))\n",
    "                print('Svm_result :',classification_report(test_y, np.argmax(test_pred, axis=1))) \n",
    "                #保存结果\n",
    "                train_oof[valid_index] = val_pred / kf.n_splits / len(seeds)\n",
    "                test_oof += test_pred / kf.n_splits / len(seeds)\n",
    "                \n",
    "            if clf == 'tabnet':\n",
    "                print(f\"     Tab_model  Fold {i+1}  Training Starting       \")\n",
    "                if torch.cuda.is_available():\n",
    "                    print(\"Using GPU\")\n",
    "                    device = \"cuda\"\n",
    "                else:\n",
    "                    print(\"Using CPU\")\n",
    "                    device = \"cpu\"\n",
    "                    \n",
    "                torch.manual_seed(seed)\n",
    "                np.random.seed(seed)\n",
    "                model = tab_model.TabNetClassifier()\n",
    "\n",
    "                model.fit(trn_x, trn_y,eval_set=[(val_x, val_y)],eval_metric=['accuracy'])\n",
    "                #验证集\n",
    "                print('************ Val_Result ************')\n",
    "                val_pred  = model.predict_proba(val_x)\n",
    "                acc_score_val = accuracy_score(val_y, np.argmax(val_pred, axis=1))\n",
    "                acc_scores_val.append(acc_score_val)\n",
    "                print('AVG_acc :',sum(acc_scores_val)/len(acc_scores_val))\n",
    "                print('Tabnet_result :',classification_report(val_y, np.argmax(val_pred, axis=1)))\n",
    "                #测试集\n",
    "                print('************ Test_Result ************')\n",
    "                test_pred  = model.predict_proba(test_x)\n",
    "                acc_score_tset = accuracy_score(test_y, np.argmax(test_pred, axis=1))\n",
    "                acc_scores_test.append(acc_score_tset)\n",
    "                print('AVG_acc :',sum(acc_scores_test)/len(acc_scores_test))\n",
    "                print('Tabnet_result :',classification_report(test_y, np.argmax(test_pred, axis=1))) \n",
    "                #保存结果\n",
    "                train_oof[valid_index] = val_pred / kf.n_splits / len(seeds)\n",
    "                test_oof += test_pred / kf.n_splits / len(seeds)\n",
    "        if (len(test_x)!=0) and (len(test_x)!=0):\n",
    "            return train_oof,test_oof,model,scaler\n",
    "        else:\n",
    "            return train_oof,model,scaler\n",
    "\n",
    "# 训练 XGB模型\n",
    "xgb_train_oof_1,xgb_test_oof_1, xgb_model_1,scaler_1 = ml_model('xgb',train_df[feature_cols], train_df['label'],test_df[feature_cols], test_df['label'])\n",
    "\n",
    "# 训练 LGB模型\n",
    "lgb_train_oof_1,lgb_test_oof_1,lgb_model_1,scaler_1 = ml_model('lgb',train_df[feature_cols], train_df['label'],test_df[feature_cols], test_df['label'])\n",
    "\n",
    "# 训练 CAT模型\n",
    "cat_train_oof_1,cat_test_oof_1,cat_model_1,scaler_1 = ml_model('cat',train_df[feature_cols], train_df['label'],test_df[feature_cols], test_df['label'])\n",
    "\n",
    "# 训练 SVM模型\n",
    "svm_train_oof_1,svm_test_oof_1,svm_model_1,scaler_1 = ml_model('svm',train_df[feature_cols], train_df['label'],test_df[feature_cols], test_df['label'])\n",
    "\n",
    "# 训练 Tabnet模型\n",
    "tab_train_oof_1,tab_test_oof_1,tab_model_1,scaler_1 = ml_model('tabnet',train_df[feature_cols], train_df['label'],test_df[feature_cols], test_df['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed: 888\n",
      "|  XGB  Fold  1  Training Start           |\n",
      "[0]\tvalidation_0-mlogloss:0.96997\tvalidation_1-mlogloss:0.98364\n",
      "[99]\tvalidation_0-mlogloss:0.20272\tvalidation_1-mlogloss:0.62778\n",
      "************ Val_Result ************\n",
      "AVG_acc : 0.7344444444444445\n",
      "XGB_result :               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.68      0.67       550\n",
      "           1       0.84      0.84      0.84       772\n",
      "           2       0.65      0.62      0.64       478\n",
      "\n",
      "    accuracy                           0.73      1800\n",
      "   macro avg       0.72      0.71      0.72      1800\n",
      "weighted avg       0.73      0.73      0.73      1800\n",
      "\n",
      "|  XGB  Fold  2  Training Start           |\n",
      "[0]\tvalidation_0-mlogloss:0.97274\tvalidation_1-mlogloss:0.98713\n",
      "[99]\tvalidation_0-mlogloss:0.20816\tvalidation_1-mlogloss:0.62838\n",
      "************ Val_Result ************\n",
      "AVG_acc : 0.7380555555555556\n",
      "XGB_result :               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.65      0.68       549\n",
      "           1       0.81      0.85      0.83       773\n",
      "           2       0.66      0.67      0.67       478\n",
      "\n",
      "    accuracy                           0.74      1800\n",
      "   macro avg       0.73      0.72      0.72      1800\n",
      "weighted avg       0.74      0.74      0.74      1800\n",
      "\n",
      "|  XGB  Fold  3  Training Start           |\n",
      "[0]\tvalidation_0-mlogloss:0.96537\tvalidation_1-mlogloss:0.97914\n",
      "[99]\tvalidation_0-mlogloss:0.19456\tvalidation_1-mlogloss:0.60322\n",
      "************ Val_Result ************\n",
      "AVG_acc : 0.7396296296296296\n",
      "XGB_result :               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.68      0.69       550\n",
      "           1       0.82      0.85      0.83       773\n",
      "           2       0.66      0.63      0.65       477\n",
      "\n",
      "    accuracy                           0.74      1800\n",
      "   macro avg       0.73      0.72      0.72      1800\n",
      "weighted avg       0.74      0.74      0.74      1800\n",
      "\n",
      "|  XGB  Fold  4  Training Start           |\n",
      "[0]\tvalidation_0-mlogloss:0.96640\tvalidation_1-mlogloss:0.98179\n",
      "[99]\tvalidation_0-mlogloss:0.20520\tvalidation_1-mlogloss:0.63447\n",
      "************ Val_Result ************\n",
      "AVG_acc : 0.7433333333333334\n",
      "XGB_result :               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.69      0.70       550\n",
      "           1       0.83      0.84      0.84       773\n",
      "           2       0.68      0.68      0.68       477\n",
      "\n",
      "    accuracy                           0.75      1800\n",
      "   macro avg       0.74      0.74      0.74      1800\n",
      "weighted avg       0.75      0.75      0.75      1800\n",
      "\n",
      "|  XGB  Fold  5  Training Start           |\n",
      "[0]\tvalidation_0-mlogloss:0.96742\tvalidation_1-mlogloss:0.98601\n",
      "[99]\tvalidation_0-mlogloss:0.20029\tvalidation_1-mlogloss:0.62611\n",
      "************ Val_Result ************\n",
      "AVG_acc : 0.7400807856216417\n",
      "XGB_result :               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.65      0.66       550\n",
      "           1       0.83      0.84      0.83       772\n",
      "           2       0.63      0.64      0.64       477\n",
      "\n",
      "    accuracy                           0.73      1799\n",
      "   macro avg       0.71      0.71      0.71      1799\n",
      "weighted avg       0.73      0.73      0.73      1799\n",
      "\n",
      "Seed: 888\n",
      "|  LGB  Fold  1  Training Start           |\n",
      "************ Val_Result ************\n",
      "AVG_acc : 0.7455555555555555\n",
      "Cat_result :               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.68      0.68       550\n",
      "           1       0.84      0.85      0.85       772\n",
      "           2       0.67      0.65      0.66       478\n",
      "\n",
      "    accuracy                           0.75      1800\n",
      "   macro avg       0.73      0.73      0.73      1800\n",
      "weighted avg       0.74      0.75      0.75      1800\n",
      "\n",
      "|  LGB  Fold  2  Training Start           |\n",
      "************ Val_Result ************\n",
      "AVG_acc : 0.7494444444444444\n",
      "Cat_result :               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.69      0.69       549\n",
      "           1       0.82      0.87      0.85       773\n",
      "           2       0.69      0.64      0.66       478\n",
      "\n",
      "    accuracy                           0.75      1800\n",
      "   macro avg       0.74      0.73      0.73      1800\n",
      "weighted avg       0.75      0.75      0.75      1800\n",
      "\n",
      "|  LGB  Fold  3  Training Start           |\n",
      "************ Val_Result ************\n",
      "AVG_acc : 0.7498148148148148\n",
      "Cat_result :               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.70      0.70       550\n",
      "           1       0.82      0.87      0.84       773\n",
      "           2       0.67      0.62      0.65       477\n",
      "\n",
      "    accuracy                           0.75      1800\n",
      "   macro avg       0.73      0.73      0.73      1800\n",
      "weighted avg       0.75      0.75      0.75      1800\n",
      "\n",
      "|  LGB  Fold  4  Training Start           |\n",
      "************ Val_Result ************\n",
      "AVG_acc : 0.7509722222222222\n",
      "Cat_result :               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.68      0.70       550\n",
      "           1       0.82      0.85      0.83       773\n",
      "           2       0.69      0.69      0.69       477\n",
      "\n",
      "    accuracy                           0.75      1800\n",
      "   macro avg       0.74      0.74      0.74      1800\n",
      "weighted avg       0.75      0.75      0.75      1800\n",
      "\n",
      "|  LGB  Fold  5  Training Start           |\n",
      "************ Val_Result ************\n",
      "AVG_acc : 0.7484153542091285\n",
      "Cat_result :               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.68      0.68       550\n",
      "           1       0.82      0.85      0.84       772\n",
      "           2       0.65      0.63      0.64       477\n",
      "\n",
      "    accuracy                           0.74      1799\n",
      "   macro avg       0.72      0.72      0.72      1799\n",
      "weighted avg       0.74      0.74      0.74      1799\n",
      "\n",
      "Seed: 888\n",
      "|  cat  Fold  1  Training Start           |\n",
      "************ Val_Result ************\n",
      "AVG_acc : 0.74\n",
      "Cat_result :               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.68      0.67       550\n",
      "           1       0.83      0.85      0.84       772\n",
      "           2       0.67      0.63      0.65       478\n",
      "\n",
      "    accuracy                           0.74      1800\n",
      "   macro avg       0.72      0.72      0.72      1800\n",
      "weighted avg       0.74      0.74      0.74      1800\n",
      "\n",
      "|  cat  Fold  2  Training Start           |\n",
      "************ Val_Result ************\n",
      "AVG_acc : 0.745\n",
      "Cat_result :               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.67      0.68       549\n",
      "           1       0.82      0.87      0.84       773\n",
      "           2       0.69      0.65      0.67       478\n",
      "\n",
      "    accuracy                           0.75      1800\n",
      "   macro avg       0.73      0.73      0.73      1800\n",
      "weighted avg       0.75      0.75      0.75      1800\n",
      "\n",
      "|  cat  Fold  3  Training Start           |\n",
      "************ Val_Result ************\n",
      "AVG_acc : 0.7437037037037038\n",
      "Cat_result :               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.70      0.69       550\n",
      "           1       0.82      0.85      0.83       773\n",
      "           2       0.66      0.62      0.64       477\n",
      "\n",
      "    accuracy                           0.74      1800\n",
      "   macro avg       0.72      0.72      0.72      1800\n",
      "weighted avg       0.74      0.74      0.74      1800\n",
      "\n",
      "|  cat  Fold  4  Training Start           |\n",
      "************ Val_Result ************\n",
      "AVG_acc : 0.7445833333333334\n",
      "Cat_result :               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.68      0.70       550\n",
      "           1       0.80      0.84      0.82       773\n",
      "           2       0.69      0.67      0.68       477\n",
      "\n",
      "    accuracy                           0.75      1800\n",
      "   macro avg       0.73      0.73      0.73      1800\n",
      "weighted avg       0.74      0.75      0.75      1800\n",
      "\n",
      "|  cat  Fold  5  Training Start           |\n",
      "************ Val_Result ************\n",
      "AVG_acc : 0.7429707244765611\n",
      "Cat_result :               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.67      0.67       550\n",
      "           1       0.82      0.85      0.83       772\n",
      "           2       0.66      0.63      0.64       477\n",
      "\n",
      "    accuracy                           0.74      1799\n",
      "   macro avg       0.72      0.72      0.72      1799\n",
      "weighted avg       0.73      0.74      0.73      1799\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#训练集困难样本处理\n",
    "df_pre = pd.DataFrame()\n",
    "df_pre['xgb_pre'] = np.argmax(xgb_train_oof_1,axis=1)\n",
    "df_pre['lgb_pre'] = np.argmax(lgb_train_oof_1,axis=1)\n",
    "df_pre['cat_pre'] = np.argmax(cat_train_oof_1,axis=1)\n",
    "df_pre['label'] = train_df['label']\n",
    "\n",
    "grade_list = []\n",
    "for row in df_pre.itertuples():\n",
    "    grade = 0\n",
    "    if getattr(row,'xgb_pre') == getattr(row,'label'):\n",
    "        grade += 1\n",
    "    if getattr(row,'lgb_pre') == getattr(row,'label'):\n",
    "        grade += 1\n",
    "    if getattr(row,'cat_pre') == getattr(row,'label'):\n",
    "        grade += 1\n",
    "    grade_list.append(grade)\n",
    "\n",
    "#困难样本识别\n",
    "df_pre['grade'] = grade_list\n",
    "train_hard_index = df_pre.loc[(df_pre['grade']==0)].index\n",
    "\n",
    "train_hard_df = train_df.loc[train_hard_index].reset_index(drop=True)\n",
    "xgb_train_hard_oof, xgb_train_hard_model,scaler_2 = ml_model('xgb',train_hard_df[feature_cols],train_hard_df['label'])\n",
    "lgb_train_hard_oof, lgb_train_hard_model,scaler_2 = ml_model('lgb',train_hard_df[feature_cols],train_hard_df['label'])\n",
    "cat_train_hard_oof, cat_train_hard_model,scaler_2 = ml_model('cat',train_hard_df[feature_cols],train_hard_df['label'])\n",
    "\n",
    "#训练集替换困难样本结果\n",
    "xgb_train_oof_2 = np.argmax(xgb_train_oof_1,axis=1)\n",
    "xgb_train_oof_2[train_hard_index]=np.argmax(xgb_train_hard_oof,axis=1)\n",
    "\n",
    "lgb_train_oof_2 = np.argmax(lgb_train_oof_1,axis=1)\n",
    "lgb_train_oof_2[train_hard_index]=np.argmax(lgb_train_hard_oof,axis=1)\n",
    "\n",
    "cat_train_oof_2 = np.argmax(cat_train_oof_1,axis=1)\n",
    "cat_train_oof_2[train_hard_index]=np.argmax(cat_train_hard_oof,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test_hard result :\n",
      "xgb_test_hard result: 0.7598797250859106\n",
      "lgb_test_hard result: 0.7800687285223368\n",
      "cat_test_hard result: 0.7736254295532646\n"
     ]
    }
   ],
   "source": [
    "#测试集困难样本处理\n",
    "df_pre = pd.DataFrame()\n",
    "df_pre['xgb_pre'] = np.argmax(xgb_test_oof_1,axis=1)\n",
    "df_pre['lgb_pre'] = np.argmax(lgb_test_oof_1,axis=1)\n",
    "df_pre['cat_pre'] = np.argmax(cat_test_oof_1,axis=1).flatten()\n",
    "df_pre['label'] = test_df['label']\n",
    "\n",
    "grade_list = []\n",
    "for row in df_pre.itertuples():\n",
    "    grade = 0\n",
    "    if getattr(row,'xgb_pre') == getattr(row,'label'):\n",
    "        grade += 1\n",
    "    if getattr(row,'lgb_pre') == getattr(row,'label'):\n",
    "        grade += 1\n",
    "    if getattr(row,'cat_pre') == getattr(row,'label'):\n",
    "        grade += 1\n",
    "    grade_list.append(grade)\n",
    "\n",
    "#测试集替换困难样本结果\n",
    "df_pre['grade'] = grade_list\n",
    "test_hard_index = df_pre.loc[(df_pre['grade']==0)].index\n",
    "test_hard_df = test_df.loc[test_hard_index].reset_index(drop=True)\n",
    "\n",
    "print('Test_hard result :')\n",
    "xgb_test_hard_oof = xgb_train_hard_model.predict(scaler_2.transform(test_hard_df[feature_cols]))\n",
    "print('xgb_test_hard result:',accuracy_score(xgb_test_hard_oof,test_hard_df['label']))\n",
    "lgb_test_hard_oof = lgb_train_hard_model.predict(scaler_2.transform(test_hard_df[feature_cols]))\n",
    "print('lgb_test_hard result:',accuracy_score(lgb_test_hard_oof,test_hard_df['label']))\n",
    "cat_test_hard_oof = cat_train_hard_model.predict(scaler_2.transform(test_hard_df[feature_cols]))\n",
    "print('cat_test_hard result:',accuracy_score(cat_test_hard_oof,test_hard_df['label']))\n",
    "\n",
    "\n",
    "xgb_test_oof_2 = np.argmax(xgb_test_oof_1,axis=1)\n",
    "xgb_test_oof_2[test_hard_index]=xgb_test_hard_oof\n",
    "\n",
    "lgb_test_oof_2 = np.argmax(lgb_test_oof_1,axis=1)\n",
    "lgb_test_oof_2[test_hard_index]=lgb_test_hard_oof\n",
    "\n",
    "cat_test_oof_2 = np.argmax(cat_test_oof_1,axis=1)\n",
    "cat_test_oof_2[test_hard_index]=cat_test_hard_oof.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed: 888\n",
      "     Tab_model  Fold 1  Training Starting       \n",
      "Using CPU\n",
      "epoch 0  | loss: 0.54512 | val_0_accuracy: 0.91737 |  0:00:03s\n",
      "epoch 1  | loss: 0.28215 | val_0_accuracy: 0.92328 |  0:00:07s\n",
      "epoch 2  | loss: 0.2617  | val_0_accuracy: 0.92237 |  0:00:10s\n",
      "epoch 3  | loss: 0.24948 | val_0_accuracy: 0.92383 |  0:00:14s\n",
      "epoch 4  | loss: 0.24828 | val_0_accuracy: 0.9231  |  0:00:18s\n",
      "epoch 5  | loss: 0.24792 | val_0_accuracy: 0.92292 |  0:00:21s\n",
      "epoch 6  | loss: 0.24348 | val_0_accuracy: 0.92264 |  0:00:25s\n",
      "epoch 7  | loss: 0.24308 | val_0_accuracy: 0.92283 |  0:00:28s\n",
      "epoch 8  | loss: 0.24088 | val_0_accuracy: 0.92364 |  0:00:32s\n",
      "epoch 9  | loss: 0.24043 | val_0_accuracy: 0.92373 |  0:00:35s\n",
      "epoch 10 | loss: 0.23849 | val_0_accuracy: 0.92455 |  0:00:39s\n",
      "epoch 11 | loss: 0.23753 | val_0_accuracy: 0.92446 |  0:00:43s\n",
      "epoch 12 | loss: 0.23354 | val_0_accuracy: 0.92473 |  0:00:46s\n",
      "epoch 13 | loss: 0.23128 | val_0_accuracy: 0.92383 |  0:00:50s\n",
      "epoch 14 | loss: 0.23101 | val_0_accuracy: 0.92428 |  0:00:53s\n",
      "epoch 15 | loss: 0.22951 | val_0_accuracy: 0.9231  |  0:00:57s\n",
      "epoch 16 | loss: 0.22903 | val_0_accuracy: 0.92373 |  0:01:00s\n",
      "epoch 17 | loss: 0.22679 | val_0_accuracy: 0.9241  |  0:01:04s\n",
      "epoch 18 | loss: 0.23069 | val_0_accuracy: 0.92083 |  0:01:07s\n",
      "epoch 19 | loss: 0.23827 | val_0_accuracy: 0.92446 |  0:01:11s\n",
      "epoch 20 | loss: 0.22725 | val_0_accuracy: 0.92183 |  0:01:15s\n",
      "epoch 21 | loss: 0.22788 | val_0_accuracy: 0.9231  |  0:01:18s\n",
      "epoch 22 | loss: 0.22465 | val_0_accuracy: 0.92401 |  0:01:22s\n",
      "\n",
      "Early stopping occurred at epoch 22 with best_epoch = 12 and best_val_0_accuracy = 0.92473\n",
      "************ Val_Result ************\n",
      "AVG_acc : 0.9247341150804472\n",
      "Tabnet_result :               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.95      0.95      5910\n",
      "           1       0.90      0.93      0.92      3685\n",
      "           2       0.84      0.81      0.83      1406\n",
      "\n",
      "    accuracy                           0.92     11001\n",
      "   macro avg       0.90      0.90      0.90     11001\n",
      "weighted avg       0.92      0.92      0.92     11001\n",
      "\n",
      "************ Test_Result ************\n",
      "AVG_acc : 0.9387680895934841\n",
      "Tabnet_result :               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.96      0.96      7458\n",
      "           1       0.92      0.95      0.93      4501\n",
      "           2       0.88      0.84      0.86      1792\n",
      "\n",
      "    accuracy                           0.94     13751\n",
      "   macro avg       0.92      0.92      0.92     13751\n",
      "weighted avg       0.94      0.94      0.94     13751\n",
      "\n",
      "     Tab_model  Fold 2  Training Starting       \n",
      "Using CPU\n",
      "epoch 0  | loss: 0.54327 | val_0_accuracy: 0.92464 |  0:00:03s\n",
      "epoch 1  | loss: 0.27113 | val_0_accuracy: 0.92492 |  0:00:07s\n",
      "epoch 2  | loss: 0.25439 | val_0_accuracy: 0.92619 |  0:00:10s\n",
      "epoch 3  | loss: 0.24845 | val_0_accuracy: 0.92473 |  0:00:14s\n",
      "epoch 4  | loss: 0.24415 | val_0_accuracy: 0.92483 |  0:00:17s\n",
      "epoch 5  | loss: 0.24221 | val_0_accuracy: 0.92446 |  0:00:21s\n",
      "epoch 6  | loss: 0.2402  | val_0_accuracy: 0.92582 |  0:00:24s\n",
      "epoch 7  | loss: 0.24235 | val_0_accuracy: 0.92619 |  0:00:28s\n",
      "epoch 8  | loss: 0.23794 | val_0_accuracy: 0.92292 |  0:00:32s\n",
      "epoch 9  | loss: 0.23526 | val_0_accuracy: 0.92464 |  0:00:35s\n",
      "epoch 10 | loss: 0.23243 | val_0_accuracy: 0.92383 |  0:00:39s\n",
      "epoch 11 | loss: 0.22973 | val_0_accuracy: 0.92437 |  0:00:42s\n",
      "epoch 12 | loss: 0.22766 | val_0_accuracy: 0.92419 |  0:00:46s\n",
      "\n",
      "Early stopping occurred at epoch 12 with best_epoch = 2 and best_val_0_accuracy = 0.92619\n",
      "************ Val_Result ************\n",
      "AVG_acc : 0.9254613216980274\n",
      "Tabnet_result :               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.96      0.96      5909\n",
      "           1       0.92      0.91      0.92      3685\n",
      "           2       0.83      0.83      0.83      1407\n",
      "\n",
      "    accuracy                           0.93     11001\n",
      "   macro avg       0.90      0.90      0.90     11001\n",
      "weighted avg       0.93      0.93      0.93     11001\n",
      "\n",
      "************ Test_Result ************\n",
      "AVG_acc : 0.9390953385208349\n",
      "Tabnet_result :               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.96      0.96      7458\n",
      "           1       0.93      0.94      0.93      4501\n",
      "           2       0.86      0.85      0.86      1792\n",
      "\n",
      "    accuracy                           0.94     13751\n",
      "   macro avg       0.92      0.92      0.92     13751\n",
      "weighted avg       0.94      0.94      0.94     13751\n",
      "\n",
      "     Tab_model  Fold 3  Training Starting       \n",
      "Using CPU\n",
      "epoch 0  | loss: 0.51906 | val_0_accuracy: 0.92164 |  0:00:03s\n",
      "epoch 1  | loss: 0.28345 | val_0_accuracy: 0.92337 |  0:00:07s\n",
      "epoch 2  | loss: 0.26261 | val_0_accuracy: 0.92801 |  0:00:10s\n",
      "epoch 3  | loss: 0.25602 | val_0_accuracy: 0.92573 |  0:00:14s\n",
      "epoch 4  | loss: 0.25283 | val_0_accuracy: 0.92919 |  0:00:17s\n",
      "epoch 5  | loss: 0.24944 | val_0_accuracy: 0.92546 |  0:00:21s\n",
      "epoch 6  | loss: 0.24326 | val_0_accuracy: 0.92501 |  0:00:25s\n",
      "epoch 7  | loss: 0.24422 | val_0_accuracy: 0.92437 |  0:00:28s\n",
      "epoch 8  | loss: 0.24307 | val_0_accuracy: 0.92664 |  0:00:32s\n",
      "epoch 9  | loss: 0.24731 | val_0_accuracy: 0.92673 |  0:00:35s\n",
      "epoch 10 | loss: 0.24202 | val_0_accuracy: 0.92555 |  0:00:39s\n",
      "epoch 11 | loss: 0.23974 | val_0_accuracy: 0.92764 |  0:00:43s\n",
      "epoch 12 | loss: 0.2351  | val_0_accuracy: 0.9261  |  0:00:46s\n",
      "epoch 13 | loss: 0.23545 | val_0_accuracy: 0.92646 |  0:00:50s\n",
      "epoch 14 | loss: 0.23408 | val_0_accuracy: 0.92601 |  0:00:53s\n",
      "\n",
      "Early stopping occurred at epoch 14 with best_epoch = 4 and best_val_0_accuracy = 0.92919\n",
      "************ Val_Result ************\n",
      "AVG_acc : 0.9267036330030604\n",
      "Tabnet_result :               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.95      0.95      5909\n",
      "           1       0.92      0.92      0.92      3685\n",
      "           2       0.84      0.84      0.84      1407\n",
      "\n",
      "    accuracy                           0.93     11001\n",
      "   macro avg       0.91      0.91      0.91     11001\n",
      "weighted avg       0.93      0.93      0.93     11001\n",
      "\n",
      "************ Test_Result ************\n",
      "AVG_acc : 0.939761956706179\n",
      "Tabnet_result :               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.96      0.96      7458\n",
      "           1       0.93      0.94      0.94      4501\n",
      "           2       0.87      0.85      0.86      1792\n",
      "\n",
      "    accuracy                           0.94     13751\n",
      "   macro avg       0.92      0.92      0.92     13751\n",
      "weighted avg       0.94      0.94      0.94     13751\n",
      "\n",
      "     Tab_model  Fold 4  Training Starting       \n",
      "Using CPU\n",
      "epoch 0  | loss: 0.51792 | val_0_accuracy: 0.92555 |  0:00:03s\n",
      "epoch 1  | loss: 0.28873 | val_0_accuracy: 0.92537 |  0:00:07s\n",
      "epoch 2  | loss: 0.2601  | val_0_accuracy: 0.92764 |  0:00:10s\n",
      "epoch 3  | loss: 0.25338 | val_0_accuracy: 0.92737 |  0:00:14s\n",
      "epoch 4  | loss: 0.24948 | val_0_accuracy: 0.92682 |  0:00:17s\n",
      "epoch 5  | loss: 0.25187 | val_0_accuracy: 0.92601 |  0:00:21s\n",
      "epoch 6  | loss: 0.24899 | val_0_accuracy: 0.92582 |  0:00:24s\n",
      "epoch 7  | loss: 0.24515 | val_0_accuracy: 0.92619 |  0:00:28s\n",
      "epoch 8  | loss: 0.24325 | val_0_accuracy: 0.92782 |  0:00:32s\n",
      "epoch 9  | loss: 0.24203 | val_0_accuracy: 0.92646 |  0:00:36s\n",
      "epoch 10 | loss: 0.24279 | val_0_accuracy: 0.92619 |  0:00:39s\n",
      "epoch 11 | loss: 0.24514 | val_0_accuracy: 0.92728 |  0:00:43s\n",
      "epoch 12 | loss: 0.2443  | val_0_accuracy: 0.92673 |  0:00:46s\n",
      "epoch 13 | loss: 0.24197 | val_0_accuracy: 0.92828 |  0:00:50s\n",
      "epoch 14 | loss: 0.23909 | val_0_accuracy: 0.92682 |  0:00:54s\n",
      "epoch 15 | loss: 0.23794 | val_0_accuracy: 0.92764 |  0:00:57s\n",
      "epoch 16 | loss: 0.23881 | val_0_accuracy: 0.92728 |  0:01:01s\n",
      "epoch 17 | loss: 0.23937 | val_0_accuracy: 0.92782 |  0:01:04s\n",
      "epoch 18 | loss: 0.2372  | val_0_accuracy: 0.92746 |  0:01:08s\n",
      "epoch 19 | loss: 0.23653 | val_0_accuracy: 0.92873 |  0:01:12s\n",
      "epoch 20 | loss: 0.23637 | val_0_accuracy: 0.92655 |  0:01:15s\n",
      "epoch 21 | loss: 0.23513 | val_0_accuracy: 0.92792 |  0:01:19s\n",
      "epoch 22 | loss: 0.23335 | val_0_accuracy: 0.92855 |  0:01:22s\n",
      "epoch 23 | loss: 0.23257 | val_0_accuracy: 0.92801 |  0:01:26s\n",
      "epoch 24 | loss: 0.23227 | val_0_accuracy: 0.92701 |  0:01:30s\n",
      "epoch 25 | loss: 0.23226 | val_0_accuracy: 0.92901 |  0:01:33s\n",
      "epoch 26 | loss: 0.2364  | val_0_accuracy: 0.92746 |  0:01:37s\n",
      "epoch 27 | loss: 0.23608 | val_0_accuracy: 0.92864 |  0:01:40s\n",
      "epoch 28 | loss: 0.23642 | val_0_accuracy: 0.92592 |  0:01:44s\n",
      "epoch 29 | loss: 0.2355  | val_0_accuracy: 0.92801 |  0:01:47s\n",
      "epoch 30 | loss: 0.23381 | val_0_accuracy: 0.92728 |  0:01:51s\n",
      "epoch 31 | loss: 0.23084 | val_0_accuracy: 0.92892 |  0:01:55s\n",
      "epoch 32 | loss: 0.23104 | val_0_accuracy: 0.92846 |  0:01:58s\n",
      "epoch 33 | loss: 0.23035 | val_0_accuracy: 0.92628 |  0:02:02s\n",
      "epoch 34 | loss: 0.22927 | val_0_accuracy: 0.92946 |  0:02:05s\n",
      "epoch 35 | loss: 0.23005 | val_0_accuracy: 0.92973 |  0:02:09s\n",
      "epoch 36 | loss: 0.23016 | val_0_accuracy: 0.92564 |  0:02:12s\n",
      "epoch 37 | loss: 0.22785 | val_0_accuracy: 0.92882 |  0:02:16s\n",
      "epoch 38 | loss: 0.22636 | val_0_accuracy: 0.92837 |  0:02:20s\n",
      "epoch 39 | loss: 0.2256  | val_0_accuracy: 0.92837 |  0:02:23s\n",
      "epoch 40 | loss: 0.22835 | val_0_accuracy: 0.92828 |  0:02:27s\n",
      "epoch 41 | loss: 0.22888 | val_0_accuracy: 0.92801 |  0:02:31s\n",
      "epoch 42 | loss: 0.22967 | val_0_accuracy: 0.92782 |  0:02:34s\n",
      "epoch 43 | loss: 0.22726 | val_0_accuracy: 0.92628 |  0:02:38s\n",
      "epoch 44 | loss: 0.22572 | val_0_accuracy: 0.92846 |  0:02:41s\n",
      "epoch 45 | loss: 0.22638 | val_0_accuracy: 0.92855 |  0:02:45s\n",
      "\n",
      "Early stopping occurred at epoch 45 with best_epoch = 35 and best_val_0_accuracy = 0.92973\n",
      "************ Val_Result ************\n",
      "AVG_acc : 0.927461139896373\n",
      "Tabnet_result :               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.96      0.96      5910\n",
      "           1       0.93      0.91      0.92      3684\n",
      "           2       0.85      0.84      0.84      1407\n",
      "\n",
      "    accuracy                           0.93     11001\n",
      "   macro avg       0.91      0.90      0.91     11001\n",
      "weighted avg       0.93      0.93      0.93     11001\n",
      "\n",
      "************ Test_Result ************\n",
      "AVG_acc : 0.9397134753836085\n",
      "Tabnet_result :               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.97      0.96      7458\n",
      "           1       0.94      0.93      0.93      4501\n",
      "           2       0.86      0.85      0.86      1792\n",
      "\n",
      "    accuracy                           0.94     13751\n",
      "   macro avg       0.92      0.92      0.92     13751\n",
      "weighted avg       0.94      0.94      0.94     13751\n",
      "\n",
      "     Tab_model  Fold 5  Training Starting       \n",
      "Using CPU\n",
      "epoch 0  | loss: 0.54548 | val_0_accuracy: 0.92209 |  0:00:03s\n",
      "epoch 1  | loss: 0.2814  | val_0_accuracy: 0.922   |  0:00:07s\n",
      "epoch 2  | loss: 0.25741 | val_0_accuracy: 0.92064 |  0:00:10s\n",
      "epoch 3  | loss: 0.24399 | val_0_accuracy: 0.92409 |  0:00:14s\n",
      "epoch 4  | loss: 0.23867 | val_0_accuracy: 0.92427 |  0:00:18s\n",
      "epoch 5  | loss: 0.23736 | val_0_accuracy: 0.92173 |  0:00:21s\n",
      "epoch 6  | loss: 0.2396  | val_0_accuracy: 0.92309 |  0:00:25s\n",
      "epoch 7  | loss: 0.23805 | val_0_accuracy: 0.925   |  0:00:28s\n",
      "epoch 8  | loss: 0.23592 | val_0_accuracy: 0.92473 |  0:00:32s\n",
      "epoch 9  | loss: 0.23438 | val_0_accuracy: 0.92491 |  0:00:36s\n",
      "epoch 10 | loss: 0.23234 | val_0_accuracy: 0.92436 |  0:00:39s\n",
      "epoch 11 | loss: 0.23072 | val_0_accuracy: 0.92473 |  0:00:43s\n",
      "epoch 12 | loss: 0.23222 | val_0_accuracy: 0.92509 |  0:00:46s\n",
      "epoch 13 | loss: 0.23053 | val_0_accuracy: 0.92491 |  0:00:50s\n",
      "epoch 14 | loss: 0.22898 | val_0_accuracy: 0.92536 |  0:00:54s\n",
      "epoch 15 | loss: 0.22773 | val_0_accuracy: 0.92427 |  0:00:57s\n",
      "epoch 16 | loss: 0.22813 | val_0_accuracy: 0.92264 |  0:01:01s\n",
      "epoch 17 | loss: 0.22816 | val_0_accuracy: 0.92418 |  0:01:04s\n",
      "epoch 18 | loss: 0.22745 | val_0_accuracy: 0.92427 |  0:01:08s\n",
      "epoch 19 | loss: 0.22817 | val_0_accuracy: 0.924   |  0:01:11s\n",
      "epoch 20 | loss: 0.22584 | val_0_accuracy: 0.92327 |  0:01:15s\n",
      "epoch 21 | loss: 0.22542 | val_0_accuracy: 0.92409 |  0:01:19s\n",
      "epoch 22 | loss: 0.22474 | val_0_accuracy: 0.923   |  0:01:22s\n",
      "epoch 23 | loss: 0.22452 | val_0_accuracy: 0.92455 |  0:01:26s\n",
      "epoch 24 | loss: 0.22574 | val_0_accuracy: 0.92536 |  0:01:30s\n",
      "\n",
      "Early stopping occurred at epoch 24 with best_epoch = 14 and best_val_0_accuracy = 0.92536\n",
      "************ Val_Result ************\n",
      "AVG_acc : 0.9270416391898257\n",
      "Tabnet_result :               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.94      0.95      5910\n",
      "           1       0.90      0.94      0.92      3684\n",
      "           2       0.83      0.83      0.83      1406\n",
      "\n",
      "    accuracy                           0.93     11000\n",
      "   macro avg       0.90      0.90      0.90     11000\n",
      "weighted avg       0.93      0.93      0.93     11000\n",
      "\n",
      "************ Test_Result ************\n",
      "AVG_acc : 0.939669842193295\n",
      "Tabnet_result :               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.96      0.96      7458\n",
      "           1       0.92      0.95      0.94      4501\n",
      "           2       0.88      0.84      0.86      1792\n",
      "\n",
      "    accuracy                           0.94     13751\n",
      "   macro avg       0.92      0.92      0.92     13751\n",
      "weighted avg       0.94      0.94      0.94     13751\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#xgb预测结果作为新特征(替换困难样本)\n",
    "train_df['xgb_pre'] = xgb_train_oof_2\n",
    "test_df['xgb_pre'] = xgb_test_oof_2\n",
    "#lgb预测结果作为新特征(替换困难样本)\n",
    "train_df['lgb_pre'] = lgb_train_oof_2\n",
    "test_df['lgb_pre'] = lgb_test_oof_2\n",
    "#cat预测结果作为新特征(替换困难样本)\n",
    "train_df['cat_pre'] = cat_train_oof_2\n",
    "test_df['cat_pre'] = cat_test_oof_2\n",
    "\n",
    "#训练特征\n",
    "feature_cols = [cols for cols in train_df if cols not in ['msisdn','label','end_time','stime','times_month']]\n",
    "# 训练tabnet模型\n",
    "final_tab_train_oof,final_tab_test_oof,final_tab_model,scaler = ml_model('tabnet',train_df[feature_cols], train_df['label'],test_df[feature_cols], test_df['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "#模型保存\n",
    "def save_model(mdoel,save_path):\n",
    "    # save_path = save_path + '/' +  mdoel_name +'.pkl'\n",
    "    joblib.dump(mdoel, save_path)\n",
    "#模型保存路径\n",
    "xgb_model_path = './model/xgb.pkl'\n",
    "lgb_model_path = './model/lgb.pkl'\n",
    "cat_model_path = './model/cat.pkl'\n",
    "svm_model_path = './model/svm.pkl'\n",
    "tabnet_model_path = './model/tabnet.pkl'\n",
    "final_tab_model_path = './model/final_tab.pkl'\n",
    "#保存\n",
    "save_model(xgb_model,xgb_model_path)\n",
    "save_model(lgb_model, lgb_model_path)\n",
    "save_model(cat_model,  cat_model_path)\n",
    "save_model(svm_model,svm_model_path)\n",
    "save_model(tab_model, tabnet_model_path)\n",
    "save_model(final_tab_model,  final_tab_model_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machine_learning_39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
